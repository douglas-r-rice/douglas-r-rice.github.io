---
layout: post
title: "Shifting Conversations on Parler Across January 6th (Linberg)"
author: Steve Linberg
date: 2021-04-18 12:24:54 -0500
categories: linberg
---


The TL;DR on this final blog post is:

1. Realized my approach from the [previous post](https://douglas-r-rice.github.io/linberg/2021/03/28/4-linberg.html) wasn't going to work
2. Spent a couple of weeks thrashing various approaches, none of which worked particularly well
3. Did a dry-run on a process that worked reasonably on a small data sample
4. Still to be determined is whether we stop there, if the full results show something interesting, or whether we'll have to push farther into newer methods.

The overall goal is still to try to ascertain whether there was an overall shift in sentiment on the Parler website after the insurrection at the U.S. Capitol on January 6. Previous posts have focused on the challenge of obtaining, processing, and filtering the enormous amount of data.

I am only looking at posts made between January 1, 2021 and January 12, 2021, when the Parler service was deplatformed from Amazon for violating its terms of service regarding lack of moderation of calls for violence. I filtered these posts to only include users who made at least 5, but no more than 250 posts both before and after the 6th, in an effort to weed out spam, fanatical users and very casual users. In an effort to increase the document size, as the single-post-sized documents are a bit small for some of the code libraries we use, I concatenated each user's posts together in "pre", "post" and "on" categories for posts before, after and on the 6th, respectively, thinking I could look at the overall sentiment of individual users. 

The end of my previous blog post shows pretty clearly why this won't work; looking at sample results for a single user, it's clear that the topics of the concatenated posts range widely, making any kind of real sentiment analysis impossible. Too many things are being looked at. I realized I needed to try to drill down more to just posts about the insurrection.

Rather than attempt some sort of topical analysis, I decided on a more brute-force approach of just using regular expressions, to get posts that explicitly reference the 6th by date in some manner.

First, we load the January 2021 posts, and filter it down to posts made by the users we selected last time:

```
parler_dir <- "~/Parler_data/parler_data"
load(paste(parler_dir, "all_January_2021_posts.Rdata", sep='/'))
active_usernames <- read_rds(paste(parler_dir, "filtered_January_2021_usernames.rds", sep='/'))
filtered_posts <- all_posts %>%
  filter(username %in% active_usernames$username)
```

Then, we do a few blocks of fairly straightforward regular expressions. First, for posts made in January that explicitly mention the 6th (this will match strings like "jan 6", "january 6", "1/6", "6 jan" or anything with "sixth", case-insensitive):

```
jan6_regex = "(jan\\w*\\s*6|1/6|6.jan|sixth)"
matched_january_posts <- filtered_posts %>%
  filter(str_detect(body, regex(jan6_regex, ignore_case = TRUE)))
```

Then posts from January 5 mentioning "tomorrow" or "tmw":

```
tomorrow_regex = "(tmw|tomorr)"
matched_jan5_tmw_posts <- filtered_posts %>%
  filter(str_detect(body, regex(tomorrow_regex, ignore_case = TRUE))) %>%
  filter(created_day == 5)
```

January 6 posts mentioning "today" or "right now":

```
today_regex = "\\b(today|right\\s+now)\\b"
matched_jan6_today_posts <- filtered_posts %>%
  filter(str_detect(body, regex(today_regex, ignore_case = TRUE))) %>%
  filter(created_day == 6)
```

January 7 posts mentioning "yesterday":

```
yesterday_regex = "yester"
matched_jan7_yest_posts <- filtered_posts %>%
  filter(str_detect(body, regex(yesterday_regex, ignore_case = TRUE))) %>%
  filter(created_day == 7)
```

I tested one matching variants of "Wednesday" (January 6 was a Wednesday), but it ended up being too broad, as was one mentioning "Save America" (ostensibly the name of the rally), so I discarded these.

I then combine all of these sets of posts, removing any duplicates and adding the convenience category variable `ceeated_class` containing "pre", "on" or "post" (technically still a string here, we turn it into a proper factor later), and save the resulting data frame.

```
n_day <- 6
posts_about_jan6 <- matched_january_posts %>%
  rbind(matched_jan5_tmw_posts) %>%
  rbind(matched_jan6_today_posts) %>%
  rbind(matched_jan7_yest_posts) %>%
  # filter out any duplicates
  distinct(id, .keep_all = TRUE) %>%
  # categorize as "pre", "on" or "post"
  mutate(created_class = case_when(
          created_day < n_day ~ "pre",
          created_day == n_day ~ "on",
          created_day > n_day ~ "post",
          TRUE ~ NA_character_
        )) %>%
  filter(!is.na(created_class))
```

Then it's time to start looking at the data, and making a DFM of it. One thing I realized that I needed to do early on was to create a couple of pseudo-terms to combine different words meaning the same thing, like "Jan" and "January", which were both used heavily and occupying primary spaces in the word count totals.

```
# Merge a couple of categories, because they're really the same thing.
posts_about_jan6$body <- posts_about_jan6$body %>%
# Change "january" and "jan" to "<combined-jan>"
# \b will help prevent a repetition of these subs
# multiple regex and ignoring cases gets messy and neccesitates splitting the
# subs and replacements; this is a little less efficient but more readable
  str_replace_all(regex("\\bjan(?:uary)*\\b", ignore_case = TRUE), "<combined-jan>") %>%
# Change "6" and "6th" to "<combined-6>"
  str_replace_all(regex("\\b6(?:th)*\\b", ignore_case = TRUE), "<combined-6>")
```

Borrowing some process and code from [Julia Silge](https://juliasilge.com/blog/sherlock-holmes-stm/), I looked at some word counts, which show the necessity of the above step. The top 10 words from the days before the insurrection were:

|word|n|
|---|---|
|\<combinedjan\>  |8324|
|\<combined6\>    |8018|
|trump  |3477|
|tomorrow   |2405|
|president  |2238|
|dc |1969|
|people |1919|
|election   |1804|
|pence  |1287|
|patriots   |1223|

Nothing too surprising there. The top 10 words after the insurrection were:

|word|n|
|---|---|
|yesterday  |7860|
|trump  |4034|
|people |3035|
|antifa |2345|
|\<combinedjan\>  |2236|
|\<combined6\>    |2122|
|president  |1377|
|capitol    |1313|
|america    |1182|
|patriots   |1156|

(One notable data point here is the word `antifa`, as a common point of conversation was a claim that the insurrection was actually carried out by disguised antifa activists.)

I'm going to omit a week or so's worth of wrong turns on processing an stm with other libraries and methods that I came across while reading supplemental materials on topic models; basically, I returned to the process described in the class tutorial to test the workflow. I eventually realized that trying to work the process out with the full data set is bad strategy, since we're now at a point in the process where analysis can take hours to days, and it can be hard to guess before any step how long it will take to run.

I decided to work with a 1% random slice of the data, just to get the steps worked out, and to build a very small dfm from it:

```
sample_posts_about_jan6 <- posts_about_jan6 %>%
  sample_frac(0.01)

lesson_based_dfm <- dfm(sample_posts_about_jan6$body, tolower = TRUE, remove = stopwords('en'), remove_punct = TRUE)
```

This results in a tiny dfm of only 322 posts, and 3285 terms - not enough to give meaningful results, but enough for a dry run of the process with quick execution time and turnaround.

Arbitrarily picking an initial value of 6 for `K`, we can create a quick correlated topic model:

```
cor_topic_model <- stm(lesson_based_dfm, K = 6, verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)
```

```
Topic 1 Top Words:
 	 Highest Prob: trump, party, today, tomorrow, republican, election, people 
 	 FREX: communist, cowards, machines, back, d.c, full, presented 
 	 Lift: assault, audit, blue, dominion, forensic, gets, single 
 	 Score: cowards, d.c, propane, party, tomorrow, socialism, machines 
Topic 2 Top Words:
 	 Highest Prob: 🇺🇲, trump, yesterday, state, just, say, president 
 	 FREX: 🇺🇲, oh, wait, say, months, still, show 
 	 Lift: federal, @azgop, @bidon2020, @dorothydotray, @douglasmastriano, @formula44kid, @loveofthegame82 
 	 Score: 🇺🇲, problem, pay, ☠, wait, federal, organize 
Topic 3 Top Words:
 	 Highest Prob: 🚨, trump, biden, now, 😂, 🤣, yesterday 
 	 FREX: 🚨, 🤣, third, sen, @blocked, 😂, electoral 
 	 Lift: #breakingnews, seat, @andweknow, @battleskaard93, @danscavino45, @dbongino, @dougcollins 
 	 Score: 🚨, 🤣, 😂, third, sen, @blocked, 👇 
Topic 4 Top Words:
 	 Highest Prob: today, 🔥, 1, now, yesterday, obama, 🇺🇸 
 	 FREX: 🔥, buddies, democrat, anniv, antifablm, bowser, math 
 	 Lift: #thestormishere, #trump2020, #trumptrain, #walkaway, 🗽, 🤠, buddies 
 	 Score: 🔥, anniv, antifablm, bowser, math, obamas, soleimani's 
Topic 5 Top Words:
 	 Highest Prob: 🇺🇸, yesterday, antifa, $, trump, 🙏, patriots 
 	 FREX: 🙏, blm, mayor, bless, stay, identified, love 
 	 Lift: 🙏🏻, dressed, identified, im, mob, pack, person 
 	 Score: 🇺🇸, $, identified, hotels, restaurants, mayor, protesters 
Topic 6 Top Words:
 	 Highest Prob: <, >, combined-6, combined-jan, trump, election, president 
 	 FREX: <, >, combined-6, combined-jan, 🤔, 2021, day 
 	 Lift: authority, cant, head, interesting, johnson, mike, movement 
 	 Score: <, >, combined-jan, combined-6, ben, movement, johnson 
```

We can't really make a judgement about our K choice with a data sample this small, so we'll just continue with the process from the lesson for the moment. `findThoughts()` shows strong posts for these theoretical topics, which contain some (predictably) strong and unfiltered language:

```
findThoughts(cor_topic_model, 
    texts = sample_posts_about_jan6$body, 
    topics = c(1:6),
    n = 1)
```

```
Topic 1: 
 	 Who are you addressing? Not trump supporters I hope?
FYI: it was ANTIFA who broke in to the capitol today! 
there’s no fucking place in America for communism or socialism as we’re a CONSTITUTIONAL REPUBLIC!
Any asshole who doesn’t understand that needs to get the fuck out of America and go live in a communist utopia of North Korea, north Vietnam, China, Russia, Venezuela, Cuba or any of the other shit holes that are veterans fought and died trying to liberate from the oppression and horrors of socialism/communism
Yo have some nerve lashing out at hard-working people who worked our asses off 16 hour days, 35 years or more only to have you motherfuckers steal our tax money to pay for your fraudulent illegals who live in better houses than mine and get free healthcare that I can’t even get after losing my job to no fault of my own and being sick with Covid! 
Before any illegalmotherfucker gets a penny of my money, I want my free house, free healthcare etc.
FUCK Commie Dems 
 Topic 2: 
 	 I understand your frustrations, as I have struggled with some myself. But I will say, personally, I think the real action taking, shit kicking patriots are waiting for EVERY last legal avenue to be exhausted before crossing that line. President Trump has implored us to trust the system. I don't think anyone TRULY desires armed conflict, and it's not something to take lightly. I believe things will be much more clear by the end of tomorrow, on whether it is indeed going to come down to the people to change this course. We are still being led to believe by our president that major revelations are coming out, so I think many of us are hoping/believing that means a miracle for our side. Nobody wants to be the guy/group that jumped the gun when there were still possible avenues forward that didn't involve bloodshed. These are just my humble thoughts on the matter, though. It's easy to sit in your armchair and call people ignorant, especially when you plan to take zero action yourself 🙄 
 Topic 3: 
 	 Maybe it was a mistake for America to act like an adult over the years and tolerate the immature rantings and threats of the uncivilized world while we tried to teach by example what it is to be a responsible and mature member of the world order. And perhaps equally a mistake to patiently endure the spastic outbursts of various home grown radical groups as they exercised their right to protest. But today all of that stops. At the behest of Donald Trump, hordes of deranged Trump co-conspirators marched on and took over parts of the US Capitol building as state electors were being recorded to install Joe Biden as our new president. This is a coup attempt. Make no mistake. While I have no doubt these radicals are being used as pawns by Trump loyalists, Its time to show them proper discipline. Democracy Must be protected, no matter what the cost. Arrest and hold all of them immediately. And that includes the traitorous Donald Trump. For every action comes an equal and opposite reaction. 
 Topic 4: 
 	 🇺🇸🤠🇺🇸🦅🗽🇺🇸🦅🗽🇺🇸🤠🇺🇸

₲ʘʘƊ MʘrńįƝǤ ☻ my fellow Americans and Patriots!

Let not your hearts ♥️ be troubled. 

It was always known this senate race would end with a “steal”—it was always part of the “plan!”

When you bate a rodent, say a DemocRAT. You must first lay out the cheese 🧀...yes?

If you have ever seen a mouse/RAT trap work...it does not release...then have the catch bar sloooooowly swing around to its final stop...no, once released—it SNAPS SHUT QUICKLY—killing the unwary RODENT—suddenly...and without mercy!

This election was ALWAYS a STING operation—years in the planning!

If the RAT knows they will get trapped by eating the cheese—THEY WOULD NEVER EAT THE CHEESE!

The final performance will begin—TODAY!

There are 190,000 indictments waiting in the DOJ to collar all the Luciferian’s and their “many minions.”

Today, the final performance will trigger the curtain’s fall—with a mighty terrible—ROAR!

Turn around and look, Patriot—THE RED SEA IS ABOUT TO PART!!! 
 Topic 5: 
 	 TRUMP: WE'RE GOING TO WALK DOWN TO THE CAPITOL AND WE'RE GOING TO CHEER OUR BRAVE SENATORS

#electionfraud #fraud #voterfraud #audit #forensicaudit #georgia #pennsylvania #wisconsin #michigan #nevada #arizona #dominion #smartmatic #mailinballots #covid19 #vaccine #vaccinetracing #vaccinepassport #billgates #india #eugenics #populationcontrol #depopulation #dna #rna #who #owg #nwo #greatreset #unitednations #lockdowns #shutdown #nomorelockdowns #masks #masksdontwork #smallbusiness #openup #facemasks #socialism #resist #resistance #countalllegalvotes #trumpwon #maga #kag2020 #holdtheline #neverbackdown #neverconcede #neverbiden #bidenisnotmypresident #TRUTH #FREEDOM #REPUBLIC #REVOLUTION #2020is1776 #marchonwashington #january6 #patriots 
 Topic 6: 
 	 The "reporting" yesterday by @newsmax was a fvcking disgrace! This media recently reaped the benefits of a wave of new viewers after their betrayal by Fox. But it didn't take long for NewsMax to show its TRUE color: piss-yellow.

How disgusting, disappointing to hear this so-called conservative news source jump immediately on the hater-donkey bandwagon to spread unverified enemedia accounts that the over quarter million populists peacefully gathered in DC were "violent" "rioting" "storming" and any other inciting UNJUSTIFIED adjectives NM talking heads could find to smear+slander its loyal audience+Americans! And this NM talking head enabled his white-haired guest to insinuate despicable lies about those Americans gathered in cold, windy DC to protest an illegal election. There was no investigation conducted! No authenticated evidence! Typical leftist media hearsay+hysteria was enough for NewsMax to immediately drop its facade of conservatism.
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/linberg-post-5-topics.png?raw=TRUE)

A structured topic model returned very similar results, so I won't reproduce them here since the results with this small sample data set isn't particularly meaningful:

```
stm_model <- stm(lesson_based_dfm,
            K = 6, 
            prevalence =~ created_class,
            data = sample_posts_about_jan6, 
            max.em.its = 1000, 
            seed = 1234, 
            init.type = "Spectral")
```

Notable here, though is the `prevalence` of `created_class`, which I am eager to see the results of in the final analysis. I will need to change `created_class` from a factor variable with 3 values to a boolean, probably with 0 for pre-6th posts and 1 for post-6th posts (likely omitting posts on the 6th itself, unless I decide it makes sense to include that in one set or the other).

Finally, running the sample K-finding code on our tiny sample produces the following:

```
differentKs <- searchK(lesson_based_dfm, 
        K = c(5,25,50), 
            prevalence =~ created_class,
            data = sample_posts_about_jan6, 
            max.em.its = 1000, 
            seed = 1234, 
        N = 300,
            init.type = "Spectral")

plot(differentKs)
```

(Note: I had to force an `N` of 300 to get this code to run, as the number of posts was smaller than the `max.em.its` value, but this will not be necessary in the full run)

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/linberg-post-5-ks.png?raw=TRUE)

This is not granular enough to go on; I'd like to test more values, but this will take a lot of time to run on the full set, so I'll have to be strategic about it.

Once this is done, we can return to the process and rebuild the model with a more appropriate number of topics, and then see what we get with the prevalence, and whether we can detect a shift in sentiment.

Finally, I've seen a lot of warnings in the literature that STMs and LDA don't work well with small data objects like tweets; Parler posts, at up to 1000 characters (at least as it existed at the time of this research), can be longer than tweets (280 characters), but often aren't. I came across some of the work of [Christopher Bail](https://www.chrisbail.net) at Duke University, who has addressed this and recommends a library called [Single-topic LDA](https://github.com/g-tierney/stLDA-C_public), which is optimized for this kind of data, presuming that each post is on a single topic, and clustering by users. I've looked over the sample code, and adapting it for this data set will be challenging; it's pretty advanced R, and I'm still trying to bootstrap into some of the core concepts. Using this library before the end of the semester will be a heavy lift, and I'm not sure whether I'll be able to pull it off in the time remaining; I'll make a decision once I get the above code run against the current data set and seeing whether the results provide a satisfactory answer to the question of pre- and post-1/6 sentiment shift on Parler, or whether I need to push harder into more bleeding-edge methods.
