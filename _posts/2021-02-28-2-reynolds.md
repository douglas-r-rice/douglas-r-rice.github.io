---
layout: post
title: "Scraping Podcast Data (Reynolds)"
author: Nate Reynolds
date: 2021-02-28 12:24:54 -0500
categories: reynolds
---

```{r include=FALSE}
library(tidyverse)
spotify_podcasts <- read.csv("spotify_metadata.csv")
scraped_podcasts <- read.csv("scraped_podcasts.csv")
```

## Getting the Data into R

As I described in my first blog post, in my project I am analyzing podcast metadata to investigate what networks of podcasts interview the same guests. Because there is no existing dataset on this subject, this data collection step was extremely involved - and surprisingly successful. After many attempts with different online resources, APIs, and Python scripts, I was able to scrape metadata from 28,830 unique podcast episodes from the last six months from 1,716 unique shows. 

First, I should clarify *why* no dataset was available for me to use. There is some data available online about podcasts; there are a lot of public podcast-listening statistics, there are online, searchable podcast databases and, as I discovered, there are multiple APIs either somewhat able to or wholly created to search for podcasts. But there is a lot in between those resources and the analysis I want to conduct. The first challenge is that I am interested in episode-level data; the second challenge is the sheer number of podcast episodes that exist. 

### Toying with a Spotify Dataset

I thought I had found a miraculous solution in the [Spotify Podcast Dataset](https://podcastsdataset.byspotify.com/), a free dataset of 100,000 podcast episodes and their metadata (including their audio, if you can download all 2TB of it) compiled and offered by Spotify for research purposes. I requested access and was eventually able to download the dataset. 

```{r}
colnames(spotify_podcasts)
dim(spotify_podcasts)
head(spotify_podcasts$show_name)
```

It's an awesome resource with a ton of data. It's good to have and I may still use it. But I quickly realized that it wasn't a perfect fit for my research question. Firstly, because Spotify sampled across many genres, this dataset included everything from *The Keeping Up with the Kardashians Podcast* to *WHAT ABOUT BIGFOOT?*. I'm all for a large, representative sample, but for a project about interview podcasts, this sample seemed irrelevant. Furthermore, because almost every episode in the dataset is from a unique show, it would be very difficult for me to see what variety of guests are brought onto the *same* show. I need data on *all* the episodes within a certain time period for different shows.

Citation:

“100,000 Podcasts: A Spoken English Document Corpus” by Ann Clifton, Sravana Reddy, Yongze Yu, Aasish Pappu, Rezvaneh Rezapour, Hamed Bonab, Maria Eskevich, Gareth Jones, Jussi Karlgren, Ben Carterette, and Rosie Jones, COLING 2020

https://www.aclweb.org/anthology/2020.coling-main.519/

### API Scraping with Python

So, I went back to Plan A and decided to get the data myself using APIs and Python. I tried using the Spotify API but gave up on that due to its complexity; it seemed like it was made for web and software developers trying to create apps that played music from Spotify. I instead found an [API offered by PodcastIndex.org](https://podcastindex.org/), an independent online podcast database, and a Python wrapper class for it called [Python-PodcastIndex](https://pypi.org/project/python-podcastindex/#episode_of_a_podcast). After a bit of work to initialize it, this API and wrapper were pretty easy to use.

I also installed SpaCy, a Named Entity Recognition library for Python. After some serious Googling, I learned how to extract names out of strings with a custom function called `find_persons()`. 

Ater a lot of trial and error, what came next was a gargantuan series of loops in Python to scrape podcast metadata. It started with searching for a single podcast; a starter of sorts. At least for now, I had to somewhat-arbitrarily pick one, so I chose the *Tim Ferriss Show*, one of the most popular interview podcasts on the internet with very frequent episodes and one with fairly down-the-center, if not apolitical guests. So in a way this dataset is an extended- Tim Ferriss network, but that's not really the point. At some point I may run it again with a different starter. I also had the idea of running it two more times, once starting with a left-leaning podcast and once with a right-leaning podcast, to try to assemble two different networks, and to see if, when and where they ever intersected. But that will be for another time.

To illustrate how much time this took both to write and run, here is the Python code I used for scraping:

```{python eval=FALSE, python.reticulate=FALSE}
# run request
episoderesults = index.episodesByFeedId(739525, since=1598932800, max_results = 100)
e_searchcount = episoderesults['count']

# blank dictionary
episodedata = {'title': [], 'description': [], 'datePublished': [], 
  'showID': [], 'showName': [], 'names': []}

# find data for i number of episodes in original search
for i in range(0, e_searchcount): 
  title = episoderesults['items'][i]['title']
  description = episoderesults['items'][i]['description']
  datePublished = episoderesults['items'][i]['datePublished']
  showId = episoderesults['items'][i]['feedId']
  # Appends a element at an index 
  episodedata['title'].append(title)
  episodedata['description'].append(description)
  episodedata['datePublished'].append(datePublished)
  episodedata['showID'].append(showId)
  # Appends the name of the podcast
  showName = index.podcastByFeedId(showId)['feed']['title']
  episodedata['showName'].append(showName)
  # Find names
  episodenames = find_persons(title)
  episodedata['names'].append(episodenames)
  nnames = len(episodenames)

  # search for each name (j number of names per i episode)
  for j in range(0, nnames):
    j_searchname = episodenames[j]
    j_searchresult = index.search(j_searchname)
    j_searchcount = j_searchresult['count']

    # look up a podcast for each name (k number of podcasts per j name per i episode)
    for k in range(0, j_searchcount):
      k_showId = j_searchresult['feeds'][k]['id']
      k_episoderesults = index.episodesByFeedId(k_showId, since =1598932800, max_results = 100)
      k_searchcount = k_episoderesults['count']

      # append data for each episode of each podcast (l number of episodes 
          # per k podcast per j name per i episode)
      for l in range(0, k_searchcount): 
        l_title = k_episoderesults['items'][l]['title']
        l_description = k_episoderesults['items'][l]['description']
        l_datePublished = k_episoderesults['items'][l]['datePublished']
        l_showId = k_episoderesults['items'][l]['feedId']
        # Appends element at an index 
        episodedata['title'].append(l_title)
        episodedata['description'].append(l_description)
        episodedata['datePublished'].append(l_datePublished)
        episodedata['showID'].append(l_showId)
        # Appends the name of the podcast
        l_showName = index.podcastByFeedId(l_showId)['feed']['title']
        episodedata['showName'].append(l_showName)
        # Find names
        l_episodenames = find_persons(l_title)
        episodedata['names'].append(l_episodenames)
        l_nnames = len(l_episodenames)

        # search for each name (m number of names per l episode)
        for m in range(0, l_nnames):
          m_searchname = l_episodenames[m]
          m_searchresult = index.search(m_searchname)
          m_searchcount = m_searchresult['count']

          # look up a podcast for each name (n number of podcasts per m name per l episode)
          for n in range(0, m_searchcount):
            n_showid = m_searchresult['feeds'][n]['id']
            n_episoderesults = index.episodesByFeedId(n_showid, since =1601524800, max_results = 100)
            n_searchcount = n_episoderesults['count']
            
            # append data for each episode of each podcast (o number of episodes 
              # per n podcast per m name per l episode)
            for o in range(0, n_searchcount):
              o_title = n_episoderesults['items'][o]['title']
              o_description = n_episoderesults['items'][o]['description']
              o_datePublished = n_episoderesults['items'][o]['datePublished']
              o_showID = n_episoderesults['items'][o]['feedId']
              # Appends element at an index 
              episodedata['title'].append(o_title)
              episodedata['description'].append(o_description)
              episodedata['datePublished'].append(o_datePublished)
              episodedata['showID'].append(o_showID)
              # Appends the name of the podcast
              o_showName = index.podcastByFeedId(o_showID)['feed']['title']
              episodedata['showName'].append(o_showName)
              # Find names
              o_episodenames = find_persons(o_title)
              episodedata['names'].append(o_episodenames)
```

The search quickly expands far beyond the starter podcast. The first loop appends a dictionary with the title, description, and date published of every episode in the podcast. For now, I chose to look only at podcasts published since September 1, 2020, getting approximately six months of data. The loop then uses SpaCy to identify any names included in the episode titles. A second loop runs a search for any of the names included in the title of the first episode. It then searches, one by one, for any podcast episodes with those names in their title, then appends the metadata for each the episodes in the last six months of *that* podcast. And *then*, I had the nerve to write another set of loops to find names in every one of *those* episodes and search for *more* podcasts. In total, this loop and its six inner loops searches with three degrees of separation away from the starter podcast. As you can imagine, this took a *long* time to run. In fact, it's still running as I write this...

Three hours later, it finished, and I was able to move on. By some miracle it worked on the first try with this many loops, producing metadata from 70,104 podcast episodes. However, due to the generic names it recognized and searched for along the way, it scraped a lot of duplicate episodes. I was able to condense the dataset by reducing it to entries with a unique showID and a unique date published (recorded with a unix timestamp; two different episodes of the same podcast would have had to be published in the same second to be collapsed). 

```{r}
dim(scraped_podcasts)
unique_podcasts <- scraped_podcasts %>%
  group_by(showID, datePublished) %>%
  slice(1) %>%
  ungroup()
length(unique(unique_podcasts$showID))
head(unique(unique_podcasts$showName))
```

The result of this are 28,830 unique podcast episodes from 1,716 unique shows. From visual inspection, I think I made a lot of great ground; I saw a variety of names from Malcolm Gladwell to Glenn Beck. As you can see in the preview, there's also a fair amount of data from non-English podcasts (a lot of German, it seems, for whatever reason), so I will have to filter those out at some point. 

As I said, I may run this Python script again starting with different podcasts to produce more data. But for the time being, I now have a dataset ready in R and can start working on Natural Language Processing code to identify guests on podcasts from their episode titles and descriptions. 
