---
layout: post
title: "Toxic Comments Classification (Alvandi)"
author: Amirhossein Alvandi
date: 2021-04-18 12:24:54 -0500
categories: alvandi
---

<style>
body {
text-align: justify}
</style>


Nowadays, it is almost impossible to engage in online conversations without witnessing toxic behavior like unwarranted harassment or disrespectful behavior. Conversational toxicity is a growing issue that can lead people to stop genuinely expressing themselves and give up on seeking others' opinions out of fear of abuse or harassment. Because of the massive volume of comments, it has become more critical to find a practical solution to identify and classify toxic comments in a more efficient manner. For instance, New York Times and Perspective API have developed a moderator tool that can find patterns, spot abusive language, and score all comments based on their toxicity level. Accordingly, they can provide live feedback to commenters and help human moderators sort comments much faster.

In this project, we will build a model capable of detecting toxic language inside comments and distinguishing between different types of such content, including obscenity, threat, or other toxicity. Basically, the created model will predict a probability for each comment belonging to each kind of toxicity category. To this end, we will apply various classification techniques, and by analyzing the number of false negatives and false positives, we get insights about open challenges that all of the approaches share.

## Corpus

We use the Wikipedia Comment Dataset containing 159,563 annotated user comments collected from Wikipedia talk pages and is the largest publicly available for the task. These comments were annotated by human raters with the six labels ‘toxic’, ‘severe toxic, ‘insult’, ‘threat’, ‘obscene’ and ‘identity-hate’. Comments can be associated with multiple classes at once, which frames the task as a multi-label classification problem.

## Importing data

Here, we import the csv file of data in R. The test data set which is consisted of only two columns (comment id, and comment text) will be used later when we compare the accuracy of various models in predicting the toxicity probabilities of the comments.

```
#install.packages("devtools")
#install.packages("tidytext")
#install.packages("plyr")
#install.packages("tidyverse")
#install.packages("quanteda")
#install.packages("randomForest")
#install.packages("xgboost")

# load libraries
library(devtools)
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(xtable)
library(caret)
library(gridExtra)
library("quanteda.textmodels")
library(caret)
library(e1071)
library(randomForest)
library(xgboost)
library(keras)
```



```
# Set working directory
setwd("/Users/amirhosseinalvandi/UMass/Courses/POLISCI797TA")
### import the training csv file
train_data = read_csv("Data/train.csv")
# getting the dimmension of the training data
dim(train_data)
```

As we can see, there are a total of 159571 comments in the training data set. For each of the comments there exist 6 different labels indicating the type of toxicity of the given comment. Let us have a look at the beginning rows of our data set.

```
# viewing the first 5 rows of the data
head(train_data,n = 5)
```

We can observe in the above cell that the first two columns contain the comment id and comment text, and each of the next six columns contain a binary random variable where we have "1" indicating that we have some type of a toxicity in the corresponding row, and zero otherwise. Now, we may plot a bar chart of the data in order to better understand the distribution of the various kinds of toxicity.

```
### Finding the counts for each type of toxicity
toxic_labels = colnames(train_data[3:8])
toxic_counts = as.numeric(colSums(train_data[,3:8]))
bar_data = tibble(cbind(toxic_labels = toxic_labels, toxic_counts = toxic_counts))
### Plotting the histogram of various toxicity types
ggplot(data = bar_data, aes(x = toxic_labels, y = toxic_counts, fill = toxic_labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Toxicity Type")
```


![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_5_fig1.png?raw=TRUE)

From the bar plot, it is apparent that we have the largest proportion for the toxic comments, however that only covers about 10 percent of the total number of comments in this data set, and the smallest count is for comments including a threat. 


## Data Pre-processing

Data pre-processing is one of the important steps in any text data analysis task. Here, using the 'tokens' function, we removed all punctuation marks, numbers, urls, and English stop words. All characters then were converted into lower case to avoid repetitions in the set of unique words. An example of both raw and pre-processed versions of the first commment is provided below.  

It is also useful to find out how many multi-label comments are present in our data. Since, when hand labeling the comments, one's content can be considered  obscene and insulting at the same time.


```
# Find the number of comments with more than one toxicity type
multi_label = 0
for(i in 1:nrow(train_data)){
  if(sum(train_data[i,3:8]) > 1){
    multi_label = multi_label + 1   
  }
}

multi_label
```

As we can observe, 9865 from the total of 15294 toxic comments have multiple toxicity labels.

## Subsampling for Class Imbalances

It is apparent that we have a high level of class imbalance in our data set, where only about 10% of the total comments are toxic. Here, the detection of toxic comments is our primary interest; Therefore, it is very likely that a classification model would be able to achieve very good specificity since about 90% of the comments are non-toxic. Sensitivity, however, would likely be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.

One way to alleviate this issue is to sub-sample the data. There are a number of ways to do this but the most simple one is to sample down the majority class data until it occurs with the same frequency as the minority class. While it may seem counter-intuitive, throwing out a large percentage of your data can be effective at producing a useful model that can recognize both the majority and minority classes. In some cases, this even means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are better calibrated, meaning that the distributions of the class probabilities are more well behaved. As a result, the default 50% cutoff is much more likely to produce better sensitivity and specificity values than they would otherwise.


```
# Find the proportion of toxic comments
table(train_data$toxic)
# Downsmaple the non-toxic comments (majority class)
non_toxic_index = train_data$id[train_data$toxic == 0]
toxic_index = train_data$id[train_data$toxic == 1]
set.seed(123)
downsample_non_toxic_index = sample(train_data$id[train_data$toxic == 0], size = 15294)
train_data_balanced_id = c(downsample_non_toxic_index, toxic_index)
train_data_balanced = filter(train_data,train_data$id %in% train_data_balanced_id)
# Distribution of comments before downsampling
labels = c("toxic", "non-toxic")
counts = c(sum(train_data$toxic == 1), sum(train_data$toxic == 0))
bar_data_imb = tibble(cbind(labels = labels, counts = counts))
p1 = ggplot(data = bar_data_imb, aes(x = labels, y = counts, fill = labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Before Down-sampling")

# Distribution of comments after downsampling
counts_balanced = c(sum(train_data_balanced$toxic == 1), sum(train_data_balanced$toxic == 0))
bar_data_bal = tibble(cbind(labels = labels, counts_balanced = counts_balanced))
p2 = ggplot(data = bar_data_bal, aes(x = labels, y = counts_balanced, fill = labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "After Down-sampling")

grid.arrange(p1, p2, ncol = 2)
```


![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_5_fig2.png?raw=TRUE)


After down-sampling from the data, we have a total of 30588 comments. 
Now, in order to train and validate different classifiers, we split the training data into a train set (60% of all comments), a test set (20% of all comments), and a held out set (20% of all comments). Later, when we use the grid search for parameter tuning of the models, we will use a 5-fold cross validation to have more robust evaluation metrics.


```
# create training set (60% of data) and initial test set
set.seed(123)
N = nrow(train_data_balanced)
trainIndex = sample(1:N,.8 * N) 
testIndex = c(1:N)[-trainIndex]

# split test set in half (so 20% of data are test, 20% of data are held-out)
N_test = length(testIndex)
heldOutIndex = sample(1:N_test, .5 * N_test)
testIndex = testIndex[-heldOutIndex]

# pre-processing step
# create the dfm
subset_train =  train_data_balanced[trainIndex,]
train_dfm = dfm(subset_train$comment_text,
        tolower = TRUE,
        remove_punct = TRUE,
        stem = FALSE,
        remove = stopwords("english")
        )

# find out a quick summary of the dfm
train_dfm
```


```
# pre-processing step
# create the dfm
subset_test =  train_data_balanced[testIndex,]
test_dfm = dfm(subset_test$comment_text,
        tolower = TRUE,
        remove_punct = TRUE,
        stem = FALSE,
        remove = stopwords("english")
        )

# find out a quick summary of the dfm
test_dfm
```


```
subset_heldOut =  train_data_balanced[heldOutIndex,]
heldOut_dfm = dfm(subset_heldOut$comment_text,
        tolower = TRUE,
        remove_punct = TRUE,
        stem = FALSE,
        remove = stopwords("english")
        )

# find out a quick summary of the dfm
heldOut_dfm
```

In order to save some computational time, we only retain the features (words) that are used more frequently. To do so, we filtered out the words that appeared in less than 1% of all comments.


```
# Reduce the number of features for train set
smaller_train_dfm <- dfm_trim(train_dfm, min_docfreq = 0.01, docfreq_type = "prop")
smaller_train_dfm
```

As we can observe, by eliminating the infrequent terms from our training DFM, we managed to reduce the number of features from 54,263 to only 168 more commonly used words.

Now, we want to know how well the trained classifier performed. To do so, we need to keep only the words in our testing data that also appear in the training data, which only retains terms that appear in both corpora.

```
# Match the features that are present in both corpora
dfmTestMatched_small = dfm_match(test_dfm, features = featnames(smaller_train_dfm))
dfmTestMatched_small
```

## Term Frequency and Inverse Document Frequency (tf-idf)

One measure of the importance of a  word is its term frequency. Some words appear too many times in all documents that are not that important such as the set of pre-defined English stop words that we eliminated from our comments texts. The second approach of measuring the importance of a word is the term's inverse document frequency which assigns lower weight to commonly used words and higher weight to the words that are not used very often. These two measures can be combined to obtain the tf-idf for each word in our corpus. It is intended to measure how important a word is to a document in a collection (or corpus) of documents. The idf for each word can be calculated as follows,

 $$ idf(term) = ln(\frac{\text{number of documents}}{\text{number of documents containing term}})$$
Using this approach, we can control for the impact of terms with very high frequency in both classes (toxic and non-toxic), such as "Wikipedia" itself. 

```
# assign weights to each word in our dmf
tf_dfm_train =  dfm_tfidf(smaller_train_dfm)
tf_dfm_test =  dfm_tfidf(dfmTestMatched_small)
```


## Performance Evaluation Metrics

There are a wide range of metrics (loss functions) available in order to evaluate the performance of different classification models. Here, we used four different evaluation metrics as described below.


#### Accuracy 

Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy of a binary classifier is given by

$$ Accuracy = \frac{True \hspace{0.1cm} Positive + True \hspace{0.1cm} Negarive}{True \hspace{0.1cm} Positive + True \hspace{0.1cm} Negarive + False \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative} $$
In general, using accuracy for severely unbalanced classification problems could be misleading since even if a model naively classifies all data points in the major class, we would still get an accepatable accuracy. But, as we modified our data in order to have the same number of comments from both toxic and non-toxic class, we can rely on the accuracy of different classification models. 


#### F1 Measure

The F1 measure is the harmonic mean of the precision and recall. The highest possible value of an F measure is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. 

$$ F = \frac{2}{Recall^{-1} + Precision^{-1}} = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + \frac{1}{2}(False \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative)} $$
The more generic $F_{\beta}$ measure applies additional weights, valuing one of precision or recall more than the other.


#### Specifity

Specificity (True Negative rate) measures the proportion of negatives that are correctly identified. In a diagnostic test, specificity is a measure of how well a test can identify true negatives. Specificity is also referred to as selectivity or true negative rate, and it is the percentage, or proportion, of the true negatives out of all the samples that do not have the condition.

$$ Specifity = \frac{True \hspace{0.1cm} Negative}{True \hspace{0.1cm} Negative + False \hspace{0.1cm} Positive} $$

#### Recall

Recall (also called sensitivity) is the fraction of relevant instances that were retrieved or equivalently, the number of true positive results divided by the number of all samples that should have been identified as positive. 

$$ Recall = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative} $$



## Classification Methods

In order to predict the toxicity class for each comment using its words, we begin by classifying each comment as toxic or non-toxic (binary classification), and then, we use a multi-label classification to determine the type of toxicity for each of the toxic comments. Here, we provide a brief description of the different classification models that we utilize for our data analysis. 

#### Naive Bayes 

The Naive Bayes classifier is one of the most renowned classification models in text data analysis. Applying the Bayes theorem to calculate the probability that a new comment is toxic given that the comment content is given by,

$$Pr(Toxic|Comment's \hspace{0.1cm} content) = \frac{Pr(Comment's \hspace{0.1cm} content|Toxic)Pr(Toxic)}{Pr(Comment's \hspace{0.1cm} content)}$$
The probability of a comment being toxic ($Pr(Toxic)$) can be calculated as the proportion of toxic comments in our training data set. Here, the naive independence assumption let us to  compute the likelihood of a comment being toxic only by finding the product of the word appearing or not appearing in the toxic words pool. Then, the class of a new comment is given by

$$ \frac{Pr(Toxic|Comment's \hspace{0.1cm} content)}{Pr(Non-Toxic|Comment's \hspace{0.1cm} content)}$$
Now, in order to simplify the computations by taking log, we have

$$ \log (\frac{Pr(Toxic|Comment's \hspace{0.1cm} content) +0.5}{Pr(Non-Toxic|Comment's \hspace{0.1cm} content) + 0.5})$$
where the 0.5 corrections are  used to avoid division by zero. 

Here, we fit a binary Naive Bayes model for all different toxicity types, and compare the performance of the models using the listed evaluation metrics as well as their computational weight.  


```
# fit the Naive Bayes model to the train data
# Start the timer
start_time = proc.time()
NaiveBayes = textmodel_nb(x = tf_dfm_train,
                           y = train_data_balanced$toxic[trainIndex],
                           distribution = "Bernoulli") 
# End the timer
end_time = proc.time() - start_time
end_time
# create a confusion matrix 
actual_toxic = train_data_balanced$toxic[testIndex]
predicted_nb = predict(NaiveBayes, newdata = tf_dfm_test)
confusion_nb = table(actual_toxic, predicted_nb)

# now calculate a number of statistics related to the confusion matrix
NaiveBayes_summary = confusionMatrix(confusion_nb, mode = "everything")
NaiveBayes_summary
```


#### Support Vector Machine (SVM)

Support Vector Machine is a classification method that is based on the principle of finding hyper-planes that distinctly classify the data units when you plot them onto an $n$-dimensional graph where $n$ represents the number of features of the data. One of the main advantages of SVM classifiers is their effectiveness in high dimensional spaces which makes them a reliable candidate for our situation.


```
# Fit the SVM model to the smaller train set
# Start the timer
start_time = proc.time()
SVM = textmodel_svm(x = tf_dfm_train,
                           y = train_data_balanced$toxic[trainIndex],
                           distribution = "Bernoulli") 
# End the timer
end_time = proc.time() - start_time
end_time
# create a confusion matrix 
predicted_svm = predict(SVM, newdata = tf_dfm_test)
confusion_svm = table(actual_toxic, predicted_svm)
# now calculate a number of statistics related to the confusion matrix
SVM_summary = confusionMatrix(confusion_svm, mode = "everything")
SVM_summary
```


#### Random Forest

Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction. Here, we use mtry = 13 which refers to the number of variables available for splitting at each tree node. The default value for mtry for classification problems is set to the squared root of the number of features. Another important characteristic of random forest classifiers is the number of trees which is usually determined based on the size of the data. Later, when we want to obtain the optimal model for each toxicity class, we will perform parameter tuning for all different classifiers.


```
# Convert dfms to matrices 
dfmTrainSmallRf <- convert(tf_dfm_train, to = "matrix")
dfmTestMatched_smallRf <- convert(tf_dfm_test, to = "matrix")

# Fit random forest model to the training set
# Start the timer
start_time = proc.time()
RF = randomForest(dfmTrainSmallRf, 
                  y = as.factor(train_data_balanced$toxic[trainIndex]),
                  xtest = dfmTestMatched_smallRf, 
                  ytest = as.factor(train_data_balanced$toxic[testIndex]),
                  importance = TRUE,
                  mtry = 13,
                  ntree = 10)
# End the timer
end_time = proc.time() - start_time
end_time
```


```
# Predict labels for the test set
predicted_rf = RF$test[['predicted']]
confusion_rf = table(actual_toxic,predicted_rf)
rf_summary = confusionMatrix(confusion_rf, mode = "everything")
rf_summary
```


#### Gradient Boosted Decision Trees

Gradient boosted decision trees have been proven to be a powerful tool for classification problems. Using this algorithm, one goes through cycles that repeatedly build new models and combines them into an ensemble model. We start the cycle by taking an existing model and calculating the errors for each observation in the dataset. We then build a new model to predict these errors. We add predictions from this error-predicting model to the ensemble of models. Here, we use the xgboost classifier in R, which has a built-in datatype, DMatrix, that is particularly good at storing and accessing sparse matrices efficiently.

```
### Fit xgb-classifier
# preparing matrix 
dtrain = xgb.DMatrix(data = tf_dfm_train,label = train_data_balanced$toxic[trainIndex]) 
dtest = xgb.DMatrix(data = tf_dfm_test,label = train_data_balanced$toxic[testIndex])
# set model parameters
params = list(booster = "gbtree", objective = "binary:logistic", eta = 0.05, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 0.8)
# calculate the best nround for this model using cross validation
xgbcv = xgb.cv( params = params, data = dtrain, nrounds = 500, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgbcv$best_iteration
```

The model returned the lowest error at the 500 (nround) iteration. Also, if you noticed the running messages in your console, you would have understood that train and test errors are following each other. Now, we may fit the model using the optimal parameters that we obtained in gridsearch's parameter tuning.


```
# first default - model training
xgb1 = xgb.train (params = params, data = dtrain, nrounds = 500, watchlist = list(val=dtest,train=dtrain), print.every.n = 10, early.stop.round = 10, maximize = F , eval_metric = "error")
# model prediction
xgbpred = predict (xgb1,dtest)
xgbpred = ifelse (xgbpred > 0.5,1,0)
# Obtain confusion matrix
actual_toxic = train_data_balanced$toxic[testIndex]
confusion_xgb = table(actual_toxic, xgbpred)
# now calculate a number of statistics related to the confusion matrix
xgb_summary = confusionMatrix(confusion_xgb, mode = "everything")
xgb_summary
```


#### Neural Network classification

To fit a neural network, we must convert the comments to tensors by first creating a dictionary and represent each of the 1000 most common words by an integer. Consequently, each of the comments will be represented by a sequence of integers. To do so, we can pad the arrays so they would have the same length and then create an integer tensor of shape number of unique words \times the maximum length for comments. We can use an embedding layer that can handle this shape as the first layer of our neural network. 
In the next step, we need to "adapt" the text vectorization layer so that the layer could learn about unique words and assign an integer to each one. We should note that we are using the balanced training data for this classification method without eliminating the stop words. Therefore, the total number of unique words is relatively similar to the previous classification methods.


```
num_words = 1000
max_length = 50
text_vectorization = layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)

# adapt text vectorization layer
text_vectorization %>% 
  adapt(train_data_balanced$comment_text)
# obtain vocabularies in the text vectorization layer
# get_vocabulary(text_vectorization)
# check the transformation for first comment 
text_vectorization(matrix(train_data_balanced$comment_text[1], ncol = 1))
```

Now, we need to make two major architectural decisions to create our neural network. The first question is the number of layers we need to use for our network, and the second is to determine the number of hidden units we use for each layer. Here, we build a  model that takes an array of word indices and predicts each comment's label.


```
input = layer_input(shape = c(1), dtype = "string")

output = input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model = keras_model(input, output)
```


Here, we build our binary classifier by stacking layers sequentially. The first layer is an embedding layer that takes the integer encoded vocabulary and learns the embedding vector for each word index. The model trains and adds a dimension to the output array.
The resulting dimension will be (batch \times sequence  \times embedding). In the next step, a  "global_average_pooling_1d" layer returns a fixed-length output vector for each word by finding the average over the sequence dimension. This fixed output vector is piped through a fully connected layer with  16 hidden units. Also, the last layer is densely connected with a  single output node. We used a sigmoid activation function which returns the probability of the comment being toxic.
We also used the binary cross-entropy loss function, which is preferable to mean square error when dealing with probabilities in binary classification.


```
model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)

history = model %>% fit(
  train_data_balanced[trainIndex,]$comment_text,
  as.numeric(train_data_balanced[trainIndex,]$toxic == 0),
  epochs = 10,
  batch_size = 512,
  validation_split = 0.2,
  verbose = 0
)
```


Here, we used 10 iterations over all comments in our training set where we have mini batches containing 512 samples.

```
results = model %>% evaluate(train_data_balanced[testIndex,]$comment_text, as.numeric(train_data_balanced[testIndex,]$toxic == 0), verbose = 0)
results
```


```
nn_predict  = ifelse(model %>% predict(train_data_balanced[testIndex,]$comment_text, verbose = 0) > 0.5,0,1)
actual_toxic = train_data_balanced$toxic[testIndex]
confusion_nn = table(actual_toxic, nn_predict)
# now calculate a number of statistics related to the confusion matrix
nn_summary = confusionMatrix(confusion_nn, mode = "everything")
nn_summary
```

We can also visualize the changes in loss (cross-entropy) and accuracy for training and validation data in each epoch.

```
plot(history)
```


![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_5_fig3.png?raw=TRUE)


As we expected, the training accuracy is increased with each epoch using gradient descent optimization. However, we can see that the validation accuracy (over the test data) is stabilized for the last epoches.   


## Results and Concluding Remarks

In this project, we investigated the performance of five binary classifiers for toxic comment classification. We should note that a set of the same methods can be used for other types of toxicity. In addition, since we used down-sampling to have a balanced set of toxic and clean comments, we can compare the accuracy of all classifiers with high confidence. It should be noted all the classifiers considered clean labeled (toxic = 0) as positive. Below, we can see the results for all different classifiers:



![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_5_table1.png?raw=TRUE)



From the table, we can observe that the SVM, random forest, and xgboost classifiers have relatively similar accuracy, while the neural network classifier has substantially outperformed its counterparts. We have the same pattern for F-1 measures; however, xgboost has a slightly higher specificity than the neural network classifier. It is also worth noting that although we used the Naive Bayes classifier as the benchmark model, it has a recall rate close to our best classifier.























