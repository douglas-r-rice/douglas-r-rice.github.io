---
layout: post
title: "Analyzing Frequencies and Relationships Between Words Across the Amicus Briefs (Virgüez)"
author: Santiago Virgüez
date: 2021-03-28 12:24:54 -0500
categories: virguez
---


We continue working with the sample of 363 abstract constitutional cases and ~ 1,125 amicus briefs filed by different citizens and interest groups. An interesting feature about this sample is that it coould be organized as a tidy data where (i) each variable is a column, (ii) each observation is a row, and (iii) each type of observational unit is a table. As explained by Silge and Robinson (2017), a tidy text format could be defined as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. The token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the previous post, I worked with the corpus objects of the summaries of the amicus briefs and created 'dmf s' to have a look into the text-data, in this post I use a **tidy approach** to provide a better analysis of the content of the briefs.

## Word Counts
After uploading the data and the packages necessary for the analysis, I begin by trimming the text and creating a customized set of stop words in Spanish (I use a website which has a pretty decent list of stop wprds in Spanish, much better than the default one)

```{r}
library(rvest)
library(tidytext)
library(dplyr)
library(tidyr)
library(stringi)
library(tm)
library(textclean)

AMICUS <- read.csv("AmiBriefs.csv")
AMICUS$SUMMARY <- trimws(AMICUS$SUMMARY)  

#Custom stop words in Spanish
custom_stop_words <- read.table("https://raw.githubusercontent.com/stopwords-iso/stopwords-es/master/stopwords-es.txt", stringsAsFactors = F)
custom_stop_words$lexicon = "custom"
colnames(custom_stop_words)[1] = "word"
```

Then, I use the function ('unnest_tokens') to tokenize, in words, the text of the summaries of the amicus briefs. In the process, I exclude a list of words that are not stop words in Spanish but are commonly used in the legal document and which do not provide any interesting information. As you can imagine, when we talk about lawyers writing petitions you will see a great amount of unnecesary words that could affect the word counts. It is difficult to exclude all the most common words, but I have excluded almost 100. You can see some of them in the following code.

```{r}
token_counts <- AMICUS %>% 
  unnest_tokens(word, SUMMARY) %>%
  filter(!(nchar(word) < 4)) %>% 
  anti_join(custom_stop_words) %>%
  count(AMICI_NAME, word, sort = TRUE)
```

Because I am interested in the differences among the 'types' actors that filed amicus briefs, I did the word count by the type of amici filing the brief. The amici actors for this sample are divided into 6 different categories: 

1. Common citizens and individual lawyers 
2. NGOs 
3. Labor unions
4. Trade guilds
5. Think Tanks
6. Universities

We can plot a word proportion of the amicus briefs by each type of actor. 
```{r}
token_counts <- AMICUS %>% 
  unnest_tokens(word, SUMMARY) %>%
  filter(!(nchar(word) < 4)) %>% 
  filter(!(nchar(word) < 4)) %>% 
  filter(!(word=="artículo")) %>%
  filter(!(word=="constitución")) %>%
  filter(!(word=="derecho")) %>%
  filter(!(word=="constitucional")) %>%
  # ...
  anti_join(custom_stop_words) %>%
  count(AMICI_TYPE, word, sort = TRUE)
```

We can then organize a word proportion of the amicus briefs by each type of actor. 

```{r}
word_proportions <- token_counts %>%  
  group_by(AMICI_TYPE) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(AMICI_TYPE, proportion) %>%
  drop_na
```

And use 'ggplot' to plot the word proportion for ech of the categories of amici actors.

```{r}
library(ggplot2)

word_proportions %>% 
  top_n(10, `CITI`) %>%
  mutate(word = reorder(word, `CITI`)) %>%
  ggplot(aes(word, `CITI`)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

word_proportions %>% 
  top_n(10, `NGO`) %>%
  mutate(word = reorder(word, `NGO`)) %>%
  ggplot(aes(word, `NGO`)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-4-picture1.png?raw=TRUE)
![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-4-picture2.png?raw=TRUE)


As you can see, there is a higher use of the words "servicios" (services) and "libertad" (freedom) by the interventions filed by common citizens, which is consistent with the literature about how common citizens in Colombia file constitutional petitions mostly to ask for access to public services and for the protection of their individual liberties. Likewise, the NGOs usually use words as "internacional" (international) and "humanos" (human) which refers to the language of human rights and international law.

We can also use the package 'scales' to compare the word proportions of two types of amici actors. Here we can see a comparison between two of the considered 'expert' interveners: think tanks and universities.

```{r}
library(scales)

ggplot(word_proportions, aes(x = `THINK`, y = `UNI`, color = abs(`UNI` - `THINK`))) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_smooth(color = "black") +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-4-picture3.png?raw=TRUE)

There is still a lot of noise when we see some of the words in the plot but at least know we can have more 'valuable' information about the content of the amicus briefs.

## Term frequencies
As we have seen, the term frequency (how frequently a word occurs in a document) is way in which we quantify what a document is about. A good way to deal with the mentioned noise is  using a term’s 'inverse document frequency', which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents (Silge and Robinson, 2017). The statistic tf-idf, as Silge and Robinson (2017) explain, is intended to measure how important a word is to a document in a collection of documents.

```{r}
library("quanteda.textstats")
total_words <- token_counts %>% 
  group_by(AMICI_TYPE) %>% 
  summarize(total = sum(n))

amicus_words <- left_join(token_counts, total_words)

amicus_words <- amicus_words %>%
  bind_tf_idf(word, AMICI_TYPE, n)

amicus_words %>%
  select(-total) %>%
  arrange(desc(tf_idf)) %>% 
  head()

```

We can plot this to see the most important words used by each of the amici categories.

```{r}
amicus_words %>%
  filter(AMICI_TYPE %in% c("NGO", 
                           "THINK", 
                           "UNI",
                           "CITI", 
                           "TRADE", 
                           "LABOR")) %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(AMICI_TYPE) %>% 
  top_n(10) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = AMICI_TYPE)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~AMICI_TYPE, ncol = 2, scales = "free") +
  coord_flip()
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-4-picture4.png?raw=TRUE)
