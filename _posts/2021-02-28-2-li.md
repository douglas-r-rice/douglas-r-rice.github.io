---
layout: post
title: "Covid Vaccine Conspiracy Theories Corpus (Li)"
author: Zichao Li
date: 2021-02-28 12:24:54 -0500
categories: li
---

To collect the covid vaccine conspiracy theories circulationg on Reddit, I came up with a term list "covid vaccine, coronavirus vaccine, RNA vaccine, Pfizer, Moderna, Oxford vaccine, Oxford Astrazeneca". It covers vaccine category and several main covid vaccine to be more inclusive. 

## Potential solution 1
The ideal way to search posts and comments containing the key terms above is to use or operator to searh them all at once, the codes are as follows. 

```{r}
library("RedditExtractoR")
library(writexl)

df<-get_reddit(search_terms = "covid vaccine|corona vaccine|RNA vaccine|Pfizer|Moderna|Oxford vaccine|Oxford Astrazeneca", regex_filter = "", subreddit = "conspiracy",
           cn_threshold = 0, page_threshold = 1, sort_by = "comments",
           wait_time = 2)
```

This approach doesn't work for some reason. I found out that the or operator is not working well. The more terms I included, the fewer results I get in the output. I haven't found out the reason yet, so for now, I used another way around.

## Potential solution 2
I searched all 7 terms one by one and then combined them together, then deleted the duplicated items. This approach works, but it is more like a compromise and has its own downsides, which is how to detect the duplicated elements accurately. 

```{r}
library(RedditExtractoR)
library(writexl)
library(methods)

df1<-get_reddit(search_terms = "covid vaccine", regex_filter = "", subreddit = "conspiracy",
                cn_threshold = 0, page_threshold = 1, sort_by = "comments",
                wait_time = 2)

df2<-get_reddit(search_terms = "corona vaccine", regex_filter = "", subreddit = "conspiracy", cn_threshold = 0, 
               page_threshold = 1, sort_by = "comments", wait_time = 2)

df3<-get_reddit(search_terms = "RNA vaccine", regex_filter = "", subreddit = "conspiracy", cn_threshold = 0, 
                page_threshold = 1, sort_by = "comments", wait_time = 2)

df4<-get_reddit(search_terms = "Pfizer", regex_filter = "", subreddit = "conspiracy",
               cn_threshold = 0, page_threshold = 1, sort_by = "comments",
               wait_time = 2)

df5<-get_reddit(search_terms = "Moderna", regex_filter = "", subreddit = "conspiracy",
                cn_threshold = 0, page_threshold = 1, sort_by = "comments",
                wait_time = 2)

df6<-get_reddit(search_terms = "Oxford vaccine", regex_filter = "", subreddit = "conspiracy",
                cn_threshold = 0, page_threshold = 1, sort_by = "comments",
                wait_time = 2)

df7<-get_reddit(search_terms = "Oxford Astrazeneca", regex_filter = "", subreddit = "conspiracy",
                cn_threshold = 0, page_threshold = 1, sort_by = "comments",
                wait_time = 2)
```


And then combined all the separate files together, df will be a dataframe containing 29721 results. 

```{r}
df <- rbind(df1,df2,df3,df4,df5,df6,df7)
df 
```


First I tried to identify them by filtering URLs, but it turns out that all the comments under one post share the same URL with the main post. 

Then I turned to sort it by the column of comments. But the thing is all the original posts are marked as something quite long beginning by "###[Meta] Sticky comments...". If I delete all the duplicated items, then all the origianl posts will be deleted, too. 

So I need to make "###[Meta]..." as an exception from being identified as duplicated. 

### Solution 1

The duplicate () or unique() has an incomparables parameter that prevents x from being identified as duplicated, but this function is "not used yet". 

```{r}
test <- duplicated(df$comment, incomparables="NA")
test
```

### Solution 2
I used duplicated() and the | operator together to keep the original post in the data set. 
```{r}
#First use grep() to replace the "###[Meta]..." thing with something alphabetical that can be detected by duplicated(), saying "NA".
df$comment[grep("*I am a bot", df$comment)] <- "NA"
uniquedf <- df[!duplicated(df$comment)|df$comment=="NA", ]
````

That gives us a data frame called uniquedf containing 23949 results. 

This solution works, but it has tradeoff. Those original posts that are collected for multiple times remain in the dataset. However, having duplicated original posts is better than having both duplicated original posts and their comments. Commpared with the size of dataset, it is not a big number. There still is noise, but I hope it won't be a big issue...fingures crossed. 

# Random thoughts

## Debunking or spreading?
I thought r/conspiracy would be a community dedicated for producing and spreading conspiracy theories. But in the data set there are also posts debunking or questioning conspiracy theories. 

Maybe a classifier can be trained later to categorize the two types of posts and compare them.

But at the same time, debunking or questioning conspiracy theories might also trigger more discussion, which turns out to spread or reinforce conspiracy theories.

Also this might have something to do with reddit this online community. It is an information aggregation website with heterogeneous user group, not a isolated and closed circle that proactively alienats itself from others like QAnon chat groups. These could be two different conpsiracy talking production patterns. Maybe a network analysis helps to look at how users interact with each other in this subreddit.

## There is always compromise and tradeoff...

This is something I learned in my research method class in my first semester, now I feel that it is so true that method always comes with tradeoff and I need to compromise sometimes, whether it's becasue of realistic situations, or shortage in personal skills. When it comes to coding, it's the same thing. All we can do is to trust the process. 