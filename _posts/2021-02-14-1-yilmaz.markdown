---
layout: post
title: "Yilmaz: Changes in Language Use on r/Wallstreetbets"
author: Gamze Yilmaz
date: 2021-02-14 12:24:54 -0500
categories: yilmaz firstpost
---

The social identity model of deindividuation effects (SIDE; Reicher, Spears, & Postmes,1995) contends that when virtual group members are anonymous, they engage in depersonalization, and start to perceive each other as representatives of social categories rather than individual identities (Reicher et al., 1995). The salience of social categorization in this context leads to high attraction and more conformity to contextual group norms (Spears & Lea, 1994). It is well known that contextual group norms can have an important impact on real-time interactions of group members. For instance, one study showed that group norms shape the length of messages, use of paralanguage, and the personal references (Postmes et al., 2001). Another study found that when a group was primed with an efficiency norm rather than a socioemotional norm, their use of task-oriented words increased (Postmes, Spears, & Cihangir, 2001; Postmes et al., 2001). These findings confirm that language is shaped by situational group norms that are linked to emergent social categories. One recent event that can be examined through the lens of the SIDE model is the GameStop short squeeze fulfilled by amateur traders on Reddit.

# Case Description for the Project 

Reddit is one of the most popular social networking websites in the United States with almost 48 million monthly active users (Statista, 2019). Wallstreetbets is a subreddit forum on Reddit where members share, discuss and post information and tips for investing in the stock market. Users can vote on the submissions by “up” or “down” voting or respond to others’ comments. Recently, redditers in this forum organized and collectively invested in GameStop shares, leading to one of the biggest short squeeze success stories in history. After GameStop gained value by 1,600% in under a month, on January 28th - the peak of the reddit trader success - GameStop started to lose value again. The collective glue that kept reddit traders together and convinced them to hold onto their shares started to unravel. This study will examine the changes in the language use of redditors on Wallstreetbets during the two-week period after the peak of GameStop short squeeze. Additionally, we will examine the initial emotional reactions and the changing sentiments in the Wallstreetbets community as GameStop started to lose value. Using the SIDE Model as a theoretical framework, we will attempt to reveal the connections between the changing contextual norms in the Wallstreetbets community and the reflections of this change on collective language use. 

# Research Questions

*Research Question 1*: How does the language use on Wallstreetbets change after the peak of GameStop short squeeze? 
*Research Question 2*: How does the language use on Wallstreetbets reflect changing emotional reactions after the peak of GameStop short squeeze?
*Research Question 3*: What is the relationship between contextual norms on Wallstreetbets and language use? 
	
# Dataset

We acquired a publicly available dataset for this project from Kaggle.com. The dataset includes all primary comments posted on r/wallstreetbets from January 28 to February 11, 2021. The corpus created using the dataset consists of 32679 documents and 7 document-level variables.

# Methodological Considerations

One possible way to look at the language use and changing topics over the two-week period is to use Latent Dirichlet Allocation (LDA). LDA can be used to identify patterns, themes, and structures of the Wallstreetbets posts and examine how these themes are connected. Similarly, sentiment analysis of the topics can shed light on the changing emotions in the community as the markets reacted to the short squeeze. 

# References 
Postmes, T., Spears, R., & Cihangir, S. (2001). Quality of decision making and group norms. Journal of Personality and Social Psychology, 80, 918–930. doi:10.1037//0022-3514.80.6.918 

Postmes, T., Spears, R., & Lea, M. (2000). The formation of group norms in computer-mediated communication. Human Communication Research, 26, 341–371. doi:10.1093/hcr/26.3.341 

Postmes, T., Spears, R., Sakhel, K., & De Groot, D. (2001).Social influence in computer-mediated communication: The effects of anonymity on group behavior. Personality and Social Psychology Bulletin, 27, 1243–1254. doi:10.1177/01461672012710001

Reicher, S. D., Spears, R., & Postmes, T. (1995). A social identity model of deindividuation phenomena. European Review of Social Psychology, 6, 161–198. doi:10.1080/1479277944
3000049

Spears, R., & Lea, M. (1994). Panacea or panopticon? The hidden power in computer-mediated communi- cation. Communication Research, 21, 427-445. doi:10.1177/104649649903 000202

Statista (2019). Reddit - Statistics & Facts. https://www.statista.com/topics/5672/reddit/ 

Initial Code used to import data into R and to do preliminary pre-processing:

```{r}
# load libraries #not all the loaded libraries here were used in the below code
library(devtools)
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
install.packages("readtext")
require(readtext)
install.packages("wordcloud")
require(wordcloud)
library(quanteda.textplots)

#import the pre-formatted csv file and create a corpus using the text data

path_data <- system.file("/Users/gamze.yilmaz/Desktop/Amherst/Text_As_Data/Datasets for Research Paper/reddit.xls", package = "readtext")
my_corpus <- readtext(paste0(path_data, "/Users/gamze.yilmaz/Desktop/Amherst/Text_As_Data/Datasets for Research Paper/reddit.xls"), text_field = "title")
print(my_corpus)
my_corpus_text <- my_corpus$text

#create a document-feature matrix (dfm) from a corpus

my_corpus_text_dfm <- dfm(my_corpus_text, tolower =TRUE, stem = FALSE, remove_punct = TRUE, remove = stopwords("en"))
my_corpus_text_dfm

#Tabulate feature frequencies from a dfm

textstat_frequency(my_corpus_text_dfm) 
topfeatures(my_corpus_text_dfm)

#create a wordcloud to get a sense of the word frequencies 
#need to clean all non-english characters, emojis, and links

textplot_wordcloud(my_corpus_text_dfm)
```


