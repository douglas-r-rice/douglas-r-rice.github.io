---
layout: post
title: "Alvandi: Toxic Comments Classification"
author: Amirhossein Alvandi
date: 2021-02-14 12:24:54 -0500
categories: alvandi firstpost
---

<style>
body {
text-align: justify}
</style>

Nowadays, It is almost impossible to engage in online conversations without witnessing toxic behavior like unwarranted harassment or disrespectful behavior. Conversational toxicity is a growing issue that can lead people to stop genuinely expressing themselves and give up on seeking others' opinions out of fear of abuse or harassment. Because of the massive volume of comments, it has become more critical to find a practical solution to identify and classify toxic comments in a more efficient manner. For instance, New York Times and Perspective API have developed a moderator tool that can find patterns, spot abusive language, and score all comments based on their toxicity level. Accordingly, they can provide live feedback to commenters and help human moderators sort comments much faster.

In this project, we will build a model capable of detecting toxic language inside comments and distinguishing between different types of such content, including obscenity, threat, or other toxicity. Basically, the created model will predict a probability for each comment belonging to each kind of toxicity category. To this end, we will apply various classification techniques, and by analyzing the number of false negatives and false positives, we get insights about open challenges that all of the approaches share.

We use the Wikipedia Comment Dataset containing 159,563 annotated user comments collected from Wikipedia talk pages and is the largest publicly available for the task. These comments were annotated by human raters with the six labels ‘toxic’, ‘severe toxic, ‘insult’, ‘threat’, ‘obscene’ and ‘identity-hate’. Comments can be associated with multiple classes at once, which frames the task as a multi-label classification problem.
