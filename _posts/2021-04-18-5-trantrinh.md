---
layout: post
title: "Political Name-calling on Reddit (Tran-Trinh)"
author: Nathan Tran-Trinh
date: 2021-04-18 12:24:54 -0500
categories: trantrinh
---

In an attempt to create data that was easier to work with in terms of creating visualizations, I created a duplicate of my mass dataframe but edited the created_utc variable to only include the year. I also removed rows where the comment text was a duplicate

```
dfff <- dff
options(max.print = 100000000)
dfff$created_utc <- substr(dfff$created_utc, start = 1, stop = 4)
dfff <- dfff[!duplicated(dfff$body),]
```

I then used the split function to create different dataframes for each subreddit. I also split once again so that each year/subreddit combination had its own unique dataframe, for a total of 21 dataframes, but I am not including that code because it is unlikely that I am using those for anything beyond wordcloud or FCM network representations, as issues with doc_vars makes them unsuitable for anything beyond that. Also, 21 dataframes is rather unwieldy anyways. Regardless, here are 20 FCMs (r/neoliberal in 2017 had only 4 comments with any of the terms, so quite useless a representation) that let you see how frequently features co-occur on said subreddit/year basis, using only the top 30 features.

```
AA <- split(dfff, dfff$subreddit)
AABreadTube <- AA$BreadTube
AAChapoTrapHouse <- AA$ChapoTrapHouse
AAConservative <- AA$Conservative
AANeoliberal <- AA$neoliberal
AATD <- AA$The_Donald

corpus_BreadTube <- corpus(AABreadTube, text_field = "body")
corpus_ChapoTrapHouse <- corpus(AAChapoTrapHouse, text_field = "body")
corpus_neoliberal <- corpus(AANeoliberal, text_field = "body")
corpus_TD <- corpus(AATD, text_field = "body")
corpus_Conservative <- corpus(AAConservative, text_field = "body")

textplot_network(TD2016fcm)
textplot_network(TD2017fcm)
textplot_network(TD2018fcm)
textplot_network(TD2019fcm)
textplot_network(Con2016fcm)
textplot_network(Con2017fcm)
textplot_network(Con2018fcm)
textplot_network(Con2019fcm)
textplot_network(Con2020fcm)
textplot_network(Chapo2016fcm)
textplot_network(Chapo2017fcm)
textplot_network(Chapo2018fcm)
textplot_network(Chapo2019fcm)
textplot_network(Chapo2020fcm)
textplot_network(Bread2018fcm)
textplot_network(Bread2019fcm)
textplot_network(Bread2020fcm)
textplot_network(Neo2018fcm)
textplot_network(Neo2019fcm)
textplot_network(Neo2020fcm)
```
What do these networks reveal to us? Considering the density of these maps, it's difficult to extrapolate with any confidence. Notably, the only one of the four terms that appears in all of the maps for both r/conservative and r/the_donald's network mappings is "socialist". Both seem to have a frequent economic focus, though this is a bit stronger on the end of r/conservative. r/the_donald also has a unique and likely concerning frequency of the terms "jew" and "Israel" that does not appear on the other right-wing subreddit.
The same heavy economic focus continues looking at our far-left subreddits, those being r/ChapoTrapHouse and r/BreadTube. Both subreddits also seem to have a strong propensity towards ideological terms such as Nazi, fascist and anarchist. Notably, BreadTube in 2020 is the only network mapping that contains one of the four key terms as a top 30 feature, that being "simp".
r/neoliberal has perhaps the most interesting mappings of all, with 2018 and 2019 containing a strange amount of sexual discussions (including "bull" and "relationship", the former becoming clearer when looking directly at the dataframe and its relation to "cuck") and 2020 having an odd centralization on... Christmas? These network mappings really didn't provide much in regards to knowledge towards buzzwords, although at the very least, it shows that the popularization of "simp" amongst left-wing factions happens as late as 2020, and the illustration of the anti-semitism of Trump supporters is intriguing to say the least.

Let's try using liwcalike() to see how the estimated sentiment stands up.

```
ChapoSentiment <- liwcalike(corpus_ChapoTrapHouse, data_dictionary_NRC)
names(ChapoSentiment)

ggplot(ChapoSentiment) + 
  geom_histogram(aes(x=positive)) + 
  theme_bw()

ConSentiment <- liwcalike(corpus_Conservative, data_dictionary_NRC)
names(ConSentiment)

ggplot(ConSentiment) + 
  geom_histogram(aes(x=positive)) + 
  theme_bw()

BreadSentiment <- liwcalike(corpus_BreadTube, data_dictionary_NRC)
names(BreadSentiment)

ggplot(BreadSentiment) + 
  geom_histogram(aes(x=positive)) + 
  theme_bw()

TDSentiment <- liwcalike(corpus_TD, data_dictionary_NRC)
names(TDSentiment)

ggplot(TDSentiment) + 
  geom_histogram(aes(x=positive)) + 
  theme_bw()

NeoSentiment <- liwcalike(corpus_neoliberal, data_dictionary_NRC)
names(NeoSentiment)

ggplot(NeoSentiment) + 
  geom_histogram(aes(x=positive)) + 
  theme_bw()
```

Well, as I expected, it's all overwhelmingly negative, although BreadTube is slightly more positive it seems (this largely seems to be the result of the popularity of a YouTuber named Cuck Philosophy over there, who creates analysis videos on philosophy, politics and pop culture from a leftist point of view). The problem here is that the sarcasm and irony used much more heavily amongst leftist and later more centrist commenters isn't picked up upon very well. A simple perusal of the dataframes will make the more earnest usage of these terms as legitimate insults by the right-wing very apparent, as well as the more comedic, often self-deprecating or sarcastic usage of these terms in comparison amongst leftists, but this isn't very well picked up upon by dictionary approaches it seems.

Let's recreate our DFMs, our tokens and attempt to create some correlated topic models.

```
TD_dfm <- dfm(corpus_TD,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
topfeatures(TD_dfm, 20)

Chapo_dfm <- dfm(corpus_ChapoTrapHouse,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
topfeatures(Chapo_dfm, 20)

Con_dfm <- dfm(corpus_Conservative,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
topfeatures(TD_dfm, 20)

bread_dfm <- dfm(corpus_BreadTube,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
topfeatures(bread_dfm, 20)

Neo_dfm <- dfm(corpus_neoliberal,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
topfeatures(Neo_dfm, 20)

NeoTokens <- tokens(corpus_neoliberal, remove_punct = T, remove_numbers = T)
ChapoTokens <- tokens(corpus_ChapoTrapHouse, remove_punct = T, remove_numbers = T)
TDTokens <- tokens(corpus_TD, remove_punct = T, remove_numbers = T)
BreadTokens <- tokens(corpus_BreadTube, remove_punct = T, remove_numbers = T)
ConTokens <- tokens(corpus_Conservative, remove_punct = T, remove_numbers = T)

library(stm)
library(quanteda)

Chapoctm <- stm(Chapo_dfm, K = 5, verbose = FALSE, init.type = "Spectral")
Neoctm <- stm(Neo_dfm, K = 5, verbose = FALSE, init.type = "Spectral")
Breadctm <- stm(bread_dfm, K = 5, verbose = FALSE, init.type = "Spectral")
Conctm <- stm(Con_dfm, K = 5, verbose = FALSE, init.type = "Spectral")
TDctm <- stm(TD_dfm, K = 5, verbose = FALSE, init.type = "Spectral")

labelTopics(Chapoctm)
labelTopics(Neoctm)
labelTopics(Breadctm)
labelTopics(Conctm)
labelTopics(TDctm)
```
Well, from this quick analysis, things are pretty clear. r/conservative and r/the_donald each have 1 topic where things are scarily bigoted, the former racist, the latter misogynistic. Other than that, those two subreddits are about as you'd expect, with a high frequency towards economic, policy, or otherwise ideological discussions, with themes like George Orwell's 1984, Healthcare, Venezuela or former U.S. presidential candidates. You get similar ideological focuses with every other subreddit, and similar frequency of the names of figures like Joseph Stalin or Jeff Bezos.

Douglas was kind enough to assist me in creating plots for frequency of terms over the years for each subreddit. His code is as such.

```
# load workspace

# convert to corpus object
dff_corpus <- corpus(dff_corpus)

# create time variable
library(lubridate)
dff$date <- ymd_hms(dff$created_utc)
dff$year <- year(dff$date)
dff$yearFactor <- as.factor(dff$year)

# add docvars
docvars(dff_corpus) <- dff

# subreddits
subreddits <- unique(dff$subreddit)

# names
bad_names <- c("cuck", "soyboy", "simp", "socialist")

# create counter
counter <- 1

# run plot loop
for (i in 1:length(subreddits)){
  
  # choose subreddit
  tmpSubName <- subreddits[i]
  
  # subset corpus
  tmpSub <- corpus_subset(dff_corpus, subreddit == tmpSubName)

  # group by year
  byYear <- textstat_frequency(dfm(tokens(tmpSub)), groups = docvars(tmpSub)$yearFactor)
  
  # loop through words
  for (j in 1:length(bad_names)){
    # pull word
    tmpWord <- bad_names[j]
    
    # filter
    tmpFreq <- subset(byYear, byYear$feature %in% tmpWord) 
    
    # convert back to numeric
    tmpFreq$group <- as.numeric(as.character(tmpFreq$group))
    
    # save a plot
    p <- ggplot(tmpFreq, aes(x=group, y=frequency)) + 
      geom_line(color = "gray", size =2, alpha = 0.8) +
      geom_point(size = 2) + 
      xlab("Year") + 
      ylab("Frequency") + 
      theme_bw()
    
    # assign it a name
    plotCall <- paste("p",counter," <- p", sep = "")
    eval(parse(text=plotCall))
    
    # add to counter
    counter <- counter + 1
  }
}

# plot grid
library(gridExtra)

grid <- grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16, p17, p18, p19, p20, nrow = 5)
view(grid)
```

I attempted to figure out if there was a way to play with the axes using a for loop, but got stumped for quite a few hours, and Google was of little help. I'm still going to continue attempting finding a method of automatically assigning labels showing the term and subreddit, or even a way to add these labels by hand (as it's hard to tell which plots belong to which subreddit, though I'm sure I can eventually figure that out by hand). I will also continue attempting to working with polarity and structural topic models, which has been a bit of a challenge for me.