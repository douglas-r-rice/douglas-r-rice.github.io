---
layout: post
title: "Reddit Wallstreetbets Analysis (Dinnie)"
author: Ian Dinnie
date: 2021-03-14 12:24:54 -0500
categories: dinnie
---

Since the last blog post, most of the progress I have made has been towards taking the initial dataframe of comments and converting it to a corpus to be used in my analysis. I quickly realized that 279,000 comments is far too many for me to handle at the moment, I started by whittling down the dataset to 1,000 randomly selected comments. In doing so I am able to reduce the amount of labor involved in managing this very messy data while still preserving the features of the data that I am interested in (i.e. how frequent specific terms are relative to others).  
```{r}
# This dataset is huge, I am going to randomly sample 1000 records to work with for now
# I may change the sample size or selection method further on down the line
# This is to make things simpler for me right now
# Leaving it commented out so I don't accidentally overwrite my sample with a different one
#WSB_sampled <- WSB %>% 
  #slice_sample(n = 1000)
#saveRDS(WSB_sampled,"WSB sampled")
```

After reducing the size of the dataframe, I then converted the column containing the columns to a corpus. One potential alternative I thought about but have not pursued at this time is grouping the comments on each post into one vector. This would be useful in that it would compress the size of the document feature matrix I create later on and reduce feature sparsity.
```{r}
# make it into a corpus
WSB_sampled <- readRDS("WSB Sampled")
WSB_corpus <- WSB_sampled$comment
```

Next, I created a document feature matrix out of the corpus, converting to lower case and removing stop words and punctuation in the process. I may end up deciding to include at least some of the stop words, since I may end up wanting to look at pronouns to gain extra context but am removing them for now to reduce the amount of features. 
```{r}
# create a document feature matrix
WSB_dfm <- dfm(WSB_corpus,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
WSB_dfm
topfeatures(WSB_dfm, 20)
```
There are a few things that jump out at me from the above output. Firstly, it makes clear the need for me to go back and perform some additional data cleaning to remove symbols (= & $) and individual letters. From examining the data, it seems as though most of the equal and dollar signs are a result of people using emoticons in their comments, which end up getting parsed oddly. This also made me realize that the data sometimes contains squares; literal squares that break up strings that I do not know how to remove, but would like to. Another aspect of the above output that struck me was the frequency with which 'gme' (Gamestop's stock ticker) was mentioned. The fact that 'gme' was mentioned more than the terms "stock", "shares", or "buy" is striking to me in that 'gme' is only ever used in a very specific context, whereas the terms "stock", "shares", or "buy" are used in basically any context related to the stock market. This is the first interesting finding I have come across so far. The above output inspired me to create a word cloud of all the words that appear more than 10 times so I could get a visual sense of relative word frequencies. 
```{r}
# create a word cloud
textplot_wordcloud(WSB_dfm, min_count = 10, random_order = F)
```
This word cloud is interesting in that it makes clear just how frequently 'gme' is used on r/wallstreetbets by surrounding it with other words we see and use all the time, which in turn shows us how Gamestop-obsessed the subreddit is. Going forward, I am going to spend more time cleaning the comments to remove the junk that is still being included, and nail precisely what my end-goal is so I can start thinking of ways to go about achieving that goal. 


