---
layout: post
title: "Wrangling a Massive Text Corpora from Parler (Linberg)"
author: Steve Linberg
date: 2021-03-28 12:24:54 -0500
categories: linberg
---

This round involved wrangling the data some more, filtering it down, and preparing it for whatever analysis ends up being most efficient.

In the previous post, I isolated one of the 167 1-gigabyte archive segments as a testing sample, extracted the posts from January 2021 from it, ranging in date from January 1, 2021 to January 11, 2021, and ran a few wordclouds against that subset. This time, I wanted to finally go through all of the 167 gigabyte files, making the guess that the January 2021 posts were scattered throughout them. This turned out to be correct. The process of extracting the data from the archive files took around 18 hours on my laptop. The steps were:

1. Convert all of the .ndjson files to .Rdata
2. Combine the .Rdata files into a single archive
3. Filter the January 2021 posts to the users we will analyze
4. Peek at one random user's data

Each step is detailed below.

## Step 1: Convert all of the .ndjson files to .Rdata

As previously discussed, the data files from the Parler set are in .mdjson format, which makes them very easy to work with, although it is a tremendously inefficient mode of storage for analysis. Among other features, the creators of the data set extracted all of the hashtags from each post and appended them as columns titled `hashtags.0`, `hashtags.1`, etc, and did similarly for URLs. While each line of each data file had as many fields as it needed, its full expansion in memory when the whole file was read meant that if, for instance, one post out of 1,000,000 had 100 hashtags in it (not unusual), the resulting million-row matrix would have 100 columns for hashtags, even if only one post used them.

Before any processing would be possible, this would need to be cleaned up.

I also anticipated that this stage would be the longest in terms of CPU time, and I wanted to make sure the process would be resumable if it were interrupted, so I structured the loop to re-read the output directory on every iteration and begin the process with whichever lowest-numbered file was missing. 

```{r}
library(tidyverse)
library(ndjson)
library(lubridate)
library(stringr)
```

After expanding the original archive, I renamed all of the files in this pattern:

- parler_data_001.ndjson
- parler_data_002.ndjson
- parler_data_003.ndjson (...)
- parler_data_167.ndjson

Their parsed equivalents would be named "parsed_data_xxx.Rdata".

```{r}
parler_dir <- "~/Parler_data/parler_data"
data_filename_pfx <- 'parler_data_'
parsed_filename_pfx <- 'parsed_data_'

# Loop continuation condition
working <- TRUE

# Could have hard-coded this, but look for the number in the filename at 
# the position following the length of the prefix portion
full_number_start_pos <- str_length(data_filename_pfx)
parsed_number_start_pos <- str_length(parsed_filename_pfx)

# Create a vector of numbers (strings, technically, in numeric format)
# from the complete list of files; assume 3 digits long (sigh)
full_file_number_list <- list.files(path=parler_dir, pattern=data_filename_pfx) %>%
  substr(full_number_start_pos+1, 3+full_number_start_pos)

while (working) {
  # Get the full list of parsed file numbers.
  parsed_file_number_list <- list.files(path=parler_dir, pattern=parsed_filename_pfx) %>%
    substr(parsed_number_start_pos+1, 3+parsed_number_start_pos)
  # The difference between the sets is the working list. Order doesn't matter.
  missing_file_numbers <- setdiff(full_file_number_list,parsed_file_number_list)
  if (length(missing_file_numbers > 0)) {
    file_pfx <- missing_file_numbers[1]
    input_file = paste(data_filename_pfx,file_pfx,".ndjson",sep='')
    output_file = paste(parsed_filename_pfx,file_pfx,".Rdata",sep='')
    
    # Load the ndjson file
    print(paste(format(Sys.time(), "%H:%M:%S"), 
                str_interp("Processing file ${input_file}; reading file")))
    full_path <- paste(parler_dir, input_file, sep='/') 
    df <- ndjson::stream_in(full_path)
    
    # Filter the data
    print(paste(format(Sys.time(), "%H:%M:%S"), 
                str_interp("Processing file ${input_file}; filtering")))
    
    # Create the january_2021_posts data frame from this file, doing the date
    # conversion en passant to filter down to just January 2021

    january_2021_posts <- df %>%
  # Using as.POSIXlt created a lot of pain, because POSIXlt objects can't be part of data frames.
  # POSIXct works, but needs the lubridate library and a different syntax to extract parts.
    mutate(createdAtPosix = as.POSIXct(createdAtformatted, tz="America/New_York")) %>%
    mutate(created_day = as.integer(day(createdAtPosix)), 
         created_month = as.integer(month(createdAtPosix)), 
         created_year = as.integer(year(createdAtPosix))
         ) %>%
      
    # Filter the 2021 posts, containing body text (e.g. not just empty posts of images or video)
    filter(created_year == 2021,
         nchar(body) > 1) %>%
    # Store just the fields we want to keep (we don't need hashtags/etc)
    select(article,
body,
created_day,
datatype,
id,
impressions,
parent,
username,
reposts,
sensitive,
upvotes,
username,
impressions,
controversy,
downvotes)
    
    # Write the output file.
    
    output_line_count <- nrow(january_2021_posts)
    print(paste(format(Sys.time(), "%H:%M:%S"), 
                str_interp("Processing file ${input_file}; writing output file ${output_file} (${output_line_count} records)")))
    output_path <- paste(parler_dir, output_file, sep='/') 
    save(january_2021_posts, file=output_path)
    
    # Clearing the df and january_2021_posts data frames at the end of each
    # loop iteration dramatically improves speed; without it, execution time
    # roughly doubles after about 30 iterations
    rm(df)
    rm(january_2021_posts)

      }
  else {
    working <- FALSE
  }
}
```

This step took around 18 hours in total to run, and resulted in 167 parsed_data_xxx.Rdata files, each containing around 33,000 posts (remarkably consistent), and condensed from approximately 1gb to 3mb in size.

Sample output from the run process:

```
[1] "19:23:57 Processing file parler_data_030.ndjson; reading file"
[1] "19:27:18 Processing file parler_data_030.ndjson; filtering"
[1] "19:28:28 Processing file parler_data_030.ndjson; writing output file parsed_data_030.Rdata (33711 records)"
[1] "19:28:29 Processing file parler_data_031.ndjson; reading file"
[1] "19:34:20 Processing file parler_data_031.ndjson; filtering"
[1] "19:35:13 Processing file parler_data_031.ndjson; writing output file parsed_data_031.Rdata (33355 records)"
```

It took 5-6 minutes to process each file; around 80% of the time was in reading/parsing the file, and the remaining time was filtering; the writing was very fast once processing was complete.

## Step 2: Combine the .Rdata files into a single archive

The next step was to combine all of the .Rdata files into a single .Rdata data frame, representing all of the posts from January 2021. In hindsight, it would have been better to create the above files in .rds format, which does not embed variable names, rather than .Rdata, which does; it just made it necessary to keep a separate accumulating data frame in the code below, since each file would re-declare `january_2021_posts`.

As with the above step, I included a few debugging statements for monitoring the process as it ran.

```{r}
library(tidyverse)
library(stringr)
parler_dir <- "~/Parler_data/parler_data"
parsed_filename_pfx <- 'parsed_data_'

parsed_file_list <- list.files(path=parler_dir, pattern=parsed_filename_pfx)

# Load the first one here and initialize the all_posts data frame with it
load(paste(parler_dir,parsed_file_list[1],sep="/"))
all_posts <- january_2021_posts
# Remove the loaded element
rm(january_2021_posts)

for (parsed_file_name in parsed_file_list[-1]) {
    print(paste(format(Sys.time(), "%H:%M:%S"), 
                str_interp("Processing file ${parsed_file_name}; reading file")))
    full_path <- paste(parler_dir, parsed_file_name, sep='/') 
    load(full_path)
    
    print(paste(format(Sys.time(), "%H:%M:%S"), 
                str_interp("Processing file ${parsed_file_name}; merging")))
    all_posts <- rbind(all_posts, january_2021_posts)
    rm(january_2021_posts)

    new_length <- nrow(all_posts)
    print(paste(format(Sys.time(), "%H:%M:%S"), 
      str_interp("Processing file ${parsed_file_name}; length is now ${new_length} posts")))
    
}

# Write the output file.
output_file <- "all_posts.Rdata"
output_path <- paste(parler_dir, output_file, sep='/') 
save(all_posts, file=output_path)

```

Each file was read and appended to the `all_posts` data frame in sequence using `rbind`. As previously, the data frame was removed/redeclared on each iteration to improve execution time.

Sample output:

```
[1] "17:17:23 Processing file parsed_data_030.Rdata; reading file"
[1] "17:17:23 Processing file parsed_data_030.Rdata; merging"
[1] "17:17:24 Processing file parsed_data_030.Rdata; length is now 1029330 posts"
[1] "17:17:24 Processing file parsed_data_031.Rdata; reading file"
[1] "17:17:24 Processing file parsed_data_031.Rdata; merging"
[1] "17:17:24 Processing file parsed_data_031.Rdata; length is now 1062685 posts"
```

This process ran pretty quickly, and resulted in the file `all_posts.Rdata`, 539mb in size, containing all of the (non-empty) Parler posts from January 2021.

## Step 3: Filter the January 2021 posts to the users we will analyze

Although the posts have now been filtered down, we still have more posts than we want. 

The question I want to ask is: among Parler users who posted both before and after the January 6 insurrection, was there any change in sentiment before and after?

There are two basic ways to approach this. One would be to look at the sentiment of the entire user base and see if there was an overall change in tone; the second would be to look at individual users, and look at how many shifted in tone favorably, and how many unfavorably. Doing the second doesn't preclude also doing the first, but it means we have to create a data structure keyed by user that contains each user's posts during that time period. 

Also, I want to get a fairly representative portion of the users, likely to represent a "mainstream" of thought on the platform. To this end, I want to eliminate people who posted very little (and whose sentiment might therefore be hard to gauge), and those who posted excessively (who are likely to hold non-representative, extreme views, or be bots spamming the system). Somewhat arbitrarily, I decided to restrict the users to only those who posted at least 5 times before *and after* the insurrection on January 6, but no more than 250 times (averaging 50 a day). Of the 39033 unique users who posted in January 2021, 38393 fit this criteria.

I created a separate data structure of just the usernames of these users, which is then used to further filter the January 2021 data.

Beginning by reading the whole Parler archive:

```{r}
parler_dir <- "~/Parler_data/parler_data"
#input_filename <- 'parsed_data_001.Rdata'
input_filename <- 'all_posts.Rdata'

load(paste(parler_dir,input_filename,sep="/"))
```

Create a categorical variable "time_group", values "pre" when day < 6, "post" when day 7-11, "on" when day = 6. Group by username and that variable, so we can exclude users by their post counts, either too low or too high.

(As a reminder, the `all_posts` file and data frame are really all the posts from January 2021, not the entire Parler archive. I probably should have called this `all_january_2021_posts`. Ah well...)

```{r}
active_posters <- all_posts %>%
  mutate(time_group = ifelse(created_day < 6, "pre",
                             ifelse(created_day > 6, "post", "on"))) %>%
  group_by(username, time_group) %>%
  count() %>%
  spread(time_group,n) %>%
  filter(post > 5, pre > 5) %>%
  filter(post < 250, pre < 250) %>%
  # All we want is the usernames.
  select(username)
```

Save this glorious bit of data.

```{r}
output_file <- "active_posters.Rdata"
output_path <- paste(parler_dir, output_file, sep='/') 
save(active_posters, file=output_path)
```

We probably won't do much with posts from January 6 itself, but they're what's left after we distinguish between pre and post, so we might as well keep it.

Use the username column to filter down the all_posts data frame.

The wrangling below is a bit complex, but the basic logic is:

- take all of the January 2021 posts
- filter it down to posts by the "regular" users we screened in the above step
- create a categorical variable called `created_class` which is either "pre", "post", or "on" January 6
- for paranoia, omit any row not containing this variable (shoudln't be possible, but be bulletproof)
- group the data by username and class, so each user gets one row each for "pre", "post" and "or" comments
- paste together each user's comment in each class, so all of the "pre", "post" and "on" comments are appended to each other, separated by carriage returns
- `pivot_wider` to turn these separate rows into a single row for each user with "pre", "post" and "on" columns, containing their combined posts for each category

```{r}
usernames <- active_posters$username
n_day <- 6

user_posts <- all_posts %>%
  filter(username %in% usernames) %>%
#  filter(username == "00KimPossible00") %>%
#  mutate(body = paste0(body, collapse = "\n"),
  mutate(created_class = case_when(
          created_day < n_day ~ "pre",
          created_day == n_day ~ "on",
          created_day > n_day ~ "post",
          TRUE ~ NA_character_
        )) %>%
  filter(!is.na(created_class)) %>%
  select(un = username, body, created_class) %>%
  group_by(un, created_class) %>%
  summarize(joined_body=paste0(body, collapse="\n")) %>%
  pivot_wider(names_from = created_class, values_from = joined_body) %>%
  ungroup()

```

Save it. 

```{r}
output_file <- "user_posts.rds"
output_path <- paste(parler_dir, output_file, sep='/') 
saveRDS(user_posts, file=output_path)
```

If all goes well, this is the final data source we will use to investigate our question.

## Step 4: Peek at one random user's data

Just for giggles, (sort of), we can look at a random user's data and see what wordclouds for just one person might look like. This is not likely to be something we'll do much of, because wordcloud efficiency isn't going to be great on very small data sets, and some users on the low end of verbosity might not have posted enough for words to be repeated frequently; sentinemt analysis is likely to be the way we'll go here, but let's just do one sample and see what we get.

Warning: this is looking at raw, unfiltered and uncensored Parler post data.

```{r}
seed_val = 4123  # arbitrary random seed

set.seed(seed_val)
random_user <- sample_n(user_posts, 1)
```

```{r}
random_pre_posts <- corpus(random_user, text_field="pre")
random_pre_posts_dfm <- dfm(random_pre_posts,
                              tolower = TRUE,
                              remove_punct = TRUE,
                              stem = FALSE,
                              remove = stopwords("english")
)
random_post_posts <- corpus(random_user, text_field="post")
random_post_posts_dfm <- dfm(random_post_posts,
                              tolower = TRUE,
                              remove_punct = TRUE,
                              stem = FALSE,
                              remove = stopwords("english")
)
```

```{r}
# draw the wordcloud
set.seed(seed_val)
textplot_wordcloud(random_pre_posts_dfm, min_count = 2, random_order = FALSE)
set.seed(seed_val)
textplot_wordcloud(random_post_posts_dfm, min_count = 2, random_order = FALSE)
```

### Pre-January-6 wordcloud for user "RaisingUSApatriots"



![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/linberg-post-4-wordcloud-1.png?raw=TRUE)

This is a much less dense wordcloud, of course, because there is much less data. We see, again, the placeholders for emoji; the four-character one is the U. S. flag.

This wordcloud was derived from the combined posts:

>Grammy and papa raised us all to be patriots! ❤️🇺🇸💙 Amen and thank you from the bottom of our patriotic hearts ❤️🇺🇸😍 Because he has something to hide. He’s a traitor and is definitely a part of the deep state. They’ve got something on him. He’s shady like the rest. He’s deep state. He’s a traitor. He’s a dirtbag. And to think I used to have respect for him 🤦🏼‍♀️ We love you!!! Thank you for having our backs! Do you need a blanket made with the word GOVERNMENT on it to make you feel secure at night? Go troll somewhere else. Thank you 🙏🏻 It’s like a fucked up movie we are living in. But it’s not a movie, it’s reality. Fuck the deep state. We can’t let them win! GFY Can we do a class action lawsuit for censorship and violating our 1st amendment? It’s time to stand up if we want to make a change. This is not the America I want my sons growing up in. We need change and we need it now!! The asshats helped to change the outcome of our election, while censoring us. Disgusting. Love you girlie!! You are despicable. Why the hell would we fight for you when you won’t fight for us. You don’t deserve that title. You don’t represent us. You’re a sellout and a con artist. I don’t even know how you can sleep at night. My hubby, boys, and I watched you last night (we love you btw). I told my son how you just had chemo and he said “so he’s like grandma? He gets chemo and is still strong, healthy and happy. Wow!” Thank you sir, we all look up to you. We are a law enforcement family and my sons want to serve for their country. We are fighting for you! ❤️🇺🇸💙 I wonder what he does with that extra time when he abbreviates “yourself”.

A quick scan of this reveals some of the difficulty we're going to have parsing sentiment; since this is a set of combined posts from different threads, there is variation between the subjects; within this block we have fragments like "love you", "you are despicable", "you're a sellout and a con artist", and "thank you sir, we all look up to you." I think getting down to individual threads might be cutting the data up too finely, so we're going to have a challenge; it could be that we'll need to raise the minimum levels of participation in the filter so there's more data to work with per user, or we might need to break down individual users' posts and rate them individually, without trying to reconstruct threads.

If we have more data, then brute-force word counts become more plausible, but we'll have to investigate that further.

### Post-January-6 wordcloud for user "RaisingUSApatriots"

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/linberg-post-4-wordcloud-2.png?raw=TRUE)

And this is an even sparser wordcloud, derived from the following combined post-January-6 posts:

> 💯. Feminism has gone too far. We are equal. We were not meant to be inferior. Stop demeaning men! Women are strong. Men are strong. WE are strong. Keep the traditions or else there’s no point in raising men to be gentlemen. We can’t have that. Multiple men have been shown to be antifa. We are so thankful to have all three of you speak for us. Thank you 🇺🇸 Once our amazing president joins Parler, can you please announce because there are so many deceiving pages saying it’s him. Again, we just want truth, therefore we need HIM! Thank you sir, we appreciate your entire family. Words cannot express what you have done for our country. The most positive page. The page I would visit when I was upset with my generation. It was inspiring and comforting. I am absolutely disgusted. Thank you for ALL that you do! Us patriots appreciate you! BLM and Antifa didn’t get this kind of coverage. The media is our enemy and they’re winning. It was awful what happened, however the darkest day I saw was when BLM tried to break the bounds to the White House, threw bricks at secret service, and lit the church on fire. RIP. Back the blue! We are patriots. Do not turn on those we hold dearly! Stay united everyone!

It would seem, from looking at this, that the wordclouds aren't actually that bad; also, that this particular user didn't shift much in sentiment before and after the insurrection.

Next up is going to be starting the actual sentiment analysis, and I'm not sure yet what method I'm going to go with... we'll be bringing it into focus in the coming weeks, of course.

