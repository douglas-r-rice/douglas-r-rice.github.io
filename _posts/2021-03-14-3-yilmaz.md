---
layout: post
title: "Preprocessing and Representing the Reddit Text (Yilmaz)"
author: Gamze Yilmaz
date: 2021-03-14 12:24:54 -0500
categories: yilmaz
---


Blog Post 3: Preprocessing and Representing the Reddit Text    

I've been working on cleaning my corpus to set it up for data analysis and the biggest issue has been the difficulty with removing the large amounts of non-ASCII characters from it. This week I've continued to search for a way to remove non-ASCII characters. I tried a few different codes including gsub() function. Though the code runs OK, the top features still show non-ASCII characters as frequent features. I also tried to remove specific features from the dfm using the code from our discussion on Tuesday. That approach did not work either. Below are all kinds of different approaches I tried and failed to remove non-ASCII characters:

x <- c("\U0001f680")
gsub("\U0001f", "", x)

phrase_clean <- gsub("[^[:alnum:][:blank:]?&/\\-]", my_corpus)
phrase_clean

phrase <- c ("\U0001f680", " \U0001f48e", " \U0001f64c")
phrase

phrase_clean <- gsub(" ", phrase)

replace_non_ascii("\U0001f680", replacement = "", remove.nonconverted = TRUE)

Though the above codes did not give any error messages when I ran them on R, they did not remove non-ASCII characters from my corpus. After searching and trying several different codes to get rid of these characters, I used the iconv() function and it WORKED! The unicode characters do not show up in the top features anymore. 

my_corpus <- iconv(my_corpus, 'utf-8', 'ascii', sub='')
my_corpus <- corpus(my_corpus)

After I managed to remove all non-ASCII characters from my corpus, I created a document-feature matrix and explored the top features in my corpus as well as a wordcloud visually representing these features. Everything so far looks clean and there are no strange characters in the dfm. My document-feature matrix consists of 39,734 documents, 19,935 features (100.0% sparse) and 10 docvars. I believe that the sparsity is an issue because of the brevity of each comment (document) in my corpus. I need to make sure that not only the comments in the "title" column but also those in the "body" column are included in my corpus as "text" data. 

my_corpus_text_dfm <- dfm(my_corpus, tolower =TRUE, stem = FALSE, remove_punct = TRUE, remove = stopwords("en"))
my_corpus_text_dfm
textstat_frequency(my_corpus_text_dfm) 
textplot_wordcloud(my_corpus_text_dfm)
topfeatures(my_corpus_text_dfm, 20)

  gme         $       buy      hold robinhood       amc      just       can   holding 
     7641      5952      3622      3171      3074      2999      1959      1939      1813 
    still       now     stock      moon      sell      like    shares        im        bb 
     1659      1652      1558      1428      1417      1360      1291      1190      1170 
    short       get 
     1155      1142

I also created a smaller dfm using the code from this week along with the textplot network, but I'm not sure how meaningful this information is given that the documents (comments) in my corpus are very short.

Thoughts about Data Analysis 

In light of the domain assumptions and statistical complexity discussed in O'Connor, Bamman and Smith (2011), I plan on taking an exploratory approach to analyze language change in Wallstreetbets community after the peak of short squeeze. As my corpus includes valuable non-textual metadata such as the timestamps and votes for each comment, it would be interesting to examine the correlations between these variables and word frequencies. More specifically, I can test the research question about the relationship between group norms and language use using two-variable models between text frequency ratios and the metadata variable of “votes”. This can inform us about the type of messages that were liked or disliked by the community, shedding light on conformity with the community norms. Similarly, I can examine the change in text frequencies based on timestamps using regression analysis. Change in the text frequecies as the market and the public react to short squeeze can also provide information on the changing sentiments in the community. I still want to try using unsupervised topic modeling to explore latent themes in my corpus but need to keep the computational complexity in mind as I approach data analysis.    


