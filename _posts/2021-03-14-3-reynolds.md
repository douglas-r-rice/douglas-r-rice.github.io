---
layout: post
title: "Politics and Podcasts (Reynolds)"
author: Nate Reynolds
date: 2021-03-14 12:24:54 -0500
categories: reynolds
---


Progress is coming along steadily with my project. Because the project thus far has largely been a web-scraping exercise with API's, I spent the last two weeks catching up with the recent tutorials to create a corpus with my data and tokenize the text. I now have a corpus of 16,449 documents (podcast episodes) which is preprocessed in R. I also created a few word clouds for exploratory analysis, detailed below.

### Updating my Sample

Using the elaborate API-based loops which I created and detailed in my last blog post, I also gathered more data and updated my dataset. In my first attempt to create a dataset of podcast networks, I used a series of loops beginning with *The Tim Ferriss Show* to run through every name in the title of every episode of the podcast and search for podcasts by that name... repeated three times. This took several hours to run and the sample it churned out was not as relevantly political as I was hoping. It also contained a lot of duplicate shows and a lot of non-English shows. One big problem was that the Named Entity Recognition script which I ran was imprecise, recognizing generic first names or brand names as named entities and then searching for those terms over and over again.

So I decided to run more restrained but more wide-reaching loops to capture a more precise sample. Instead of scraping once, three searches deep, I scraped three times, two searches deep, starting with the the left-leaning *Vox Conversations*, right-leaning *The Glenn Beck Program*, and the reportedly down-the-middle *Left, Right & Center*, to compile a sample that was more relevant to the subject of this project. See my last blog post for details on how this scraping worked.

```{r}
# loading the data that was scraped in Colab
lcr <- read.csv("lcr.csv")
glenn <- read.csv("glennbeck.csv") 
vox <- read.csv("vox.csv")
# collapse data function
makeUnique <- function(x) { 
  x %>%
  group_by(showID, datePublished) %>% # unique show, unique time published
  slice(1) %>% 
  ungroup() %>% 
  filter(grepl('en', lang)) # English-speaking podcasts
}
# collapse data
lcr <- makeUnique(lcr)
glenn <- makeUnique(glenn)
vox <- makeUnique(vox)
combined <- lcr %>%
  full_join(glenn) %>%
  full_join(vox) %>%
  makeUnique() 
```

The three datasets scraped from Python contained 18,545 podcast episodes, which collapsed to 16,449 when filtering for unique, English-speaking episodes, saved as dataset *combined*. For comparison, my duplicate-ridden first attempt was reduced from 70,104 to 28,830 when only collapsing down to unique episodes, so this was a big improvement, albeit a smaller sample.

I also saved datasets of the unique episodes from each of those podcast sources separately, so that I can compare them as seperate corpora later. I'm still not exactly sure if that's the direction I'm going to take the rest of the project, but I thought it would be interesting to have for exploration.

I may collect more data starting with different podcasts in future weeks, but I decided to move forward with this data. For now, I want to focus on developing an effective Named Entity Recognition script to identify podcast guests from podcast episode descriptions. In the future, I may combine this process with data collection; if I can identify guest names with decent accuracy, I would be more comfortable searching for more episodes by those names with deeper searches. I was hoping that Python compatibility in the new version of R Studio (1.4) would help with this, but I had trouble figuring out how to run the API in R Studio. I also understand that this snowball-esque sampling method has its problems as a research tool, but I think it has potential to very accurately find the data I'm looking to analyze.

### Creating the Corpus

First I created a corpus from the descriptions of the podcast episodes in my dataset. I also updated the corpus' docvars with the summary of each document, in addition to the show name, episode title, published date, and names recognized from the original search. Featured below is the process with the combined corpora of podcasts stemming from *Left, Right & Center*, *Vox Conversations*, and *The Glenn Beck Program*, but I also did the same process with each of those podcast networks individually.

```{r}
#create a character vector out of description data
descriptions <- combined$description
#create into corpus
podcast_corpus <- corpus(descriptions)
#create summary docvars
n_corpus <- length(podcast_corpus)
podcast_summary <- summary(podcast_corpus, n = n_corpus)
podcast_summary$show <- combined$showID
podcast_summary$title <- combined$title
podcast_summary$names <- combined$names
podcast_summary$date <- combined$datePublished
docvars(podcast_corpus)<-podcast_summary
```

In this blog post I will show examples using this full **combined** dataset, but I also created corpora for the component networks individually.

I then preprocessed the corpus to standardize the text of the episode titles within. This is especially important for this data because there are a lot of leftover HTML elements from the episode description formatting, like \<p\> and \<&nbsp\>.

```{r}
# create tokens, remove unnecessary characters
podcast_tokens <- tokens(podcast_corpus, 
    remove_punct = T,
    remove_numbers = T,
    remove_symbols = T,
    remove_url = T)
# remove stop words
podcast_tokens <- tokens_select(podcast_tokens,
                                pattern = stopwords("en"),
                                selection = "remove")
# remove HTML tags
podcast_tokens <- tokens_select(podcast_tokens,
                      pattern = c("p","br","nbsp"),
                      selection = "remove")
```

### Exploring the Data

To begin some exploratory text analysis of this corpus, I created a few word clouds. First I found the top words from the entire combined corpus. Unsurprisingly, the most frequent words in podcast episode descriptions are "podcast" and "episode", so I removed those from analysis as well as stop words and remaining HTML elements.

```{r}
podcast_dfm <- dfm(podcast_corpus,
        tolower = TRUE,
        remove_punct = TRUE,
        remove_numbers = T,
        remove_symbols = T,
        remove_url = T,
        stem = FALSE,
        remove = c(stopwords("english"), "<", ">", "p", "br", "nbsp", "b",
                   "podcast", "episode")
)
podcast_small <- dfm_trim(podcast_dfm, min_docfreq = 0.02, docfreq_type = "prop")
set.seed(100)
textplot_wordcloud(podcast_small, min_count = 50, random_order = FALSE)
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/combinedcloud.png?raw=TRUE)

The results are substantive, and yet it is not very surpising or interesting to see "discuss", "show", and "talk" near the top of the list. I presume that "us" is the top term both because of phrases like "this person joins us this week" and because it is the lower case version of "U.S.". However, it is telling that the first explicitly political term is "trump", which comes in at the 31st most frequent term with 1,063 appearances, above "news", "media", "talks", "know", and "joins", among others. Liberal or conservative, angry or supportive, podcasters love talking about Donald Trump.

I also tried making individual word clouds for the liberal component and the conservative component of my data, but the results were not all that interesting; again, conversational words dominated. To try a more precise comparison, I created two show-level subsets of my corpus to compare the descriptions of episodes belonging to different shows. In this example, I compare the wordcloud of CNN's *Cuomo Prime Time with Chris Cuomo* and the popular conservative talk show podcast *The Dan Bongino Show*.

First, the wordcloud for *Cuomo Prime Time with Chris Cuomo*:

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/cuomocloud.png?raw=TRUE)

Secondly, the wordcloud for *The Dan Bongino Show*:

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/bonginocloud.png?raw=TRUE)

Even these direct comparisons take some digging to find interesting results, but what was apparent to me was the relative frequency of coronavirus-related terms in *Cuomo Prime Time* like "coronavirus", "vaccine", and "COVID-19", and the focus on terms like "election", "trump" and "biden" in *The Dan Bongino Show*.

Ultimately these word clouds and the pursuit of frequent terms are flawed because a podcaster can format episode descriptions in whatever way they want. It may be one sentence or multiple paragraphs. Every episode of the same podcast may have an indentical line at the end of its description, or a promotional advertisement. I'm not worried about these flaws because this is not the point of this project - it was merely an exploratory exercise to understand what language I am working with. The rest of the project will focus around developing natural language processing to identify names of guest features within individual episode descriptions and building network data.
