---
layout: post
title: "An SVM Classifier of Subreddit Discussions (Dinnie)"
author: Ian Dinnie
date: 2021-04-18 12:24:54 -0500
categories: dinnie
---

In this blog post, I am going to play around with the coded data and also tweak the SVM model to see if I can't up the accuracy in a significant way. The first thing I will do is cross-validate my model by trying it out a different set of training and testing comments.

```
library(tidyverse)
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(caret)
library(e1071)

coded <- read_csv("coded.csv") %>% 
  mutate(new_comment = gsub("[^A-Za-z0-9:) ]","", comment)) %>% 
  mutate(new_comment = gsub(":)", "smiley_face", new_comment)) %>% 
  mutate(new_comment = gsub("[^A-Za-z0-9 ]","",new_comment),
         upvote_class = case_when(
           comment_score <= median(comment_score) ~ 0,
           comment_score > median(comment_score) ~ 1)
         ,
         sentiment = case_when(
           Score == 1 | Score == 2 ~ 0,
           Score == 3 | Score == 4 ~ 1
         )) 

coded_corpus <- corpus(coded, text_field = "new_comment")
# split the corpus into training and testing
# set seed
set.seed(1844)

# create doc ids
docvars(coded_corpus, "id") <- 1:ndoc(coded_corpus)

# create training set (60% of data) and initial test set
N <- ndoc(coded_corpus)
trainIndex <- sample(1:N,.6 * N) 
testIndex <- c(1:N)[-trainIndex]

# split test set in half (so 20% of data are test, 20% of data are held-out)
N <- length(testIndex)
heldOutIndex <- sample(1:N, .5 * N)
testIndex <- testIndex[-heldOutIndex]

# now apply indices to create subsets and dfms
dfmTrain <- corpus_subset(coded_corpus, id %in% trainIndex) %>% dfm()
dfmTest <- corpus_subset(coded_corpus, id %in% testIndex) %>% dfm()
dfmHeldOut <- corpus_subset(coded_corpus, id %in% heldOutIndex) %>% dfm()
```

I have now replicated the subsetting procedure from last blog post, but with a different seed ensuring I get a different sample of texts in my training and testing sets. Now to see if I can replicate the accuracy from last time! (We're shooting for 63% right now)

```
sentiment_SVM <- textmodel_svm(dfmTrain, docvars(dfmTrain, "sentiment"))
dfmTestMatched <- dfm_match(dfmTest, features = featnames(dfmTrain))
actual <- docvars(dfmTestMatched, "sentiment")
predicted <- predict(sentiment_SVM, newdata = dfmTestMatched)
confusion <- table(actual, predicted)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion, mode = "everything")
```

It's actually a little more accurate this time! The confidence interval is also about the same as last time. Since this model is performing well regardless of what sample it is trained on, I feel confident that this is the correct approach, and that the next step is try to tune this SVM to see if I can make it even more accurate.

Interestingly, if I remove any comments shorter than 3 words and then increase the training set size to 70% of my corpus, I can get the accuracy up to 76%.

```
# let's remove comments less than 3 words long
coded_long <- coded %>% 
  filter(sapply(strsplit(new_comment, " "), length) >= 3)

coded_long_corpus <- corpus(coded_long, text_field = "new_comment")
# split the corpus into training and testing
# set seed
set.seed(1844)

# create doc ids
docvars(coded_long_corpus, "id") <- 1:ndoc(coded_long_corpus)

# create training set (60% of data) and initial test set
N_l <- ndoc(coded_long_corpus)
trainIndex_long <- sample(1:N_l,.7 * N_l) 
testIndex_long <- c(1:N_l)[-trainIndex_long]

# split test set in half (so 20% of data are test, 20% of data are held-out)
N_l <- length(testIndex_long)
heldOutIndex_l <- sample(1:N_l, .5 * N_l)
testIndex_long <- testIndex_long[-heldOutIndex_l]

# now apply indices to create subsets and dfms
dfmTrain_l <- corpus_subset(coded_long_corpus, id %in% trainIndex_long) %>% dfm()
dfmTest_l <- corpus_subset(coded_long_corpus, id %in% testIndex_long) %>% dfm()
dfmHeldOut_l <- corpus_subset(coded_long_corpus, id %in% heldOutIndex_l) %>% dfm()

sentiment_SVM_l<- textmodel_svm(dfmTrain_l, docvars(dfmTrain_l, "sentiment"))
dfmTestMatched_l <- dfm_match(dfmTest_l, features = featnames(dfmTrain_l))
actual_l <- docvars(dfmTestMatched_l, "sentiment")
predicted_l <- predict(sentiment_SVM_l, newdata = dfmTestMatched_l)
confusion_l <- table(actual_l, predicted_l)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion_l, mode = "everything")
```

I am now going to tune the SVM to see if I can get even better accuracy. If I can not, I will code some more comments and see how much of a difference that makes.

```
# convert DFMs to Dataframes
dftrain <- convert(dfmTrain_l, to = "data.frame")
dftest <- convert(dfmTest_l, to = "data.frame")

#have to add outcomes
# first for training data
train_id <- as.data.frame(docvars(dfmTrain_l, "id")) %>% 
  mutate(rowid = seq.int(1:nrow(dftrain)))

train_outcomes <- as.data.frame(docvars(dfmTrain_l, "sentiment")) %>% 
  mutate(rowid = seq.int(1:nrow(dftrain)))

train <- train_id %>% 
  right_join(train_outcomes) %>% 
  mutate(doc_id = paste0("text",train_id[,1]))
train<- train %>% 
  select(colnames(train)[3], doc_id)

dftrain <- dftrain %>% right_join(train) %>% 
  select(-doc_id) 

# convert matched dfm to dataframe
matcheddf <- convert(dfmTestMatched_l, to = "data.frame") %>% 
  select(-doc_id)


# now try tuning
#tuned_SVM <- tune.svm(x = dftrain[,-2099], y = factor(dftrain[,2099]),
#                      cost = 1:20,
#                      gamma = ((10^-10):1),
#                      coef0 = (-1:1))
#tuned_SVM$best.parameters$cost
#tuned_SVM$best.parameters$gamma
#tuned_SVM$best.parameters$coef0

new_svm <- svm(x=dftrain[,-2099], y = as.factor(dftrain[,2099]),
               cost = 13)


predicted <- predict(new_svm, newdata = matcheddf)
confusion_new_svm <- table(actual_l, predicted)
confusionMatrix(confusion_new_svm, mode = "everything")
```

After tuning my model in a multitude of ways, I am unable to obtain anything above 72% accuracy; worse than when I didn't tune the model at all. I tried a few different things when tuning; I changed the kernel from linear, to polynomial, and then to sigmoid, varying their respective parameters as to find the optimal model based on what I gave it. Despite this, I was unable to beat the accuracy of the default settings of the textmodel_svm function. This basically leaves me with one more option for improving the accuracy of my model; giving it more data. Hopefully, by coding 200 more comments, I will be able to improve my model's accuracy. Going forward, I plan on researching SVMs and the different ways of tuning them to better understand how these algorithms work and how I can tweak them to achieve a higher accuracy rate. 

```
# get back to the 10k comments I originally sampled
raw <- readRDS("WSB Comments Raw")
set.seed(1234)
tenthousand <- raw %>% slice_sample(n = 10000)

# remove the ones I coded already
coded <- coded %>% 
  mutate(comment_author_score = paste0(comment,author,comment_score))
lesscoded <- tenthousand %>%
  mutate(comment_author_score = paste0(comment,author,comment_score)) %>% 
  filter(!comment_author_score %in% coded$comment_author_score)
tocode <- lesscoded %>% 
  mutate(new_comment = gsub("[^A-Za-z0-9:) ]","", comment)) %>% 
  mutate(new_comment = gsub(":)", "smiley_face", new_comment)) %>% 
  mutate(new_comment = gsub("[^A-Za-z0-9 ]","",new_comment)) %>% 
  filter(sapply(strsplit(new_comment, " "), length) >= 3) %>% 
  slice_sample(n = 200) %>% 
  select(author,user,comment_score,comment,new_comment)
write_csv(tocode, "tocode_4_18.csv")

coded_4_18 <- coded %>% 
  dplyr::rename(Sentiment = sentiment) %>% 
  select(author,user,comment_score,comment,new_comment,Sentiment) %>% 
  rbind(read_csv("coded_4_18.csv")) %>% 
  mutate(Sentiment = as.factor(Sentiment)) %>% 
  filter(sapply(strsplit(new_comment, " "), length) >= 3)
```

I am now going to use these newly added comments to replicate the first SVM I created, and see if the accuracy improves.

```
corpus_4_18 <- corpus(coded_4_18, text_field = "new_comment")
# split the corpus into training and testing
# set seed
set.seed(1844)

# create doc ids
docvars(corpus_4_18, "id") <- 1:ndoc(corpus_4_18)

# create training set (70% of data) and initial test set
N <- ndoc(corpus_4_18)
trainIndex_418 <- sample(1:N,.7 * N) 
testIndex_418 <- c(1:N)[-trainIndex_418]

# split test set in half (so 20% of data are test, 20% of data are held-out)
N <- length(testIndex_418)
heldOutIndex_418 <- sample(1:N, .5 * N)
testIndex_418 <- testIndex_418[-heldOutIndex_418]

# now apply indices to create subsets and dfms
dfmtrain418 <- corpus_subset(corpus_4_18, id %in% trainIndex_418) %>% dfm()
dfmTest_418 <- corpus_subset(corpus_4_18, id %in% testIndex_418) %>% dfm()
dfmHeldOut_418 <- corpus_subset(corpus_4_18, id %in% heldOutIndex_418) %>% dfm()

sentiment_SVM_418<- textmodel_svm(dfmtrain418, docvars(dfmtrain418, "Sentiment"))
dfmTestMatched_418 <- dfm_match(dfmTest_418, features = featnames(dfmtrain418))
actual_418 <- docvars(dfmTestMatched_418, "Sentiment")
predicted_418 <- predict(sentiment_SVM_418, newdata = dfmTestMatched_418)
confusion_418 <- table(actual_418, predicted_418)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion_418, mode = "everything")
```
Unfortunately, adding new data reduced the accuracy to be statistically indistinguishable from the no information rate. This is concerning to me in that it makes me question how this model will perform when I apply it to the rest of the comments I have pulled. As of writing this, I am unsure of what the correct next step is, but I am going to certainly reassess my pre-processing choices, and look more into tuning the SVM. 



