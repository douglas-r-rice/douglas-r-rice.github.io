---
layout: post
title: "Conspiracies on Reddit (Li)"
author: Zichao Li
date: 2021-03-14 12:24:54 -0500
categories: li
---

First, I finished the data collection part by collecting posts and comments by URLs of the reddit posts. This solved the problem of duplicated elements in a better way than what I did last week. 

```{r setup, include=FALSE}
library(RedditExtractoR)
library(writexl)
library(methods)
library(cleanNLP)
library(tidytext)
library(tidyverse)
library(quanteda)

#extract ulrs and remove duplicated urls
listurl <- uniquedf$URL[!duplicated(uniquedf$URL)]
listurl <- data.frame(listurl)

#collect reddit content based on url
for(i in listurl){
  
  temp <- reddit_content(i, wait_time = 2)
  
  
}

#write it into a dataframe and csv file
ultidata <- data.frame(temp)
write.csv(ultidata, "C:\\Users\\zclis\\Documents\\2021spring\\POLSCI 797TA text as data\\polisci797\\ultidata.csv")

#check average number of comments
avgx<-mean(table(as.factor(uniquedf$title)))
```

One obervation that might not be useful now is that the average number of comments of this data corupus is 159, which indicates that this subreddit is a quite active community. 

#Data Pro-processing
## generating word cloud
I did the mainposts, comments and all the text combined together separately to have a better idea of how the data looks like. 
```{r cars}
#pull out the content
mainpost <- ultidata$post_text
mainpost <- mainpost [!duplicated(mainpost)]

# create the corpus and dfm
post_corpus <- corpus(mainpost)
post_dfm <- dfm(post_corpus,
                tolower = TRUE,
                remove_punct = TRUE,
                stem = FALSE,
                remove = stopwords("english"))
post_dfm
topfeatures(post_dfm, 30)

#draw word cloud
set.seed(1234)
textplot_wordcloud(post_dfm, min_count = 50, random_order = FALSE)

#pull out comments and draw word cloud
comments <- ultidata$comment
comment_corpus <- comments
post_corpus <- corpus(mainpost)
comment_dfm <- dfm(comment_corpus,
                tolower = TRUE,
                remove_punct = TRUE,
                stem = FALSE,
                remove = stopwords("english"))
topfeatures(comment_dfm, 30)

set.seed(567)
textplot_wordcloud(comment_dfm, min_count = 300, random_order = FALSE)

#all data combined and word cloud
alltext <- c(mainpost, comments)
alltext
alltext_corpus <- corpus(alltext)
alltext_dfm <- dfm(alltext_corpus,
                   tolower = TRUE,
                   remove_punct = TRUE,
                   stem = FALSE,
                   remove = stopwords("english"))
topfeatures(alltext_dfm, 30)

set.seed(567)
textplot_wordcloud(comment_dfm, min_count = 300, random_order = FALSE)

alltext_dfm1 <- dfm(alltext_corpus,
                   tolower = TRUE,
                   remove_punct = TRUE,
                   stem = TRUE,
                   remove = stopwords("english"))
set.seed(567)
textplot_wordcloud(alltext_dfm1, min_count = 300, random_order = FALSE)
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-comment word cloud.pdf?raw=TRUE)
![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-post word cloud.pdf?raw=TRUE)
![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-all text word cloud.pdf?raw=TRUE)
![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-all text word cloud.pdf?raw=TRUE)
## Generating feature co-occurrence matrix


```{r pressure, echo=FALSE}
#trim dfm and creat feature co-occurence matrix
small_alltext_dfm <- dfm_trim(alltext_dfm, min_termfreq = 50)
small_alltext_fcm <- fcm(small_alltext_dfm)
dim(small_alltext_fcm)

#filter the top features to create the fcm
myFeatures <- names(topfeatures(small_alltext_fcm, 50))
smaller_alltext_fcm <- fcm_select(small_alltext_fcm, pattern = myFeatures, selection = "keep")

dim(smaller_alltext_fcm)

# compute size weight for vertices in network
size <- log(colSums(smaller_alltext_fcm))

# create plot
textplot_network(smaller_alltext_fcm, vertex_size = size / max(size) * 3)

```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-FCM.pdf?raw=TRUE) 

The figures don't look neat. It has some symbols and words that appear a lot but don't have actual meanings here. The data need to be cleaned better. 

## generating user network

Since the semantic network doesn't work too well, I tried to pull out a user network to see the interaction among users. 

```{}
usernetwork <- user_network(ultidata, include_author = TRUE, agg = TRUE)
usernetwork$plot 
```

I used user_network function from RedditExtractor, but it gets an error
"Error in gsub("</", "\\u003c/", payload, fixed = TRUE) : input string 1 is invalid UTF-8".

# Keywords in the wordcloud
One hypothesis that I had when I started this project is that covid vaccine conspiracy theories are politicized, compared to conspiracy theories about previous public health crisis, such as Zika virus conspiracy theories. 

Now in the wordcloud, we can see keywords like "trump", "China" and "government", but no clear clue of more words and how these words are engaged. That will be the next step to do in this project. 