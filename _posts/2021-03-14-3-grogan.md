---
layout: post
title: "Pre-Processing and Representing Reddit Data (Grogan)"
author: Helene Grogan
date: 2021-03-14 12:24:54 -0500
categories: grogan
---

In my last blog post, I described my efforts to create a corpus object using the Reddit data from r/QAnonCasualities that I gathered using RedditExtractoR, and how I had met with only partial success. Since then, I have taken the matrix of thread data with a separate line for each post and comment, and turned it into a corpus object where each thread is its own document. But first, I went back and added some code to make the thread *title* part of my reduced dataset. I realized that I might be missing some meaningful content by leaving it out, because many people use highly descriptive titles for their posts, and while many words in the title are likely to be included in the post itself, some might not be. So I decided to keep those in the corpus I will eventually be analyzing.

Then I wrote the following function to take that slimmed-down set of thread data and create a new matrix where each thread was collapsed into a single row. In this set of data, I am retaining only the thread_id I created, the date of the original post, and the combined text of the title, post, and comments. The thread_id will allow me to go back to the full set of thread data with usernames, network structure, and other metadata as necessary.

```r
collapse_threads <- function(content) {
    # Takes a matrix of reduced text content, with each comment on a separate line, and combines the text from each thread into one document
    # Args: content - matrix of Reddit content
    # Returns a matrix of the same content with each thread collapsed into one line
    
    # Create a list of unique thread IDs, not including NA values (if any)
    threads <- na.omit(unique(content[, "thread_id"]))
    
    # Initialize a matrix to hold the collapsed content of each thread,
    # retaining only the thread ID, the date of the original post, and the text
    collapsed_content <- matrix(nrow=length(threads), ncol=3)
    colnames(collapsed_content) <- c("thread_id", "date", "text")
    
    # Populate the thread_id column with the list of unique threads
    collapsed_content[, "thread_id"] <- threads
    
    # Then loop over those threads to populate the rest of the matrix
    for (id in threads) {
        # First, pull out the contents of just that thread from the full contents
        thread_content <- content[content[, "thread_id"]==id,]
        
        # Remove entries posted by the Auto Moderator, which is the same canned message in each thread
        thread_content <- thread_content[thread_content[, "user"] != "AutoModerator",]
        
        # Fill in the date field for this thread with the date of the initial post
        collapsed_content[collapsed_content[,"thread_id"]==id, "date"] <- thread_content[1, "date"]
        
        # Initialize a vector to hold the combined text of the whole thread,
        # and start it off with the thread title as the first piece of text
        thread_text <- na.omit(thread_content[1, "title"])
        
        # Then loop over each line of the thread
        for (i in 1:nrow(thread_content)) {
            # Paste together the text of each post and comment, omitting any NA values and trimming whitespace
            thread_text <- paste(thread_text, trimws(na.omit(thread_content[i, "text"])), sep = " ")
            
            # Then fill in the text column of the collapsed content line for this thread with the full text
            collapsed_content[collapsed_content[,"thread_id"]==id, "text"] <- thread_text
        }
    }
    return(collapsed_content)  # Return the matrix of collapsed content
}
```
After that, it was easy to convert that matrix into a dataframe, then run the quanteda corpus() method to create a corpus object for analysis. After creating the corpus, I saved it as an RDS file for easy loading later. Then I took a quick look using the summary() function.

```r
text_df <- as.data.frame(collapsed_content, stringsAsFactors = FALSE)
qcthreads <- corpus(text_df, docid_field = "thread_id", text_field = "text", unique_docnames = TRUE)
saveRDS(qcthreads, file="qcthreads.rds")

summary(qcthreads)
Corpus consisting of 3090 documents, showing 100 documents:

                                                       Text Types Tokens Sentences     date
  keqwqi/a_virtual_reality_game_designers_explanation_on_q/   243    452        20 17-12-20
       ker4qo/lost_my_bf_of_35_years_to_his_addiction_to_q/  1556   6147       347 17-12-20
              kesbwj/boyfriends_mom_is_a_giant_q_supporter/   213    359        21 17-12-20
    kesj04/cult_or_just_weird_podcast_episode_about_qanons/   182    320        12 17-12-20
```

## Tokenizing and Pre-Processing Decisions

Next, it was time to take a look at this corpus, and make some pre-processing decisions. Ultimately, I decided to tokenize based on whitespace, while also removing punctuation, URLs, and symbols. I did not remove numbers, because I expect some to be meaningful in this context, and I don't think there will be so many non-meaningful numbers that it will overwhelm the analysis. I also converted everything to lower case, then removed stop words. Here is a sample of the final tokenization:

```r
# Tokenize while removing punctuation, symbols, and URLs
qctokens <- tokens(qcthreads, remove_punct=TRUE, remove_symbols=TRUE, remove_url=TRUE)

# Convert to lower case
qctokens <- tokens_tolower(qctokens)

# Remove English stop words
qctokens <- tokens_select(qctokens, pattern = stopwords("en"), selection = "remove")

print(qctokens)

Tokens consisting of 3,090 documents and 1 docvar.
keqwqi/a_virtual_reality_game_designers_explanation_on_q/ :
 [1] "virtual"     "reality"     "game"        "designers"   "explanation" "q"           "analysis"   
 [8] "helpful"     "wrapping"    "head"        "around"      "people"     
[ ... and 182 more ]

ker4qo/lost_my_bf_of_35_years_to_his_addiction_to_q/ :
 [1] "lost"      "bf"        "3.5"       "years"     "addiction" "q"         "based"     "theories"  "twitter"  
[10] "knew"      "bf"        "trump"    
[ ... and 2,680 more ]

kesbwj/boyfriends_mom_is_a_giant_q_supporter/ :
 [1] "boyfriends" "mom"        "giant"      "q"          "supporter"  "boyfriends" "mother"     "huge"      
 [9] "q"          "supporter"  "posts"      "12"        
[ ... and 155 more ]

kesj04/cult_or_just_weird_podcast_episode_about_qanons/ :
 [1] "cult"       "just"       "weird"      "podcast"    "episode"    "qanon's"    "victims"    "amp"       
 [9] "casualties" "greetings"  "qanon"      "casualties"
[ ... and 127 more ]

```
There were a few more numbers than I was expecting, so I may end up going back and removing numbers after all. But some numbers, like 17 for Q (which is the 17th letter of the alphabet) and references to "January 6th" as shorthand for the insurrection at the Capitol, are likely to be important, so for now I'm going to leave numbers in.

## Document-Feature and Feature Co-occurrence Matrices

My other major effort since last time was to start taking a look at the distribution of tokens within my corpus. First, I created a document-feature matrix (dfm) from the tokenized corpus shown above. Looking at the top 20 features yields this list:

``` r
topfeatures(qc_dfm, 20)

people   just   like      s    can      q      t   know  think    get    one  trump   even    now really   time 
 19014  17099  15303  14909  13701  12839  12174   9792   9723   9534   8679   7810   7524   7305   7155   6664 
 going  qanon family      m 
  6476   5992   5955   5871 
```
You'll see that this includes quite a few single letters; the "q" one makes sense in this context, but I figured that the others (s, t, and m) were likely to be remnants of contractions (like *it's*, *can't*, and *I'm*) that maybe had some extra whitespace accidentally included. Then when the punctuation and stopwords were removed (the *it'*, *can'*, and *I'* in those contractions), just those single letters were left. I took a look using kwic (keyword-in-context) on the original, pre-tokenized corpus and discovered that this was overwhelmingly the case. So I removed those standalone letters from the tokenized corpus. I also removed the word "don," which I also found to be a remnant of "don't" that remained due to unintended whitespace that prevented it from being removed as a stopword. After that, the top features looked a lot cleaner.

```r
qctokens <- tokens_select(qctokens, pattern = c("s", "t", "m", "don"), selection = "remove")
qc_dfm <- dfm(qctokens)

topfeatures(qc_dfm, 20)

people   just   like    can      q   know  think    get    one  trump   even    now really   time  going  qanon 
 19014  17099  15303  13701  12839   9792   9723   9534   8679   7810   7524   7305   7155   6664   6476   5992 
family   also   good    see 
  5955   5777   5628   5494 
```
Creating a word cloud of tokens that appeared at least 50 times produced this:

![word cloud of common words](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Grogan-3-wordcloud_1.png?raw=TRUE)
  
I should note that the dfm at this point was a matrix of 3,090 documents and 50,938 features, which was 99.3% sparse (ouch). Turning it into a feature co-occurrence matrix (fcm), then, produced a 50,938 x 50,938 matrix, which was super unwieldy to work with. So instead I created a smaller dfm, choosing only terms that were used at least 20 times and appeared in at least 20% of threads. Those numbers were fairly arbitrary, but when I played around with them I found it interesting that it was the document frequency that made the most difference in the number of terms that ended up in the smaller dfm. I'm not really sure what should guide my decisions about limiting the data in this way, but since I only wanted to take a preliminary look, I decided not to worry about it.

```r
smaller_dfm <- dfm_trim(qc_dfm, min_termfreq = 20)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .20, docfreq_type = "prop")

smaller_dfm
Document-feature matrix of: 3,090 documents, 299 features (67.1% sparse) and 1 docvar.
```

Creating an fcm from this smaller matrix was a lot easier, and of course gave me a 299 x 299 feature co-occurrence matrix. Since I wanted to make a quick network plot of the co-occurrences, though, I thought 300 terms would make it too hard to read. *(Spoiler: I came back and did this anyway. See below.)* To create a smaller plot, I trimmed the fcm down even further to just the top 50 terms, then plotted their connections to each other.

```r
# Identify the top 50 features by name (word)
myFeatures <- names(topfeatures(smaller_fcm, 50))

# Trim the fcm down to just those features
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# Compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# Create network plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)

```
![network plot of top 50 features](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Grogan-3-textnetwork_1.png?raw=TRUE)

So that's really interesting, and starts to show the frequency of the word "cult" in the discussions on this forum, as well as how many people are talking about their parents being involved with QAnon beliefs. Then, just for fun, I did the thing I knew would be hard to read, and plotted the network of words using the full 299-word "smaller_fcm," and...I kind of love it. Check it out:

![network plot of top 299 features](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Grogan-3-bigtextnetwork_1.png?raw=TRUE)
  
The connections are impossible to follow, but it's still interesting to see what words show up, and which ones are the most central. I'm looking forward to getting into dictionary approaches next time, and refining my approach to analyzing the content of discussions on this forum.





