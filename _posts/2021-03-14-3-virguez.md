---
layout: post
title: "A First Glance at the Amici Data (Virgüez)"
author: Santiago Virgüez
date: 2021-03-14 12:24:54 -0500
categories: virguez
---

In the previous blog post, I explained how to upload the data (the CCC's opinions) in to a data frame in R. There are hundreds of things you can analize with this data, but as you recall, I'm interested in analizing the amicus briefs filed before the court. An **amicus curiae** or **friend of the court** is someone who is not a party to a  case but assists a court by offering information, expertise, or insight that has a bearing on the issues in the case. Of course, not being a party to a case is different than being interested in the outcome of the same case, so you'll see dozens of interested individuals and advocacy groups filing legal briefs to influence  courts decisions.

There are some reasons why it is interesting and important to study amici participation before the courts. [Here you can find some of the reasons and more information about previous studies] (https://www.annualreviews.org/doi/abs/10.1146/annurev-lawsocsci-101317-031248). Personally, I'm interested in understanding how those advocacy groups come to work together to increase their probabilities of influence. So, to do this I need to extract only the information about the amici interventions filed in each case. As you can see, each opinion (at least for abstract constitutional review) has a subsection where the court summarizes the briefs filed. This subsection is usually (but not always and that will be a further problem) titled **"Intervenciones"** or "interventions" in English. 

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-blog3-picture1?raw=TRUE)

[picture 1]

To extract this information from the opinions we can use the following loop (created by Doug Rice):

```{r}
# set up character vector to hold interventions
interventions <- rep(NA, length(opinions))

# pull intervenciones
for (i in 1:length(opinions)){
  
  # set up vector for opinion
  tmp <- opinions[[i]]
  
  # look for intervention start
  first <- which(str_detect(tmp, "\\s+INTERVENCIONES")==TRUE) 
  
  # skip the rest if there's no intervention section
  if(length(first) == 0){next}
  
  # figure out what section it is
  section <- str_extract(interventions[1], "[I]?[I]?[V]?[I]?[I]?\\.")
  
  # there's probably a better way to do this but it's late and I'm tired.
  # look for intervention end; subtract 1 to make sure you aren't including the next section marker
  if (section == "III."){last <- (which(str_detect(tmp, "^\\s?IV\\.\\s+")==TRUE))-1}
  if (section == "IV."){last <- (which(str_detect(tmp, "^\\s?V\\.\\s+")==TRUE))-1}
  if (section == "V."){last <- (which(str_detect(tmp, "^\\s?VI\\.\\s+")==TRUE))-1}
  if (section == "VI."){last <- (which(str_detect(tmp, "^\\s?VII\\.\\s+")==TRUE))-1}
  
  # collapse elements to single "document" and add to the interventions vector
  interventions[i] <- paste(tmp[first:last], collapse = "\n")
  
}
```

Although an elegant code, the mentioned inconsistency in the structure of the subsection  "intervenciones" - which is also called "opiniones expertas" or "intervencion de ciudadanos", among other possible ways - make it difficult to obtain this specific information using this loop. I will keep working on this. In the meantime, lets work with the text summaries of a sample of 363 abstract constitutional cases and ~ 1,125 amicus briefs filed by different advocacy groups, individuals, and government agencies.

```{r}
library(textreadr)
library(tidyverse)
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)

amicus_briefs <- read.csv("ByAmicus.csv")
```

Once I upload the dataset that includes the summaries and the name of the intervener, I convert these summaries to corpus objects that are easier to work with. Before doing this, I use the function "trimws" to remove leading and/or trailing whitespace from character strings. 

```{r}
amicus_briefs$SUMMARY <- trimws(amicus_briefs$SUMMARY)

briefs_corpus <- corpus(amicus_briefs$SUMMARY)

briefs_summary <- summary(briefs_corpus, n = 1195)

# create a amicus indicator
briefs_summary$amicus <- amicus_briefs$AMICI_NAME

# add the metadata
docvars(briefs_corpus) <- briefs_summary
```

As first step of the text analysis is converting the summaries from their written format to a numerical representation. To do that, I create a document-feature matrix directly from the corpus object. 

```{r}
# create the dfm
briefs_dfm <- dfm(briefs_corpus)


# create the dfm
briefs_dfm <- dfm(briefs_corpus,
                              tolower = TRUE,
                              remove_punct = TRUE,
                              stem = FALSE,
                              remove = stopwords("spanish")
)

# find out a quick summary of the dfm
briefs_dfm
```

With the dfm we can learn some basic information about the briefs. Here is a fancy not-fancy word cloud that represents the frequency of the most common terms throughout the briefs.

```{r}
set.seed(1234)

# draw the wordcloud
textplot_wordcloud(briefs_dfm, min_count = 50, random_order = FALSE)
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-blog3-picture2?raw=TRUE)

We can also turn to the feature co-occurrence matrix and construct a matrix that "presents the *number of times word{a} appears in the same document as word{b}". Using this we can visualize a semantic network using the function "textplot_network()".

```{r}
#smaller dfm by limiting to words that appear frequently and are in more than 10% of briefs
smaller_dfm <- dfm_trim(briefs_dfm, min_termfreq = 5)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .1, docfreq_type = "prop")

#create fcm
smaller_fcm <- fcm(smaller_dfm)


#create semantic network
myFeatures <- names(topfeatures(smaller_fcm, 30))

even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-blog3-picture3?raw=TRUE)

We can see that, as one might expect, the commond words connected are "rights", "court", "law", "intervention", "article", among others. This give us an idea about how there is still a long way to go to trim more stopwords that don't give us any interesting information.

Finally, we can use the library("quanteda.textstats") to obtain other types of visualizations like a frequency plot:

```{r}
library("quanteda.textstats")
features_dfm_briefs <- textstat_frequency(briefs_dfm, n = 100)

# Sort by reverse frequency order
features_dfm_briefs$feature <- with(features_dfm_briefs, reorder(feature, -frequency))

ggplot(features_dfm_briefs, aes(x = feature, y = frequency)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/virguez-blog3-picture4?raw=TRUE)
