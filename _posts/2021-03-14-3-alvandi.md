---
layout: post
title: "Toxic Comments Classification (Alvandi)"
author: Amirhossein Alvandi
date: 2021-03-14 12:24:54 -0500
categories: alvandi
---


<style>
body {
text-align: justify}
</style>

## Introduction

Nowadays, It is almost impossible to engage in online conversations without witnessing toxic behavior like unwarranted harassment or disrespectful behavior. Conversational toxicity is a growing issue that can lead people to stop genuinely expressing themselves and give up on seeking others' opinions out of fear of abuse or harassment. Because of the massive volume of comments, it has become more critical to find a practical solution to identify and classify toxic comments in a more efficient manner. For instance, New York Times and Perspective API have developed a moderator tool that can find patterns, spot abusive language, and score all comments based on their toxicity level. Accordingly, they can provide live feedback to commenters and help human moderators sort comments much faster.

In this project, we will build a model capable of detecting toxic language inside comments and distinguishing between different types of such content, including obscenity, threat, or other toxicity. Basically, the created model will predict a probability for each comment belonging to each kind of toxicity category. To this end, we will apply various classification techniques, and by analyzing the number of false negatives and false positives, we get insights about open challenges that all of the approaches share.

## Corpus

We use the Wikipedia Comment Dataset containing 159,563 annotated user comments collected from Wikipedia talk pages and is the largest publicly available for the task. These comments were annotated by human raters with the six labels ‘toxic’, ‘severe toxic, ‘insult’, ‘threat’, ‘obscene’ and ‘identity-hate’. Comments can be associated with multiple classes at once, which frames the task as a multi-label classification problem.

## Importing data

Here, we import the csv file of data in R. The test data set which is consisted of only two columns (comment id, and comment text) will be used later when we compare the accuracy of various models in predicting the toxicity probabilities of the comments.

```{r,message = FALSE, warning = FALSE}
#install.packages("devtools")
#install.packages("tidytext")
#install.packages("plyr")
#install.packages("tidyverse")
#install.packages("quanteda")

# load libraries
library(devtools)
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(xtable)
library(caret)
library(gridExtra)
```



```{r,message = FALSE, warning = FALSE}
### import the training csv file
train_data = read_csv("Data/train.csv")
test_data = read_csv("Data/test.csv")
# getting the dimmension of the training data
dim(train_data)
```

As we can see, there are a total of 159571 comments in the training data set. For each of the comments there exist 6 different labels indicating the type of toxicity of the given comment. Let us have a look at the beginning rows of our data set.

```{r,message = FALSE, warning = FALSE}
# viewing the first 5 rows of the data
head(train_data,n = 5)
```

We can observe in the above cell that the first two columns contain the comment id and comment text, and each of the next six columns contain a binary random variable where we have "1" indicating that we have some type of a toxicity in the corresponding row, and zero otherwise. Now, we may plot a bar chart of the data in order to better understand the distribution of the various kinds of toxicity.

```{r,message = FALSE, warning = FALSE}
### Finding the counts for each type of toxicity
toxic_labels = colnames(train_data[3:8])
toxic_counts = as.numeric(colSums(train_data[,3:8]))
bar_data = tibble(cbind(toxic_labels = toxic_labels, toxic_counts = toxic_counts))
### Plotting the histogram of various toxicity types
ggplot(data = bar_data, aes(x = toxic_labels, y = toxic_counts, fill = toxic_labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Toxicity Type")
```


![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_blog3_fig1?raw=TRUE)


From the bar plot, it is apparent that we have the largest proportion for the toxic comments, however that only covers about 10 percent of the total number of comments in this data set, and the smallest count is for comments including a threat. 


## Data Pre-processing

Data pre-processing is one of the important steps in any text data analysis task. Here, using the 'tokens' function, we removed all punctuation marks, numbers, urls, and English stop words. All characters then were converted into lower case to avoid repetitions in the set of unique words. An example of both raw and pre-processed versions of the first commment is provided below.  


```{r}
# pre-processing step
# removing numbers, puncs, and urls  
comment_processed = tokens(train_data$comment_text,
                                remove_numbers = TRUE,
                                remove_punct = TRUE,
                                remove_url = TRUE)

# converting all tokens to lower case
comment_processed = tokens_tolower(comment_processed)

# removing stop words
comment_processed = tokens_select(comment_processed, 
                                  pattern = stopwords("en"),
                                  selection = "remove")
# Print both versions
cat("Before Pre-processing:","\n", train_data$comment_text[1],"\n",
            "After Pre-processing:","\n",comment_processed[[1]])
```


It is also useful to find out how many multi-label comments are present in our data. Since, when hand labeling the comments, one's content can be considered  obscene and insulting at the same time.


```{r}
# Find the number of comments with more than one toxicity type
multi_label = 0
for(i in 1:nrow(train_data)){
  if(sum(train_data[i,3:8]) > 1){
    multi_label = multi_label + 1   
  }
}

multi_label
```

As we can observe, 9865 from the total of 15294 toxic comments have multiple toxicity labels.


## Subsampling for Class Imbalances

It is apparent that we have a high level of class imbalance in our data set, where only about 10% of the total comments are toxic. Here, the detection of toxic comments is our primary interest; Therefore, it is very likely that a classification model would be able to achieve very good specificity since about 90% of the comments are non-toxic. Sensitivity, however, would likely be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.

One way to alleviate this issue is to sub-sample the data. There are a number of ways to do this but the most simple one is to sample down the majority class data until it occurs with the same frequency as the minority class. While it may seem counter-intuitive, throwing out a large percentage of your data can be effective at producing a useful model that can recognize both the majority and minority classes. In some cases, this even means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are better calibrated, meaning that the distributions of the class probabilities are more well behaved. As a result, the default 50% cutoff is much more likely to produce better sensitivity and specificity values than they would otherwise.

```{r}
# Downsmaple the non-toxic comments (majority class)
train_data_balanced = downSample(x = train_data$comment_text, y = as.factor(train_data$toxic))

# Distribution of comments before downsampling
labels = c("toxic", "non-toxic")
counts = c(sum(train_data$toxic == 1), sum(train_data$toxic == 0))
bar_data_imb = tibble(cbind(labels = labels, counts = counts))
p1 = ggplot(data = bar_data_imb, aes(x = labels, y = counts, fill = labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Before Down-sampling")

# Distribution of comments after downsampling
counts_balanced = c(sum(train_data_balanced$Class == 1), sum(train_data_balanced$Class == 0))
bar_data_bal = tibble(cbind(labels = labels, counts_balanced = counts_balanced))
p2 = ggplot(data = bar_data_bal, aes(x = labels, y = counts_balanced, fill = labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "After Down-sampling")

grid.arrange(p1, p2, ncol = 2)

```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_blog3_fig2?raw=TRUE)


After down-sampling from the data, we have a total of 30588 comments.



## Classification Methods

In order to predict the toxicity class for each comment using its words, we begin by classifying each comment as toxic or non-toxic (binary classification), and then, we use a multi-label classification to determine the type of toxicity for each of the toxic comments. Here, we provide a brief description of the different classification models that we utilize for our data analysis. 

#### Naive Bayes 

The Naive Bayes classifier is one of the most renowned classification models in text data analysis. Applying the Bayes theorem to calculate the probability that a new comment is toxic given that the comment content is given by,

$$Pr(Toxic|Comment's \hspace{0.1cm} content) = \frac{Pr(Comment's \hspace{0.1cm} content|Toxic)Pr(Toxic)}{Pr(Comment's \hspace{0.1cm} content)}$$
The probability of a comment being toxic ($Pr(Toxic)$) can be calculated as the proportion of toxic comments in our training data set. Here, the naive independence assumption let us to  compute the likelihood of a comment being toxic only by finding the product of the word appearing or not appearing in the toxic words pool. Then, the class of a new comment is given by

$$ \frac{Pr(Toxic|Comment's \hspace{0.1cm} content)}{Pr(Non-Toxic|Comment's \hspace{0.1cm} content)}$$
Now, in order to simplify the computations by taking log, we have

$$ \log (\frac{Pr(Toxic|Comment's \hspace{0.1cm} content) +0.5}{Pr(Non-Toxic|Comment's \hspace{0.1cm} content) + 0.5})$$
where the 0.5 corrections are  used to avoid division by zero.


#### Support Vector Machine (SVM) with Naive Bayes Features

Support Vector Machine is a classification method that is based on the principle of finding hyper-planes that distinctly classify the data units when you plot them onto an $n$-dimensional graph where $n$ represents the number of features of the data. In Naive Bayes - Support Vector Machine the probabilities calculated in the multinomial Naive Bayes are then fed to SVM to classify. NBSVM is observed to give better results than a simple NB classifier or SVM classifier when used separately.


#### Long Short Term Memory (LSTM)

Recurrent Neural Networks (RNN) refer to a  class  of neural networks that contain cyclic connections. The output of a given hidden layer is fed back to itself in an RNN to remember some information as the memory from previous computations. This makes RNNs a powerful tool for text classification. Long Short Term Memory (LSTM) is an RNN that can learn long term dependencies which is a challenging task for traditional RNNs. An LSTM model, like RNN, has a chain-like architecture where each unit of this repeating structure is called an LSTM cell.

An LSTM cell contains an input gate, an output gate and a forget gate that regulates the data which is flown into and outside of the cell. The forget gate decides what information it's going to discard from the current cell. The input gate then decides what new information is going to be added to modify the current state of the memory and finally, the output gate decides what information leaves the cell.


## Performance Evaluation Metrics

There are a wide range of metrics (loss functions) available in order to evaluate the performance of different classification models. Here, we used four different evaluation metrics as described below.

#### Precision

Classification precision is one of most commonly used metrics which is defined as the fraction of relevant instances among the retrieved instances. In other words, the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.

$$ Precision = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + False \hspace{0.1cm} Positive} $$
For instance, in our case, the "True Positive" is the number of toxic comments that are classified as toxic by the classification model.


#### Recall

Recall (also called sensitivity) is the fraction of relevant instances that were retrieved or equivalently, the number of true positive results divided by the number of all samples that should have been identified as positive. 

$$ Recall = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative} $$

#### F Measure

The F measure is the harmonic mean of the precision and recall. The highest possible value of an F measure is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. 

$$ F = \frac{2}{Recall^{-1} + Precision^{-1}} = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + \frac{1}{2}(False \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative)} $$
The more generic $F_{\beta}$ measure applies additional weights, valuing one of precision or recall more than the other.

#### ROC AUC

The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the sensitivity against false positive rate at various threshold values. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. Therefore, the higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.

#### Future Direction
###### 1. Look into different vectorization techniques for converting words into feature vectors

###### 2. Implement GloVe embedding to train the classification models



















