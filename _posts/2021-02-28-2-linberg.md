---
layout: post
title: "Organizing the Parler Corpus (Linberg)"
author: Steve Linberg
date: 2021-02-28 12:24:54 -0500
categories: linberg
---


Work progresses on the Parler data set. One of the main challenges with this set, as discussed previously, is its size; it is about 183 million posts, and analyzing it is going to require some planning.

The archive consists of approximately 160 separate `.ndjson` files, each around 1gb uncompressed, and containing approximately 1 million comments or posts. So far I have examined some of this data manually, just via the RStudio viewer, to continue to get a sense of its structure and scope. Although the [paper](https://arxiv.org/abs/2101.03820) I am using as a starting point for my analysis provides a short description of some of the key fields, it makes reference to a document from the [archive](https://doi.org/ 10.5281/zenodo.4442460) that provides a complete dictionary; unfortunately, this document appears to be missing. I have emailed the lead author to ask if it is available somewhere.

My first examinations of the data from the archive involved just parsing the first thousand lines out of the first data file, from which I was able to make the following observations:

- The `.ndjson` format is very convenient for quickly subdividing the large files into smaller chunks, since it means one record (post or comment) per line of the data files.
- There is no particular order to the lines in the data files that I can glean so far (which is fine) - they are not organized by date, thread, or poster as far as I can tell.
- The paper authors warned that some of the language from the posts in the data set "might be toxic, racist, and hateful, and can overall be disturbing," and my own experience so far bears that out; some of it can be pretty shocking.
- Some posts appear to be blank, or empty of content, and I'm not sure yet why this would be; I do not have a Parler account, and my only familiarity with it to this point is from reading this paper and looking at the raw data that the authors scraped before it went down in mid-January. It is possible that the empty posts are placeholders for deleted content, or that there is some other attribute of post content that is not reflected in the data set. I do not have a sense yet of the scope of this issue, and whether it will be significant or not.
- The paper authors did some processing of their own in the data set, including creating versions of the `body` field with and without URLs, and separating out hashtags and referenced media, which can be helpful to build on; the data has all been anonymized, but usernames are preserved. 
- The dataset includes a separate dump of approximately 4 million Parler users, which I have not yet looked at, and I'm not sure if it will be relevant or not.

(It is also worth noting that Parler has come back online in recent weeks, and found new hosting outside the US. The API that the authors used to scrape the original data has been closed, and the links (of which there are vast numbers) to Parler-hosted media, images and video, no longer work. At least at this point, I am not planning to attempt to create a Parler account myself to get access to data from the inside, but will continue working solely with this archive.)

Since I wasn't sure whether anything about the physical sequencing of the lines in the files had any significance, I decided to pick one of the files nearer the middle of the set to continue examining. Working with a full uncompressed json files is not ideal for performance, so an early priority has been to get a subset of data that can be worked with more efficiently while I continue to focus on the research question and methods. I anticipate running whatever analysis I end up with against the full dataset, but for practical purposes I want to keep things a bit leaner in the early going. The process was:

1. Extract a single file from the large archive
```{sh, eval=FALSE}
[slinberg@fifteen Parler_data]$ unzip parler_data.zip parler_data000000000080.ndjson 
Archive:  parler_data.zip
  inflating: parler_data000000000080.ndjson  
```

2. Import it into R with the `ndjson` library
```{r, eval=FALSE}
library("ndjson")
data_filename <- 'parler_data000000000080'
full_path <- paste('~/Parler_data/', data_filename, '.ndjson', sep='') 
df <- ndjson::stream_in(full_path)
```

3. Filter the large data frame down by the following steps
  - only include elements with at least 1 character in the body
  - convert the text field `createdAtformatted` into an actual POSIX date that we can use for subsequent filtering
  - use this new date field to only include content starting in September of 2020 (the archive goes back several years, but this is the time period I want to look at)
  - preserve the `body`, `createdAtPosix`, `datatype`, `urls.0.metadata.mimeType`, and `urls.0.long` fields, the latter 3 relating to media material within posts (images, videos, links)
```{r, eval=FALSE}
filtered_df <- df %>%
  filter(nchar(body) > 1) %>%
  mutate(createdAtPosix = as.POSIXlt(createdAtformatted, tz="America/New_York")) %>%
  filter(createdAtPosix > as.POSIXlt("2020-09-01 00:00:00", tz="America/New_York")) %>%
  select("body", "createdAtPosix", "datatype", "urls.0.metadata.mimeType", "urls.0.long")
```

4. Write the data out to a new .Rdata file.

```{r, eval=FALSE}
output_path <- paste('~/Parler_data/', data_filename, '.Rdata', sep='') 
save(filtered_df, file=output_path)
```

The `.Rdata` file is only 22mb, compared to the file's original size of 883mb, and contains around 430,000 posts and comments. Working with it is noticeably faster and easier.

With this data file created, it can then be examined with the base R tools, and analyzed with the `quanteda` library.

Load the file:

```{r, eval=FALSE}
data_filename <- 'parler_data000000000080'
full_path <- paste('~/Parler_data/', data_filename, '.Rdata', sep='') 
load(full_path)
```

Create a corpus and a summary using the `body` field for text:

```{r, eval=FALSE}
parler_corpus <- corpus(filtered_df, text_field="body")
parler_summary <- summary(parler_corpus)
```

Extract the tokens from the corpus, removing punctuation, numbers, URLs and symbols (there is a lot of emoji in this data):

```{r, eval=FALSE}
parler_tokens <- tokens(parler_corpus,
    remove_punct = T,
    remove_numbers = T,
    remove_url = T,
    remove_symbols = T
    )
```

This results in a corpus of what I presume are unusual dimensions, 430,000 rows long, mostly quite short.

Next up is probably to begin making n-grams and continue with analysis as we learn further techniques in class.

My main interest is still in whether I can determine an overall state of sentiment in the chatter around the January 6 insurrection, comparing the state of things in the runup to January 6, the day itself, and the days following.

There is still a fair amount of this that I don't know how to do, but I know we will be covering material in the weeks ahead that will inform these tasks.

A secondary area of interest is whether the quality of language in this particular type of data poses any challenges to the corpus analysis tools themselves. The use of memes, images, links to videos and use of hashtags and codewords (such as "wwg1wga", apparently a popular Qanon saying meaining "Where we go one, we go all") could concel or obfuscate meaming that may be significant, but hard to process with the available tools.