---
layout: post
title: "Political Name-Calling (Tran-Trinh)"
author: Nathan Tran-Trinh
date: 2021-03-28 12:24:54 -0500
categories: trantrinh
---

The amount of datasets proved to be highly unwieldly from the last blog, and I also suffered a data loss from my last session due to a memory failure on my laptop. Resultantly, I reduced the number of terms analyzed to 4, those 4 being personally selected as ones I recognize as largely co-opted by the opposite political wing of those who used it as a pejorative. I also limited myself to 5 subreddits that represented different factions of the left and right wing in America. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
install.packages("quanteda")
library(quanteda)
install.packages("RedditExtractoR")
library(RedditExtractoR)
install.packages("devtools")
devtools::install_github("whereofonecannotspeak/pushshiftr")
devtools::install_github("geoffwlamb/redditr")

library(redditr)
library(readr)
library(dplyr)
library(pushshiftr)
```


* cuck
* soyboy
* simp
* socialist

* r/The_Donald
* r/breadtube
* r/ChapoTrapHouse
* r/Conservative
* r/neoliberal

Using the PushShift API, I once again collected 100 comments from bi-yearly periods based on each subreddit and buzzword combination. Even before any further analysis, it was clear that in many cases, words were popularized amongst one subreddit long before any others, this phenomenon being most obvious with r/the_donald, who saw heavy usage of every word before any other subreddits with the occasional exception of r/conservative and the exception of the term "socialist", which is less of a novel term than the other 3, at least within a political context. I then (somewhat tediously) merged all of my dataframes one-by-one using the rbind function as follows

```{r}
dff <- rbind(dff,df1)
```

Repeating that line of code after recoding all of my re-obtained dataframes to drop all columns except for body, date created and subreddit. Following that, I recoded my combined dataframe into a corpus and then into a document feature matrix, removing punctuation and stopwords as well as converting the text into all lowercase.

```{r}
dff_corpus <- dff$body
dff_dfm <- dfm(dff_corpus,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
dff_dfm
topfeatures(dff_dfm, 20)
```

I also split the large dataset into 5 datasets based on subreddit and created a corpus and document feature matrix for each individual one (theoretically, I could've done this before doing the entire combined dataset but had not thought of that at the time)

```{r}
AA <- split(dff, dff$subreddit)
AABreadTube <- AA$BreadTube
AAChapoTrapHouse <- AA$ChapoTrapHouse
AAConservative <- AA$Conservative
AANeoliberal <- AA$neoliberal
AATD <- AA$The_Donald

corpus_BreadTube <- AABreadTube$body
corpus_ChapoTrapHouse <- AAChapoTrapHouse$body
corpus_Conservative <- AAConservative$body
corpus_neoliberal <- AANeoliberal$body
corpus_TD <- AATD$body

BreadTube_dfm <- dfm(corpus_BreadTube,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
BreadTube_dfm
topfeatures(BreadTube_dfm, 20)

ChapoTrapHouse_dfm <- dfm(corpus_ChapoTrapHouse,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
ChapoTrapHouse_dfm
topfeatures(ChapoTrapHouse_dfm, 20)

Conservative_dfm <- dfm(corpus_Conservative,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
Conservative_dfm
topfeatures(Conservative_dfm, 20)

neoliberal_dfm <- dfm(corpus_neoliberal,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
neoliberal_dfm
topfeatures(neoliberal_dfm, 20)

TD_dfm <- dfm(corpus_TD,
               tolower = T,
               remove_punct = T,
               stem = F,
               remove = stopwords("english"))
TD_dfm
topfeatures(TD_dfm, 20)
```

The split output has some rather surprising results, such as that the terms "soyboy" and "cuck" were used more heavily within r/neoliberal than that of r/the_donald (whereas centrist Democrats are heavily stereotyped as being focused on "respectability politics" moreso than both the entire right-wing and far-left factions that frequently refer to themselves as "dirtbag"), as well as the term "simp" not even being amongst the top 20 most common terms within r/conservative or r/ChapoTrapHouse.

Attempts to create wordclouds STILL have not been fruitful and I am not sure why, even after reloading quanteda. Nevertheless, I am hoping that by next blog I have different methods of visualization that incorporate multiple variables, ones that allow for comparison between date, comment content and subreddit to see how popularization of buzzwords amongst different political factions changes over time.