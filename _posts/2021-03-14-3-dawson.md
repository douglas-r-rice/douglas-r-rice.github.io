---
layout: post
title: "Once more (again), with different data (Dawson)"
author: Nick Dawson
date: 2021-03-14 12:24:54 -0500
categories: dawson
---

My original corpus was meant to be the set of oral arguments and associated opinions generated by the Supreme Court during the telephonic argument era of CoVID. With the original, rather undefined goal of asking, "what can we learn," this data-set seemed appropriate. But, having moved to "can we predict from oral arguments when a justice will write an opinion (or just a dissent)," I needed to look elsewhere for more data.

Thankfully I had spent a good bit of time working with RegEx, and I felt confident I could parse the lot of the transcripts posted and get back on track in little time.
I was wrong. Not for want of RegEx skills, but for want of better transcripts.

Prior to about 2005, Supreme Court transcripts looked like this:

![A page from an oral argument transcript.](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-question.png?raw=TRUE)

There was never a Justice Question -- this is how the reporter marked any justice's question.


This did not immediately seem fatal to the project. But it did arbitrarily limit our data to cases post-2005.
At this point my approach was to pick a justice, and see if we could start by predicting their votes.
With the 2005 cutoff, we're looking at the career of those to the right of the yellow line:

![A chart of Supreme Court Justice timelines.](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-cutoff.png?raw=TRUE)


How to choose? We want a Justice who neither speaks little at argument (Thomas), or dissents at a significantly higher rate than others (Alito). We also don't want the Chief, because the unique nature of their role certainly affects their behavior in a way we aren't equipped to study. We also want as much of a Justice's career as possible -- any result that only analyzed 70% of their cases would be hard to declare conclusive. 
Having eliminated Thomas, Roberts, and Alito, we're left with a few justices.
Justice Ginsburg, sadly, is removed from consideration due to her untimely death. 
Justices Gorsuch, Kavanaugh, and Barrett have spent too little time to say much conclusively.

So we're left with Kagan, Sotomayor, and Breyer.

I chose Justice Breyer. He both speaks and writes a lot, and previous predictive work has found notable success with analyzing his writing 
(e.g., USING ALGORITHMIC ATTRIBUTION
TECHNIQUES TO DETERMINE AUTHORSHIP IN UNSIGNED JUDICIAL OPINIONS, 16 STAN. TECH. L. REV. 503 (2013) http://stlr.stanford.edu/pdf/algorithmicattribution.pdf)

## Working with the data

At this point I've chosen Breyer and accepted the fact that I will only be studying a limited set of his data. This was not the most satisfying decision, but I set to work parsing the transcripts and opinions.
Using the Supreme Court Database's Justice-centered data to determine which cases Breyer wrote, I processed the transcript text into data frames of his speech, tagged with the case and vote.  
I manually processed a few cases, but quickly realized that the time spent (and my error rate) made processing a significant number of cases unthinkable.  

A few days into some simple analysis of what data I had were unsatisfying. But then I had a breakthrough (which in retrospect I'm surprised took me so long).

## Finding New Data
For some reason, I wanted to show my partner a funny clip from Bush v. Gore where Scalia reminds the advocate who he is.
On the transcript, that exchange looks like this:

![A funny Scalia exchange.](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-bush transcript.png?raw=TRUE)

This transcript is actually wrong. I don't blame the court reporter, as there's a lot of laughter, but it's actually Scalia that says "Correct me."
How do I know that?

Well, when I wanted to play the clip I didn't look for the transcript. I looked on Oyez, which has transcripts synchronized to the audio.

What does that same passage look like there?    
![The same exchange on Oyez.](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-bush oyez.png?raw=TRUE)

Oyez knows which justice said what; they have solved the Justice Question problem!
I quickly emailed Oyez's David Kemp who responded in hours, giving me access to their bulk data.
70+ years of argument, all neatly formatted in JSON files.

This opened up the possibilities for analysis. Every justice since about 1950 has their oral argument data in these files.

At this point, I pivoted from studying Justice Breyer to Justice Stevens. His 35-year career is the third-longest of any justice, and the longest of any justice for whom complete data is available. So now the data set has expanded to include the cases Stevens heard, from 1975 to 2010.

## Working with new data

These files have a somewhat complex structure. For example, here's a highlight of how the text, timing, and speaker's name relate in the structure.
![Highlighting the relationship between JSON objects.](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-JSON structure.png?raw=TRUE)  
Highlighted in yellow is "turns," which represents each side's argument (thus turns usually is 1:2, but in cases with amicii might be as much as 1:5).
In green is the list of speakers. For each index of the list of speakers, text_blocks has the text that person spoke.

These files have all the information about the oral argument, as well as the short case name. It doesn't have an answer to the question, "did this justice write an opinion," nor does it say how they voted.

Here's a sample of the SCDB's Justice-centered data, showing where I can find that information:  
![SDCB structure](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-SCDB structure.png?raw=TRUE)

The columns we care about are highlighted in green. The first is the justice's name (in an odd form, which was dealt with similarly to how case names will be handled). "Vote" answers the question "did they dissent," with 1:no, 2: yes (and up to 8 for weird votes we don't care about). "Opinion" similarly answers the question, "did they write an opinion?"

But how do we get these files to interact? I want to get the text out of one file, and tag it with the vote and opinion data from the other.
Maybe we can compare case names?  

Here's how the SDCB lists a case name, with the full title nobody ever uses. 
![SDCB structure](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-scdb names.png?raw=TRUE)  


Here's how the JSON does it, with the regular name:  
![SDCB structure](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-JSON names.png?raw=TRUE)

Getting my code to understand that "Hancock v. Train" means "HANCOCK, ATTORNEY GENERAL OF KENTUCKY v. TRAIN, ADMINISTRATOR, ENVIRONMENTAL PROTECTION AGENCY, et al." is surprisingly difficult.  


I've worked with fuzzy string matching before, so I knew I could use edit distance to see if the strings were similar:


```{r pressure, eval=FALSE}
agrep("Hancock v. Train",vote_data$caseName,max.distance=list(all=999,deletions=999,insertions=0, substitutions=0),ignore.case=TRUE)
```

This code basically says:  
Return the list of rows in the SCDB file (called vote_data) where the caseName field can have any number of characters removed such that it can be made to say "Hancock v. Train"
This should return 9 numbers.
It returns 18 because of the case "JOHN HANCOCK MUTUAL LIFE INSURANCE COMPANY v. HARRIS TRUST AND SAVINGS BANK, AS TRUSTEE OF THE SPERRY MASTER RETIREMENT TRUST NO. 2" or 
"john **Hancock** mutual life insurance company **v.** harris **Tr**ust **a**nd sav**in**gs ..."

Case names seemed like a bad approach. I was frustrated at this point, unable to get two files to communicate with each other.
But eventually I figured out that there is a piece of unique data that is in the exact same format in both files.

It's in the images above, can you see it?  
![The secret is the docket numbering!](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-docket.png?raw=TRUE)

The docket numbering is identical in the files! Problem solved.
It's a simple bit of RegEx to get the string between "case data/####/" and the next "/"  

```{r extract, eval=FALSE}
str_extract(JSON[["media_file"]][["href"]], "(?<=case_data\\/\\d\\d\\d\\d\\/).*(?=\\/)")[1]
```
The "href" object is a list (arguments used to be split over multiple days, and therefore files), so we only need to return the first result that matches.

Now we can associate the cases in the DB with the text in the transcript, down to the justice level!  
That's right, we're finally at:

## The Beginning of the Project
Rebecca Hamlin once described their research process (paraphrasing) as one involving numerous existential crises and project restarts. I share that process.

**Now**, I can begin to create my corpus!

At this point, while I'm still looking at the set of cases Stevens heard, I decided that collecting the information for every speaker would be beneficial.
The next week or so was dedicated to the following task:
Make a corpus-ready object that contains the transcript text associated with the speaking justice, their vote, and whether or not they wrote in the case* (I didn't capture this the first time, whoops).  

Before I reveal the beast of code I used to accomplish this, I'd like to disclaim it as much as humanly possible. I've been programming for almost 20 years, which is both a blessing and a curse. I'm still a bit baffled that calculating edit distance doesn't involve writing my own functions; it's not obvious to me that things should be simple.  
This is all to say that yes, I know that lapply exists. What I don't quite understand is how to turn my archaic nested for loops into that.
I also know that parallel programming exists, but see above.

Without further ado, here is the beast that creates my corpus-ready object:  

```{r}
setwd("C:/Users/Nick/Desktop/UMass/2021/Spring/Text as Data/PROJECT/Transcripts")
smalldf <-
  data.frame(
    text = character(),
    speaker = character(),
    case = character(),
    wrote = integer(),
    vote = integer()
  )
wrote <- 0 
time <- Sys.time()
parsed <- 0
for (x in list.files()) {
  # go through the folder
  j <- fromJSON(x) # get each file
  print(x) # print the file name
  print(difftime(Sys.time(), time)) # how much time have we spent?
  print(paste0("Documents processed: ", parsed)) # how many documents have we processed?
  for (i in 1:length(j[["transcript"]][["sections"]][["turns"]])) {
    # for each turn
    indices <-
      j[["transcript"]][["sections"]][["turns"]][[i]][["speaker"]][["last_name"]] # get the list of "justice speech indices"
    for (l in 1:length(indices)) {
      # for each of those indices
      if (is.null(j$transcript$sections$turns[[i]][["text_blocks"]][[l]]$text) == FALSE) {
        caseRowNum <- which(
          vote_data$docket == str_extract(j[["media_file"]][["href"]], "(?<=case_data\\/\\d\\d\\d\\d\\/).*(?=\\/)")[1]
        )[1] - 1
        justiceName <-
          j[["transcript"]][["sections"]][["turns"]][[i]][["speaker"]][["last_name"]][l]
        r <-
          vote_data[grep(justiceName, vote_data[which(
            vote_data$docket == str_extract(j[["media_file"]][["href"]], "(?<=case_data\\/\\d\\d\\d\\d\\/).*(?=\\/)")[1]
          ),]$justiceName) + caseRowNum,]$opinion
        vote <- vote_data[grep(justiceName, vote_data[which(
          vote_data$docket == str_extract(j[["media_file"]][["href"]], "(?<=case_data\\/\\d\\d\\d\\d\\/).*(?=\\/)")[1]
        ),]$justiceName) + caseRowNum,]$vote
        if (length(r) != 0) {
          wrote <- r
        } else{
          wrote <- 99
        }
        if (length(vote) == 0) {
          vote <- 99
        }
        
        
        speaker <-
          j[["transcript"]][["sections"]][["turns"]][[i]][["speaker"]]$name[[l]]
        text <-
          toString(j$transcript$sections$turns[[i]][["text_blocks"]][[l]]$text)
        smalldf[nrow(smalldf) + 1, ] <-
          list(
            text = text,
            speaker = speaker,
            case = j[["transcript"]][["title"]],
            wrote = wrote,
            vote = vote
          )
      }
      
    }
    
  }
  parsed <- parsed + 1
}
```
This monster walks through the directory of files, finds the associated SCDB rows, crawls through each "turn" and cobbles together the data. It likely repeats some or all of these processes more than necessary.  

It also takes a *very* **long** time, while using little memory or RAM.  
![It took a long time at first...](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-processing time.png?raw=TRUE)

Oh, whoops, forgot to include the "wrote" field.  
![It only added like 25 minutes!](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-processing time 2.png?raw=TRUE)

Ah, there, that's "better." 

It's gross and hacky, but it's mine. And it works:
![The "finished" product](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/dawson-corpus object.png?raw=TRUE)

Well, it *mostly* works. The remaining problems, I've found (after days of: debugging, running for 7 hours, failing, debugging...) are in the data itself. Some files do not have a case name (in fact, file 4085 did not, causing my program to brick inches from the finish line). Many of the files do not have the speakers completely tagged. We are left with a significant amount of "NA" speech, and associating that speech with a specific justice would take a while.

## Where am I?

Currently, I'm torn.  
On the one hand, I want to rewrite the program to be able to parse the entirety of the data I have. But I don't think I should limit my analysis to one justice, I should study of the data I downloaded from Oyez.  
I can do that with the beast, but I estimate it will take around 30 hours. Parallelizing the beast would involve a lot of programming, the benefit of which is unclear because: **I still don't have a theory about what makes a justice write an opinion or dissent**.  
(Other than the Theory of Broccoli, which holds that any justice who "substantively engages" [to excluded Breyer's mention of the term in NFIB v. Sebelius] with the term 'broccoli' at oral argument will write an opinion. If Chief Justice Roberts writes an opinion in California v. Texas, this theory will pass its second test.)  

