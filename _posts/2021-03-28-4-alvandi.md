---
layout: post
title: "Toxic Comments Classification (Alvandi)"
author: Amirhossein Alvandi
date: 2021-03-28 12:24:54 -0500
categories: alvandi
---


<style>
body {
text-align: justify}
</style>


Nowadays, it is almost impossible to engage in online conversations without witnessing toxic behavior like unwarranted harassment or disrespectful behavior. Conversational toxicity is a growing issue that can lead people to stop genuinely expressing themselves and give up on seeking others' opinions out of fear of abuse or harassment. Because of the massive volume of comments, it has become more critical to find a practical solution to identify and classify toxic comments in a more efficient manner. For instance, New York Times and Perspective API have developed a moderator tool that can find patterns, spot abusive language, and score all comments based on their toxicity level. Accordingly, they can provide live feedback to commenters and help human moderators sort comments much faster.

In this project, we will build a model capable of detecting toxic language inside comments and distinguishing between different types of such content, including obscenity, threat, or other toxicity. Basically, the created model will predict a probability for each comment belonging to each kind of toxicity category. To this end, we will apply various classification techniques, and by analyzing the number of false negatives and false positives, we get insights about open challenges that all of the approaches share.

## Corpus

We use the Wikipedia Comment Dataset containing 159,563 annotated user comments collected from Wikipedia talk pages and is the largest publicly available for the task. These comments were annotated by human raters with the six labels ‘toxic’, ‘severe toxic, ‘insult’, ‘threat’, ‘obscene’ and ‘identity-hate’. Comments can be associated with multiple classes at once, which frames the task as a multi-label classification problem.

## Importing data

Here, we import the csv file of data in R. The test data set which is consisted of only two columns (comment id, and comment text) will be used later when we compare the accuracy of various models in predicting the toxicity probabilities of the comments.

```{r,message = FALSE, warning = FALSE}
#install.packages("devtools")
#install.packages("tidytext")
#install.packages("plyr")
#install.packages("tidyverse")
#install.packages("quanteda")
#install.packages("randomForest")

# load libraries
library(devtools)
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(xtable)
library(caret)
library(gridExtra)
library("quanteda.textmodels")
library(caret)
library(e1071)
library(randomForest)
```



```{r,message = FALSE, warning = FALSE}
# Set working directory
setwd("/Users/amirhosseinalvandi/POLISCI797TA")
### import the training csv file
train_data = read_csv("Data/train.csv")
# getting the dimmension of the training data
dim(train_data)
```

As we can see, there are a total of 159571 comments in the training data set. For each of the comments there exist 6 different labels indicating the type of toxicity of the given comment. Let us have a look at the beginning rows of our data set.

```{r,message = FALSE, warning = FALSE}
# viewing the first 5 rows of the data
head(train_data,n = 5)
```

We can observe in the above cell that the first two columns contain the comment id and comment text, and each of the next six columns contain a binary random variable where we have "1" indicating that we have some type of a toxicity in the corresponding row, and zero otherwise. Now, we may plot a bar chart of the data in order to better understand the distribution of the various kinds of toxicity.

```{r,message = FALSE, warning = FALSE}
### Finding the counts for each type of toxicity
toxic_labels = colnames(train_data[3:8])
toxic_counts = as.numeric(colSums(train_data[,3:8]))
bar_data = tibble(cbind(toxic_labels = toxic_labels, toxic_counts = toxic_counts))
### Plotting the histogram of various toxicity types
ggplot(data = bar_data, aes(x = toxic_labels, y = toxic_counts, fill = toxic_labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Toxicity Type")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi-4-fig1.png?raw=TRUE)

From the bar plot, it is apparent that we have the largest proportion for the toxic comments, however that only covers about 10 percent of the total number of comments in this data set, and the smallest count is for comments including a threat. 


## Data Pre-processing

Data pre-processing is one of the important steps in any text data analysis task. Here, using the 'tokens' function, we removed all punctuation marks, numbers, urls, and English stop words. All characters then were converted into lower case to avoid repetitions in the set of unique words. An example of both raw and pre-processed versions of the first commment is provided below.  

It is also useful to find out how many multi-label comments are present in our data. Since, when hand labeling the comments, one's content can be considered  obscene and insulting at the same time.


```{r}
# Find the number of comments with more than one toxicity type
multi_label = 0
for(i in 1:nrow(train_data)){
  if(sum(train_data[i,3:8]) > 1){
    multi_label = multi_label + 1   
  }
}

multi_label
```

As we can observe, 9865 from the total of 15294 toxic comments have multiple toxicity labels.

## Subsampling for Class Imbalances

It is apparent that we have a high level of class imbalance in our data set, where only about 10% of the total comments are toxic. Here, the detection of toxic comments is our primary interest; Therefore, it is very likely that a classification model would be able to achieve very good specificity since about 90% of the comments are non-toxic. Sensitivity, however, would likely be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.

One way to alleviate this issue is to sub-sample the data. There are a number of ways to do this but the most simple one is to sample down the majority class data until it occurs with the same frequency as the minority class. While it may seem counter-intuitive, throwing out a large percentage of your data can be effective at producing a useful model that can recognize both the majority and minority classes. In some cases, this even means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are better calibrated, meaning that the distributions of the class probabilities are more well behaved. As a result, the default 50% cutoff is much more likely to produce better sensitivity and specificity values than they would otherwise.


```{r}
# Find the proportion of toxic comments
table(train_data$toxic)
# Downsmaple the non-toxic comments (majority class)
non_toxic_index = train_data$id[train_data$toxic == 0]
toxic_index = train_data$id[train_data$toxic == 1]
set.seed(123)
downsample_non_toxic_index = sample(train_data$id[train_data$toxic == 0], size = 15294)
train_data_balanced_id = c(downsample_non_toxic_index, toxic_index)
train_data_balanced = filter(train_data,train_data$id %in% train_data_balanced_id)
# Distribution of comments before downsampling
labels = c("toxic", "non-toxic")
counts = c(sum(train_data$toxic == 1), sum(train_data$toxic == 0))
bar_data_imb = tibble(cbind(labels = labels, counts = counts))
p1 = ggplot(data = bar_data_imb, aes(x = labels, y = counts, fill = labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Before Down-sampling")

# Distribution of comments after downsampling
counts_balanced = c(sum(train_data_balanced$toxic == 1), sum(train_data_balanced$toxic == 0))
bar_data_bal = tibble(cbind(labels = labels, counts_balanced = counts_balanced))
p2 = ggplot(data = bar_data_bal, aes(x = labels, y = counts_balanced, fill = labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "After Down-sampling")

grid.arrange(p1, p2, ncol = 2)
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi-4-fig2.png?raw=TRUE)


After down-sampling from the data, we have a total of 30588 comments. 
Now, in order to train and validate different classifiers, we split the training data into a train set (60% of all comments), a test set (20% of all comments), and a held out set (20% of all comments). Later, when we use the grid search for parameter tuning of the models, we will use a 5-fold cross validation to have more robust evaluation metrics.


```{r}
# create training set (60% of data) and initial test set
set.seed(123)
N = nrow(train_data_balanced)
trainIndex = sample(1:N,.6 * N) 
testIndex = c(1:N)[-trainIndex]

# split test set in half (so 20% of data are test, 20% of data are held-out)
N_test = length(testIndex)
heldOutIndex = sample(1:N_test, .5 * N_test)
testIndex = testIndex[-heldOutIndex]

# pre-processing step
# create the dfm
subset_train =  train_data_balanced[trainIndex,]
train_dfm = dfm(subset_train$comment_text,
        tolower = TRUE,
        remove_punct = TRUE,
        stem = FALSE,
        remove = stopwords("english")
        )

# find out a quick summary of the dfm
train_dfm
```


```{r}
# pre-processing step
# create the dfm
subset_test =  train_data_balanced[testIndex,]
test_dfm = dfm(subset_test$comment_text,
        tolower = TRUE,
        remove_punct = TRUE,
        stem = FALSE,
        remove = stopwords("english")
        )

# find out a quick summary of the dfm
test_dfm
```


```{r}
subset_heldOut =  train_data_balanced[heldOutIndex,]
heldOut_dfm = dfm(subset_heldOut$comment_text,
        tolower = TRUE,
        remove_punct = TRUE,
        stem = FALSE,
        remove = stopwords("english")
        )

# find out a quick summary of the dfm
heldOut_dfm
```

In order to save some computational time, we only retain the features (words) that are used more frequently. To do so, we filtered out the words that appeared in less than 2% of all comments.


```{r}
# Reduce the number of features for train set
smaller_train_dfm <- dfm_trim(train_dfm, min_docfreq = 0.02, docfreq_type = "prop")
smaller_train_dfm
```

As we can observe, by eliminating the very rare terms from our training dfm, we managed to reduce the number of features from 54,263 to only 168 more commonly used words.

Now we want to know how well the trained classifier performed. To do so, we need to keep only the words in our testing data that also appear in the training data. which only retains terms that appear in both corpora.

```{r}
# Match the features that are present in both corpora
dfmTestMatched_small = dfm_match(test_dfm, features = featnames(smaller_train_dfm))
dfmTestMatched_small
```


## Performance Evaluation Metrics

There are a wide range of metrics (loss functions) available in order to evaluate the performance of different classification models. Here, we used four different evaluation metrics as described below.


#### Accuracy 

Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy of a binary classifier is given by

$$ Accuracy = \frac{True \hspace{0.1cm} Positive + True \hspace{0.1cm} Negarive}{True \hspace{0.1cm} Positive + True \hspace{0.1cm} Negarive + False \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative} $$
In general, using accuracy for severely unbalanced classification problems could be misleading since even if a model naively classifies all data points in the major class, we would still get an accepatable accuracy. But, as we modified our data in order to have the same number of comments from both toxic and non-toxic class, we can rely on the accuracy of different classification models. 


#### Precision

Classification precision is one of most commonly used metrics which is defined as the fraction of relevant instances among the retrieved instances. In other words, the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.

$$ Precision = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + False \hspace{0.1cm} Positive} $$
For instance, in our case, the "True Positive" is the number of toxic comments that are classified as toxic by the classification model.


#### Recall

Recall (also called sensitivity) is the fraction of relevant instances that were retrieved or equivalently, the number of true positive results divided by the number of all samples that should have been identified as positive. 

$$ Recall = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative} $$

#### F1 Measure

The F1 measure is the harmonic mean of the precision and recall. The highest possible value of an F measure is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. 

$$ F = \frac{2}{Recall^{-1} + Precision^{-1}} = \frac{True \hspace{0.1cm} Positive}{True \hspace{0.1cm} Positive + \frac{1}{2}(False \hspace{0.1cm} Positive + False \hspace{0.1cm} Negative)} $$
The more generic $F_{\beta}$ measure applies additional weights, valuing one of precision or recall more than the other.


## Classification Methods

In order to predict the toxicity class for each comment using its words, we begin by classifying each comment as toxic or non-toxic (binary classification), and then, we use a multi-label classification to determine the type of toxicity for each of the toxic comments. Here, we provide a brief description of the different classification models that we utilize for our data analysis. 

#### Naive Bayes 

The Naive Bayes classifier is one of the most renowned classification models in text data analysis. Applying the Bayes theorem to calculate the probability that a new comment is toxic given that the comment content is given by,

$$Pr(Toxic|Comment's \hspace{0.1cm} content) = \frac{Pr(Comment's \hspace{0.1cm} content|Toxic)Pr(Toxic)}{Pr(Comment's \hspace{0.1cm} content)}$$
The probability of a comment being toxic ($Pr(Toxic)$) can be calculated as the proportion of toxic comments in our training data set. Here, the naive independence assumption let us to  compute the likelihood of a comment being toxic only by finding the product of the word appearing or not appearing in the toxic words pool. Then, the class of a new comment is given by

$$ \frac{Pr(Toxic|Comment's \hspace{0.1cm} content)}{Pr(Non-Toxic|Comment's \hspace{0.1cm} content)}$$
Now, in order to simplify the computations by taking log, we have

$$ \log (\frac{Pr(Toxic|Comment's \hspace{0.1cm} content) +0.5}{Pr(Non-Toxic|Comment's \hspace{0.1cm} content) + 0.5})$$
where the 0.5 corrections are  used to avoid division by zero. 

Here, we fit a binary Naive Bayes model for all different toxicity types, and compare the performance of the models using the listed evaluation metrics as well as their computational weight.  


```{r}
# fit the Naive Bayes model to the train data
# Start the timer
start_time = proc.time()
NaiveBayes = textmodel_nb(x = smaller_train_dfm,
                           y = train_data_balanced$toxic[trainIndex],
                           distribution = "Bernoulli") 
# End the timer
end_time = proc.time() - start_time
end_time
# create a confusion matrix 
actual_toxic = train_data_balanced$toxic[testIndex]
predicted_nb = predict(NaiveBayes, newdata = dfmTestMatched_small)
confusion_nb = table(actual_toxic, predicted_nb)

# now calculate a number of statistics related to the confusion matrix
NaiveBayes_summary = confusionMatrix(confusion_nb, mode = "everything")
NaiveBayes_summary
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi-4-table1.png?raw=TRUE)


#### Support Vector Machine (SVM)

Support Vector Machine is a classification method that is based on the principle of finding hyper-planes that distinctly classify the data units when you plot them onto an $n$-dimensional graph where $n$ represents the number of features of the data. One of the main advantages of SVM classifiers is their effectiveness in high dimensional spaces which makes them a reliable candidate for our situation.


```{r}
# Fit the SVM model to the smaller train set
# Start the timer
start_time = proc.time()
SVM = textmodel_svm(x = smaller_train_dfm,
                           y = train_data_balanced$toxic[trainIndex],
                           distribution = "Bernoulli") 
# End the timer
end_time = proc.time() - start_time
end_time
# create a confusion matrix 
predicted_svm = predict(SVM, newdata = dfmTestMatched_small)
confusion_svm = table(actual_toxic, predicted_svm)
# now calculate a number of statistics related to the confusion matrix
SVM_summary = confusionMatrix(confusion_svm, mode = "everything")
SVM_summary
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi-4-table2.png?raw=TRUE)



#### Random Forest

Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction. Here, we use mtry = 13 which refers to the number of variables available for splitting at each tree node. The default value for mtry for classification problems is set to the squared root of the number of features. Another important characteristic of random forest classifiers is the number of trees which is usualy determined based on the size of the data. Later, when we want to obtain the optimal model for each toxicity class, we will perform parameter tuning for all different classifiers.



```{r}
# Convert dfms to matrices 
dfmTrainSmallRf <- convert(smaller_train_dfm, to = "matrix")
dfmTestMatched_smallRf <- convert(dfmTestMatched_small, to = "matrix")

# Fit random forest model to the training set
# Start the timer
start_time = proc.time()
RF = randomForest(dfmTrainSmallRf, 
                  y = as.factor(train_data_balanced$toxic[trainIndex]),
                  xtest = dfmTestMatched_smallRf, 
                  ytest = as.factor(train_data_balanced$toxic[testIndex]),
                  importance = TRUE,
                  mtry = 13,
                  ntree = 100)
# End the timer
end_time = proc.time() - start_time
end_time
```



```{r}
# Predict labels for the test set
predicted_rf = RF$test[['predicted']]
confusion_rf = table(actual_toxic,predicted_rf)
rf_summary = confusionMatrix(confusion_rf, mode = "everything")
rf_summary
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi-4-table3.png?raw=TRUE)


As we can see from the tables, the random forest classifier has an overall advantage. However, we have very high accuracy for toxicity types that are quite rare (e.g. threat and identity hate), the model was not very successful in detecting those types of toxicity. 
It is also apparent that the random forest and SVM algorithms are more complex in terms of computation time.   


#### Gradient Boosted Decision Trees

Gradient boosted decision trees has been proven to be a powerful tool for classification problems. Using this algorithm, one goes through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by taking an existing model and calculating the errors for each observation in the dataset. We then build a new model to predict these errors. We add predictions from this error-predicting model to the ensemble of models. Here, we use the xgboost classifier in R which has a built-in datatype, DMatrix, that is particularly good at storing and accessing sparse matrices efficiently.



#### Long Short Term Memory (LSTM)

Recurrent Neural Networks (RNN) refer to a  class  of neural networks that contain cyclic connections. The output of a given hidden layer is fed back to itself in an RNN to remember some information as the memory from previous computations. This makes RNNs a powerful tool for text classification. Long Short Term Memory (LSTM) is an RNN that can learn long term dependencies which is a challenging task for traditional RNNs. An LSTM model, like RNN, has a chain-like architecture where each unit of this repeating structure is called an LSTM cell.

An LSTM cell contains an input gate, an output gate and a forget gate that regulates the data which is flown into and outside of the cell. The forget gate decides what information it's going to discard from the current cell. The input gate then decides what new information is going to be added to modify the current state of the memory and finally, the output gate decides what information leaves the cell.


#### Future Direction
###### 1. Develop a few more classification models and use gridsearch for parameter tuning

###### 2. Check the infuence of features that are too frequent in both groups




















