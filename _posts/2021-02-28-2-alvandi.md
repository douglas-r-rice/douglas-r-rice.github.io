---
layout: post
title: "Toxic Comments Classification (Alvandi)"
author: Amirhossein Alvandi
date: 2021-02-28 12:24:54 -0500
categories: alvandi
---

<style>
body {
text-align: justify}
</style>

Nowadays, It is almost impossible to engage in online conversations without witnessing toxic behavior like unwarranted harassment or disrespectful behavior. Conversational toxicity is a growing issue that can lead people to stop genuinely expressing themselves and give up on seeking others' opinions out of fear of abuse or harassment. Because of the massive volume of comments, it has become more critical to find a practical solution to identify and classify toxic comments in a more efficient manner. For instance, New York Times and Perspective API have developed a moderator tool that can find patterns, spot abusive language, and score all comments based on their toxicity level. Accordingly, they can provide live feedback to commenters and help human moderators sort comments much faster.

In this project, we will build a model capable of detecting toxic language inside comments and distinguishing between different types of such content, including obscenity, threat, or other toxicity. Basically, the created model will predict a probability for each comment belonging to each kind of toxicity category. To this end, we will apply various classification techniques, and by analyzing the number of false negatives and false positives, we get insights about open challenges that all of the approaches share.

# Corpus

We use the Wikipedia Comment Dataset containing 159,563 annotated user comments collected from Wikipedia talk pages and is the largest publicly available for the task. These comments were annotated by human raters with the six labels ‘toxic’, ‘severe toxic, ‘insult’, ‘threat’, ‘obscene’ and ‘identity-hate’. Comments can be associated with multiple classes at once, which frames the task as a multi-label classification problem.

# Importing data

Here, we import the csv file of data in R. The test data set which is consisted of only two columns (comment id, and comment text) will be used later when we compare the accuracy of various models in predicting the toxicity probabilities of the comments.

```{r}
library(tidyverse)
library(ggplot2)
### import the training csv file
train_data = read_csv("toxic comment classification/train.csv")
test_data = read_csv("toxic comment classification/test.csv")
# getting the dimmension of the training data
dim(train_data)
```

As we can see, there are a total of 159571 comments in the training data set. For each of the comments there exist 6 different labels indicating the type of toxicity of the given comment. Let us have a look at the beginning rows of our data set.

```{r}
# viewing the first 5 rows of the data
head(train_data)
```
We can observe in the above cell that the first two columns contain the comment id and comment text, and each of the next six columns contain a binary random variable where we have "1" indicating that we have some type of a toxicity in the corresponding row, and zero otherwise. Now, we may plot a bar chart of the data in order to better understand the distribution of the various kinds of toxicity.

```{r}
### Finding the counts for each type of toxicity
toxic_labels = colnames(train_data[3:8])
toxic_counts = as.numeric(colSums(train_data[,3:8]))
bar_data = tibble(cbind(toxic_labels = toxic_labels, toxic_counts = toxic_counts))
### Plotting the histogram of various toxicity types
ggplot(data = bar_data, aes(x = toxic_labels, y = toxic_counts, fill = toxic_labels)) + 
  geom_bar(stat = "identity") + 
  scale_fill_hue(c = 40) +
  theme(legend.position="none") +
  labs(y = "Count", x = "Toxicity Type")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/alvandi_figure_1.png?raw=TRUE)


From the bar plot, it is apparent that we have the largest proportion for the toxic comments, however that only covers about 10 percent of the total number of comments in this data set, and the smallest count is for comments including a threat. 
