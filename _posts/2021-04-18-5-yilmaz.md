---
layout: post
title: "Correlates of Comment Scores on a Subreddit (Yilmaz)"
author: Gamze Yilmaz
date: 2021-04-18 12:24:54 -0500
categories: yilmaz
---


Dictionary methods did not generate very meaningful results to address my research questions, so after meeting with Doug, I took a different approach and decided to use Structural Topic Modeling to analyze my data. More specifically, I looked at which topics were liked the most in the community by using the “score” metadata variable as a predictor. I followed the below code from the class tutorial and applied it to my own dataset. 


```
# choose our number of topics
k <- 5

#Doug reformatted the dataset
model_data <- my_data[-(which(rowSums(my_dfm) == 0)),]
model_dfm <- my_dfm[-(which(rowSums(my_dfm)==0)),]

myModel <- stm(model_dfm,
               K = k,
               prevalence =~ score,
               data = model_data,
               max.em.its = 200, 
               seed = 1234,
               init.type = "Spectral")

labelTopics(myModel)

plot(myModel, type = "summary")


# get the words
myTopicNames <- labelTopics(myModel, n=10)$frex

# set up an empty vector
myTopicLabels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
  myTopicLabels[i] <- paste(myTopicNames[i,], collapse = "_")
}

# print the names
myTopicLabels

```

Below are the initial results I got after running the model with 5 and then 10 topics:

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/top 5 topics.pdf?raw=TRUE)

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/top10 topics.pdf?raw=TRUE)

I first used the arbitrary number of 5 topics. The rationale was that after eyeballing the comments in my dataset, I decided that there were not a wide variety of topics discussed, but rather the conversations revolved around a small number of issues (e.g., holding onto the shares, critiquing Robinhood, status of the stocks, etc.). I also ran the model with 10 topics and some of the topics in this model made less qualitative sense. After looking at the top words associated with each topic in both 5-topic and 10-topic models, I decided to use the 5-topic model. Next, by closely examining a subset of my dataset, I can do qualitative validation to make further sense of these topics to answer my research question about the “most liked” comments. 

I also tried to plot the estimated effects of “score” on topic prevalence for each of the estimated topics using the below code, but this section of the code from the tutorial was a bit confusing for me to interpret, so I need to work on this section before I finalize the data analysis. 

```
# estimate effects
modelEffects <- estimateEffect(formula=1:k~score, 
                               stmobj = myModel, 
                               metadata = model_data)


# plot effects
myRows <- 2
par(mfrow=c(myRows,3), bty="n", lwd=2)
for (i in 1:k){
  plot.estimateEffect(modelEffects, 
                      covariate ="score",  
                      xlim=c(-.25,.25), 
                      model = myModel, 
                      topics = modelEffects$topics[i], 
                      method = "difference", 
                      cov.value1 = 1, 
                      cov.value2=0, 
                      main = myTopicLabels[i], 
                      printlegend=F, 
                      linecol="grey26", 
                      labeltype="custom", 
                      verbose.labels=F, 
                      custom.labels=c(""))
  par(new=F)
})
```

I also wanted to share the results from the sentiment analysis to show how poorly all the dictionaries performed in the context of my data. I was hoping that LM dictionary would perform better than Afinn and General Inquiry, yet similar to their results, the classification of negative vs. positive sentiments in LM dictionary was not very informative. Two takeaways from these results: 1) during the first week after the peak of short squeeze, there were many more comments and engagement from the community compared to the second week 2) Afinn, General Inquiry and LM dictionaries are not great tools to examine sentiment in reddit. I think one reason for this is the unique linguistic style and communication norms redditters apply in the discussion forums. 


![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/polarity plot.pdf?raw=TRUE)
