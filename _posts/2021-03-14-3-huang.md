---
layout: post
title: "Career choices for Chinese new graduates (Huang)"
author: Tieli Huang
date: 2021-03-14 12:24:54 -0500
categories: huang
---

What are the most common career choices for Chinese new graduates? Or more specifically, Why are jobs in the public sector (civil servant) so popular among young people? What contributed to its popularity?


# Background

A job is the most direct indicator for one's career plan and ambition. Many young people in China have been preparing for the National Civil Service Examination in their last year in University, either out of their parents' wishes, or because they don't have a better choice or stronger desire for other professions.

The fact is, the examination itself is extremely hard and often have thousands candidate competing for one position. Then, does young people's strong desire to work in public sectors reflect their uncertainty for the future? Should there be some pattern for the the popularity of public jobs & state of the economy?

This question is important because it shows people's confidence for the future of society. When young people start craving for civil servant jobs, they might not have as much faith in a free market as before.

# Corpus

To find out about why young people nowadays wish they had a job in the public sector, it is important to not only look at the employment report from major universities, the ministry of education and the ministry of human resources, but also social media platforms.

The most widely used networking platform in China, Douban, has all kinds of interest groups where people share with each other on one theme. And there are several of them aim specifically at the job market and the working environment. My plan is to select some of the most popular stories among these group members and to find out about their preferences for jobs in the public sector.

# Get the data into R

The corpus here is from the online forum "Douban". First, I searched across this forum for discussions using keyword "public servant", and sorted all results based on relevance. 
Of all posts, I chose one in particular with the title of "A lot of people seem to be forced into taking the Civil Service Examination", which has 635 words, posted on March 4th, 2020. This post got five pages of replies, 456 in total, which I think is also valuable for analysis. My corpus would be both the post and the replies.


```{r}
# install and load packages
install.packages("rvest") 
install.packages("tidyverse")
library(rvest)
library(tidyverse)
```



## Get titles of posts
Here is how I used the search within the forum Douban to get all the related posts for the discussion of public service jobs.

```{r}
# identify the url
Discussions <- "https://www.douban.com/group/search?cat=1013&q=gwy&sort=relevance"

# define the field
css_selector <- ".td-subject"

topics <- Discussions %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()

topics
```


# Choose from above topics the texts we want

In this step, I picked one post in particular, including the initial post and five pages of replies using loop.

```{r}
# Select web page
# identify the url
url <- "https://www.douban.com/group/topic/166598514/"
read_html(url)
```

```{r}
# use SelectorGadget to find the info
css_selector <- ".topic-doc"

topic <- url %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()

topic
```

```{r}
# select all the replies with this post
css_selector <- ".reply-content"

# use pipe operator to get the first page of replies
reply <- url %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()

reply
```

# for remaining comments from the other four pages
By comparing the other four pages of the replies to the first one, I fould that there are approximately 100 replies on one page. Therefore, for the next page, we just add 100 to the url address and then here we are with all our replies.
```{r}
new_url <- "https://www.douban.com/group/topic/166598514/?start=100"
# print these next to each other

url
new_url
```
```{r}
pagenumber <- 100 *c(1:4)
head(pagenumber)
length(pagenumber)

# set up a new vector to store urls
urls <- url

# loop through page numbers and create new urls
for (i in 1: length(pagenumber)){
  urls <- c(urls, paste("https://www.douban.com/group/topic/166598514/?start=",pagenumber[i], sep = ""))
}

# look at the first few
head(urls)
```

```{r}
# set up an empty vector to store comments
comments <- c()

css_selector <- ".reply-content"

# loop through urls
for (i in 1: length(urls)){
  # extract comments for this url
  newreply <- urls[i] %>% 
    read_html() %>% 
    html_nodes(css = css_selector) %>% 
    html_text()
  
  # add them to the set of comments
  replies <- c(reply, newreply)
}

# look at the remaining comments
replies[1:150]
```

# A quick analysis

In this part, I tried to do a quick analysis of our corpus which is in Chinese by demonstrating it in a wordcloud.
To do this, I first installed all the packages needed, of them "tmcn" and "Rwordseg" are critical. By using wordcloud2, we are able to make beautiful little pictures showing the frequencies of characters in our corpus.

```{r}
#install.packages("tmcn")
#install.packages("Rwordseg")
#install.packages("wordcloud2")

#install.packages("tm")
#install.packages("wordcloud")
#install.packages("tmap")

library(quanteda)
library(Rwordseg)
library(tmcn)
library(wordcloud2)
```



```{r}
# here I use a dictionary for Chinese characters
# the dictionary is from NTU
data(NTUSD)
positive_simple <- NTUSD[[1]]
negtive_simple <- NTUSD[[2]]
positive_tradition <- NTUSD[[3]]
negtive_tradition <- NTUSD[[4]]

insertWords(positive_simple)
insertWords(negtive_simple)
insertWords(positive_tradition)
insertWords(negtive_tradition)

# some buzzwords in our corpus
dir <- c('铁饭碗','事业编', '国企', '集美', '社畜', 
         '网课', '苦熬')

insertWords(dir)

# word segmentation
word_seg <- segmentCN(replies, returnType = 'tm')
word_seg
```


```{r}
wf <- createWordFreq(unlist(strsplit(word_seg, ' ')), stopwords= stopword)
head(wf,20)
                  
```
```{r}
library(wordcloud2)
wordcloud2(wf, color = "random-light", backgroundColor = "grey")
```

# Some prepossessing of our corpus

```{r}
# Convert these to corpus objects:
discussion_corpus <- corpus(replies)

# summary of corpus object
discussion_summary <- summary(discussion_corpus)
discussion_summary
```


```{r}
# tokenization
discussion_tokens <- tokens(discussion_corpus, remove_punct = T)
print(discussion_tokens)
```

```{r}
# get the standard stopwords list for Chinese characters
stopwordsCN()
```

```{r}
# remove stopwords
discussion_tokens <- tokens_select(discussion_tokens, 
                                   pattern = stopwordsCN(),
                                   selection = "remove")
print(discussion_tokens)
```

```{r}
# stemming
discussion_tokens <- tokens_wordstem(discussion_tokens)
print(discussion_tokens)
```


# Representing text
```{r}
# create the dfm
replies_dfm <- dfm(discussion_corpus,
                   tolower = TRUE,
                   remove_punct = TRUE,
                   stem = TRUE,
                   remove = stopwordsCN()
                   )
# a quick summary of the dfm
replies_dfm
```


```{r}
# working with dfms
topfeatures(replies_dfm, 20)
```

```{r}
# use showtext to add Chinese font
#install.packages("showtext")
library(showtext)
font_files()
# Add a Chinese font into font family
font_add(family = "hanzi", regular = "/System/Library/Fonts/Supplemental/Times New Roman.ttf")
showtext.auto()
font_families()
textplot_wordcloud(replies_dfm, min_count = 50, random_color = FALSE)
```
![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/huang-blog3.png?raw=TRUE)

