---
title: 'Blog post #3'
author: "Steve Linberg"
date: "14 March 2021"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
subtitle: 797TA, Text as Data, Spring 2021, Umass Amherst
urlcolor: blue
---

This round, we get some data into what could be a candidate for final form, and take a few specific digs into it.

One of the chief challenges with this dataset, as previously discussed, is its scale: approximately 183,000,000 text posts from Parler over a span of several years, leading up to the few days after the January 6 insurrection at the US Capitol. The archive consists of 160 1-gigabyte zipfiles, each with approximately 1.1 million posts. On my laptop, it takes R approximately 8-10 minutes to load one of these `ndjson` files. So far, I have only worked with two of these files, since a process that processes all of them is likely to take over 24 hours, and will need to be planned. There is enough data to work out concepts on smaler scales before the final work is done.

One important thing I still do not know is where exactly the 183m posts came from. I am making an assumption that they were extracted from the 70+TB archive pulled by @donk_enby at MIT before the site went down in early January. I do not know whether the archive is all of the text posts from this archive (which would mean that that vast majority of the data pulled was non-text, likely primarily video, which seems plausible), or whether it's a subset / sample of the larger set, and if the latter, what the selection criteria were. (Communication with the original paper's authors has been erratic, and I'm not sure whether I'll be able to get the full story from them; I will keep trying as we go.)

For the moment I am proceeding on the assumption that the sample in my dataset is representative of the posts on the site, whether or not this is a complete archive, and that the 160 files each have a selection of posts spread across the date range. The first file in the archive, `parler_data000000000000`, has a date range from September 21, 2010 to January 10, 2021. (The early date range is probably left over from an early implementation of the site; Parler did not go "live" on a broad scale until 2018. However, it shows that the selection in the first file is not based on date.)

I decided to restrict the examined posts to the days immediately around, on, and after the insurrection; this happened to align neatly with the year 2021, since we would have 5 days before the insurrection and about 4 days after; this is adequate for the first round of inquiry. As always, we want to strip the data down to its smallest possible expression for the purposes of the queries, as whatever wo do here will be multiplied by 160 when we use the full set. Mutating the text-based date field through a `POSIXct` structure so that time zone would be considered (the file's timestamps are in UTC, but we want to use Eastern time, the timezone of Washington DC, in case we end up digging in at a level where times might be significant). We keep only posts created in 2021, and with a body length greater than 1 character (omitting straight-media posts like memes, videos and so forth), and retain just the post `id`, the poster `username`, the day in January 2021 that it was created (since all posts are from January 2021, we do not need to store the full date), and of course the post body: 

```{r}
january_2021_posts <- df %>%
  # Using as.POSIXlt created a lot of pain, because POSIXlt objects can't be part of data frames.
  # POSIXct works, but needs the lubridate library and a different syntax to extract parts.
  mutate(createdAtPosix = as.POSIXct(createdAtformatted, tz="America/New_York")) %>%
  mutate(created_day = as.integer(day(createdAtPosix)), 
         created_month = as.integer(month(createdAtPosix)), 
         created_year = as.integer(year(createdAtPosix))
         ) %>%
  filter(created_year == 2021,
         nchar(body) > 1) %>%
  select(id, username, created_day, body)
```

This reduces the file size from 926mb to 2.6mb - obviously a tremendous reduction, and the sort of compression we'll need if and when we build out the entire dataset this way. It ended up with around 33,000 posts, from just under 26,000 unique users. (I find this a little surprising, and will investigate further as we look at more data.)

With this done, I decided to create 3 separate corpuses, one each for the days before January 6, January 6 itself, and the days after January 6. First, 3 new data frames:

```{r}
posts_before_jan6 <- january_2021_posts %>%
  filter(created_day < 6)
posts_on_jan6 <- january_2021_posts %>%
  filter(created_day == 6)
posts_after_jan6 <- january_2021_posts %>%
  filter(created_day > 6)
```

(With a round of applause for R's copy-on-modify mechanics that makes operations like this extremely inexpensive)

Then, for each, a corpus and a `dfm`. The same arguments were used for each, for obvious reasons, and appear to work reasonably at first glance. Stopwords and punctuation were removed, case went to lower to handle ALL CAPS SHOUTING, and stemming was not turned on (yet).

```{r}
posts_before_jan6_corpus <- corpus(posts_before_jan6, text_field="body")
posts_before_jan6_dfm <- dfm(posts_before_jan6_corpus,
                              tolower = TRUE,
                              remove_punct = TRUE,
                              stem = FALSE,
                              remove = stopwords("english")
)
```
```{r}
posts_on_jan6_corpus <- corpus(posts_on_jan6, text_field="body")
posts_on_jan6_dfm <- dfm(posts_on_jan6_corpus,
                              tolower = TRUE,
                              remove_punct = TRUE,
                              stem = FALSE,
                              remove = stopwords("english")
)
```
```{r}
posts_after_jan6_corpus <- corpus(posts_after_jan6, text_field="body")
posts_after_jan6_dfm <- dfm(posts_after_jan6_corpus,
                              tolower = TRUE,
                              remove_punct = TRUE,
                              stem = FALSE,
                              remove = stopwords("english")
)
```

With this, I created wordclouds, using the following code. The `min_count` variables we adjusted for each to produce something that was visually similar, since there were 12,724 posts before January 6, 3,701 posts on January 6, and 17,018 posts after:

### Before January 6

```{r}
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(posts_before_jan6_dfm, min_count = 200, random_order = FALSE)
topfeatures(posts_before_jan6_dfm, n=20)
```

![](linberg-post-3-wordcloud-1.png)

### On January 6

```{r}
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(posts_on_jan6_dfm, min_count = 75, random_order = FALSE)
```

![](linberg-post-3-wordcloud-2.png)

### After January 6

```{r}
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(posts_after_jan6_dfm, min_count = 300, random_order = FALSE)
```

![](linberg-post-3-wordcloud-3.png)

We're not going to put too much emphasis on word clouds, but they can be interesting to look at. Unsurprisingly, the most common word in all three corpuses is "trump". Also significant is that the fourth-most common word in the pre-January 6 (and also very high in the other two) is the emoji sequence "\\U0001f1fa\\U0001f1f8", which is an American flag icon. This is shown in the word clouds as four empty "missing character" boxes; there are also a couple of other pairs of missing-character boxes, representing an angry-face emoji and a laughing-face emoji. Unfortunately, the wordcloud code does not appear to be capable of rendering these emoji.

A couple of other minor observations:

- The word "fraud" is in the clouds for before and on January 6, but not after
- The word "please" is only in the cloud for after Janaury 6

Still, it's hard to glean a great deal from these images. We need more rigorous tools for sentiment analysis to dig deeper into the question of whether, and how, overall sentiment on Parler shifted before, during, and after the insurrection.
