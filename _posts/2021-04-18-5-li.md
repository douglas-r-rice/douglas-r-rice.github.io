---
layout: post
title: "Understanding Conspiracies (Li)"
author: Zichao Li
date: 2021-04-18 12:24:54 -0500
categories: li
---


First, run the svm model to classfy the data.

```
#run the svm model on the whole data corpus
alltext_df <- as.data.frame(alltext)
alltext_dfm <- dfm(alltext_corpus)
all_predicted <- predict(consp3_SVM, newdata = alltext_dfm)
all_predicted_df <- as.data.frame(all_predicted)
```

Clean data and add the time information to the predicted data set.

```
#recombine the document
post <- filter(ultidata,comment =="NA",)
as.data.frame(post)
cmnt <- ultidata[ultidata$comment !="NA",]
df <- data.frame(rbind(post,cmnt))
#combine the date value and text content together
all_predicted_df$date <- df$comm_date
alltext_df
all_predicted_df$text <- alltext_df$alltext
as.data.frame(all_predicted_df)

#pull out the conspiracy talking results by filtering consp value
library(dplyr)
conspTalk <- filter(all_predicted_df, all_predicted == "1") 
```

#LDA

```
#LDA model
library(text2vec)

#transform df into corpus (without this step, tolower()cannot 
#process texts with non graphic characters)
consp_corpus <- corpus(conspTalk,
                       docid_field = "doc_id",
                       text_field = "text" )

#tokenize the corpus
# Creates string of combined lowercased words
tokens <- tolower(consp_corpus)
# Performs tokenization
tokens <- word_tokenizer(tokens)
# Prints first two tokenized rows 
head(tokens, 3)

# Iterates over each token
it <- itoken(tokens, ids = consp_corpus$doc_id, progressbar = FALSE)
# Prints iterator
it

# Built the vocabulary
v <- create_vocabulary(it)
# Print vocabulary
v
tail(v, 60)
# Checking dimensions
dim(v)

# Prunes vocabualry
v <- prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.1)
# Check dimensions
dim(v)

# Creates a closure that helps transform list of tokens into vector space
vectorizer <- vocab_vectorizer(v)

# Creates document term matrix
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix")

# Creates new LDA model
set.seed(0122)
lda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
# Print other methods for LDA
lda_model

#fitting the model
doc_topic_distr <- 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)

# Get top n words for topics 1, 5, and 10
lda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L), lambda = 1)

# Creating plot
lda_model$plot(open.browser = TRUE)
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-LDA.pdf?raw=TRUE)


Analyze how topics change over time using stm.

```
library(stm)
myDfm <- dfm(conspTalk$text,
             tolower=TRUE,
             remove = stopwords('en'), 
             remove_punct = TRUE
)

dim(myDfm)

cor_topic_model <- stm(myDfm, K = 10, 
                       verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)

findThoughts(cor_topic_model, 
             texts = conspTalk$text, 
             topics = c(1:10),
             n = 1)

#incorporate time predicator
k<-10
myModel <- stm(myDfm,
               K = k, 
               prevalence =~ date,
               data = conspTalk, 
               max.em.its = 1000, 
               seed = 1234, 
               init.type = "Spectral")
plot(myModel, type = "summary")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/Li-summary plot.pdf?raw=TRUE)

Word prevalence of each topic.

```
td_beta <- tidy(myModel)
td_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics")
```

pic: Li-topic words