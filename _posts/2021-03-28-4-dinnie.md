---
layout: post
title: "Classifying r/wallstreetbets (Dinnie)"
author: Ian Dinnie
date: 2021-03-28 12:24:54 -0500
categories: dinnie
---

I am going to start by sampling 10,000 comments from my 279,000 comment data frame. This will be my dataset for the project going forward. Of those 10,000 comments, I am going to randomly sample 500 comments to code by hand to train and test a model on. I may have to code more later depending on the results. 

```{r}
# Sample and organize the data
set.seed(1234)
# 279k is too much for now; sample 10k to work with
tenthousand <- raw %>% slice_sample(n = 10000)

# of the 10000 we just sampled, sample 500 to handcode
set.seed(4444)
handcode <- tenthousand %>% slice_sample(n = 500)

# write the handcode file to a csv for handcoding 
handcode %>% 
  select(author,user, comment_score,comment) %>% 
write_csv("To Handcode.csv")
```

I was able to hand-code 482 of the 500 comments I selected for coding. The 18 I didn't code were comments that had been deleted, emoticons whose encoding are strings of random, indecipherable strings, or (rarely) were single words that I did not feel confident in assigning a score to. I scored them on a scale of 1 to 4, 1 being clearly negative, 2 being leaning negative, 3 being leaning positive, and 4 being clearly positive. I then removed the clutter from emoticons, but kept the smiley face emoticons since they are useful for assessing sentiment.  
```{r}
coded <- read_csv("coded.csv") %>% 
  mutate(new_comment = gsub("[^A-Za-z0-9:) ]","", comment)) %>% 
  mutate(new_comment = gsub(":)", "smiley_face", new_comment)) %>% 
  mutate(new_comment = gsub("[^A-Za-z0-9 ]","",new_comment),
         upvote_class = case_when(
           comment_score <= median(comment_score) ~ 0,
           comment_score > median(comment_score) ~ 1)
         ,
         sentiment = case_when(
           Score == 1 | Score == 2 ~ 0,
           Score == 3 | Score == 4 ~ 1
         )) 

library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(caret)
```

```{r}
# make a corpus
coded_corpus <- corpus(coded, text_field = "new_comment")

# make a dfm
coded_dfm <- coded_corpus %>% 
  dfm(tolower = T,
    remove = stopwords("english"))

topfeatures(coded_dfm,30)

```

```{r}
# split the corpus into training and testing
# set seed
set.seed(1234)

# create doc ids
docvars(coded_corpus, "id") <- 1:ndoc(coded_corpus)

# create training set (60% of data) and initial test set
N <- ndoc(coded_corpus)
trainIndex <- sample(1:N,.6 * N) 
testIndex <- c(1:N)[-trainIndex]

# split test set in half (so 20% of data are test, 20% of data are held-out)
N <- length(testIndex)
heldOutIndex <- sample(1:N, .5 * N)
testIndex <- testIndex[-heldOutIndex]

# now apply indices to create subsets and dfms
dfmTrain <- corpus_subset(coded_corpus, id %in% trainIndex) %>% dfm()
dfmTest <- corpus_subset(coded_corpus, id %in% testIndex) %>% dfm()
dfmHeldOut <- corpus_subset(coded_corpus, id %in% heldOutIndex) %>% dfm()
```

```{r}
# let's try some models

# start with naive bayes
sentiment_NB <- textmodel_nb(dfmTrain, docvars(dfmTrain, "sentiment"), distribution = "Bernoulli")
summary(sentiment_NB)
```
```{r}
# let's see how it performed
library(e1071)
dfmTestMatched <- dfm_match(dfmTest, features = featnames(dfmTrain))

# create a confusion matrix
actual <- docvars(dfmTestMatched, "sentiment")
predicted <- predict(sentiment_NB, newdata = dfmTestMatched)
confusion <- table(actual, predicted)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion, mode = "everything")
```
This model isn't great given that the accuracy is less than the no information rate. Let's try another one. 
```{r}
# Let's try a SVM

score_SVM <- textmodel_svm(dfmTrain, docvars(dfmTrain, "sentiment"))

predicted_svm <- predict(score_SVM, newdata = dfmTestMatched)
confusion_svm <- table(actual, predicted_svm)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion_svm, mode = "everything")
```
This model did a lot better; this time, the accuracy is about 10 percentage points greater than the no information rate, and this difference is statistically significant at the 90% confidence level. If I am interpreting this correctly, the confidence interval is telling me that I can be 95% confident that my model will correctly classify comments outside of this sample somewhere between 53% and 73% of the time. This range is pretty large, which suggests to me that I would probably benefit from a larger sample size. However, I think I can still obtain better results. I am going to continue playing around with different models to see how high an accuracy rate I can achieve, and may possibly end up coding some more comments to have larger training and test sets. 

I am curious to see if these models can predict whether or not a comment will receive an upvote count higher than the median. It seems to do fairly poorly at this (worse than classifying sentiment), which makes me think that upvotes are more closely related to how popular the thread is than it is a function of actual comment content (although it is probably both). It also reinforces that I would benefit from a larger sample, since it is struggling in two different classification tasks.

```{r}
# start with naive bayes
upvotes_NB <- textmodel_nb(dfmTrain, docvars(dfmTrain, "upvote_class"), distribution = "Bernoulli")

# create a confusion matrix
actual <- docvars(dfmTestMatched, "upvote_class")
predicted <- predict(upvotes_NB, newdata = dfmTestMatched)
confusion <- table(actual, predicted)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion, mode = "everything")
```


```{r}
# Let's try a SVM

upvotes_SVM <- textmodel_svm(dfmTrain, docvars(dfmTrain, "upvote_class"))

actual <- docvars(dfmTestMatched, "upvote_class")
predicted_svm <- predict(upvotes_SVM, newdata = dfmTestMatched)
confusion_svm <- table(actual, predicted_svm)

# now calculate a number of statistics related to the confusion matrix
confusionMatrix(confusion_svm, mode = "everything")
```

In the next few weeks, I am going to continue playing with models and exploring the results in detail to see if I can spot any trends in how it is classifying sentiment in hopes of improving the results.







