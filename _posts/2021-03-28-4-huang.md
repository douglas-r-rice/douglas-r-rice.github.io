---
layout: post
title: "How Economic Conditions Influence Young People's Career Choices in China (Huang)"
author: Tieli Huang
date: 2021-03-28 12:24:54 -0500
categories: huang
---

**The aim of this blog post is to examine the correlation between state of the economy and young people's career choices in China in the past ten years.**

During the last forty years, China has gone through a period of rapid economic growth, which to many was the time to find opportunities and start their own businesses, outside *the public sector*. This is especially true for people from the southern parts of China, where a tradition of commerce has existed for a long time. Therefore, for the past years, we've seen an entrepreneur booming since the reform and opening in China since the 80s in the 20th century.

However, as the economic growth rate drops to about 5% in recent years, many people would instead, prefer jobs in the public sector, because these jobs are generally more stable.

In this post, I will use internet content, especially from the most used social media platform in China, *Douban*, to gather people's opinions towards *public sector jobs*, and to find out the changes in all these years.

***

# Corpus

## General
In this post, I will using internet from last ten years to observe a change in people's attitude towards public sector jobs. The platform I chose is Douban. I chose Douban because it has always been the most widely used social media networking platform in China since its founding in 2005 by Yang Bo (a physics PhD from UC San Diego), covering topics on politics, economy, culture, and more. More information about Douban can be found here: [Douban on Wikipedia](https://en.wikipedia.org/wiki/Douban). Also, I gather content from a single platform to retain some consistency in both data collection and analysis.

## Data collect process

### Challenge
One major challenge for me in this project is **to find sufficient and accurate internet content from many years ago**.
After trying to search from within the website, and not getting satisfying results, I decided to use google search instead. I used "公务员 site:www.douban.com" and set the time period from 2011.1.1 to 2011.12.31" to get desired discussions from ten years ago ("公务员" means public servant in Chinese). And I used this same strategy for my search in years 2012 to 2021 as well.

### Data from 2011 to 2020
And as I had anticipated, there wasn't much enthusiasm towards public sector jobs back then, or it could be because internet wasn't as developed as it is now back then. I'll select the most representative posts in the discussion of public servants on Douban. 
What constitute the "most representative" posts? First of all, it should be pertinent to the topic, the benefits and drawbacks of public sector jobs. Second, it should contain enough information.
The posts I'm using here are as follows:

- 2011: [I'll rank this government divisions for you](https://www.douban.com/note/141130372/). 
- 2012: [In or out of the institution?](https://www.douban.com/note/224753605/)
- 2013: [My final choice is the Ministry of Commerce](https://www.douban.com/note/306318565/)
- 2014: [My dad talks me out of applying for public sector jobs](https://www.douban.com/group/topic/48602262/)
- 2015: [So I heard the southerners don't like public sector jobs?](https://www.douban.com/group/topic/70910523/)
- 2016: [Five questions revealing the truth of public servant jobs](https://www.douban.com/note/539254679/)
- 2017: [Is it too stupid competing for a public servant position at the age of 35?](https://www.douban.com/group/topic/109402697/)
- 2018: [Is it that good to become a public servant? I need advice!](https://www.douban.com/group/topic/124798328/)
- 2019: [Is it too late to come home and become a public servant? I'm 30 now](https://www.douban.com/note/745504873/)
- 2020: [How do yall think of the trend that everybody is trying so hard for a public sector position?](https://www.douban.com/group/topic/195581262/)


***

## Preparing
Get the packages needed for analysis.

```{r}
# install and load packages
install.packages("tidytext")
install.packages("plyr")
install.packages("tidyverse")
install.packages("quanteda")
install.packages("quanteda.textmodels")

# load libraries
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
```

### For Chinese characters

```{r}
#install.packages("tmcn")
#install.packages("Rwordseg")
#install.packages("wordcloud2")
install.packages("rvest")

library(rvest)
library(Rwordseg)
library(tmcn)
library(wordcloud2)
```

```{r}
# here I use a dictionary for Chinese characters
# the dictionary is from NTU
data(NTUSD)
positive_simple <- NTUSD[[1]]
negtive_simple <- NTUSD[[2]]
positive_tradition <- NTUSD[[3]]
negtive_tradition <- NTUSD[[4]]

insertWords(positive_simple)
insertWords(negtive_simple)
insertWords(positive_tradition)
insertWords(negtive_tradition)
```

## Get content into R
### 2011 Data

```{r}
# identify the url
url2011 <- "https://www.douban.com/note/141130372/"

# use SelectorGadget to find the info
css_selector <- ".note"

content2011 <- url2011 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2011
```

### 2012 Data

```{r}
# identify the url
url2012 <- "https://www.douban.com/note/224753605/"

# use SelectorGadget to find the info
css_selector <- ".note"

content2012 <- url2012 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2012
```

### 2013 Data
```{r}
# identify the url
url2013 <- "https://www.douban.com/note/306318565/"

# use SelectorGadget to find the info
css_selector <- ".note"

content2013 <- url2013 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2013
```

### 2014 Data
```{r}
# identify the url
url2014 <- "https://www.douban.com/group/topic/48602262/"

# use SelectorGadget to find the info
css_selector <- "p"

content2014 <- url2014 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2014
```
### 2015 Data
```{r}
# identify the url
url2015 <- "https://www.douban.com/group/topic/70910523/"

# use SelectorGadget to find the info
css_selector <- ".article , #popular-comments"

content2015 <- url2015 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2015
```

### 2016 Data

```{r}
# identify the url
url2016 <- "https://www.douban.com/note/539254679/"

# use SelectorGadget to find the info
css_selector <- ".note"

content2016 <- url2016 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2016
```
### 2017 Data
```{r}
# identify the url
url2017 <- "https://www.douban.com/group/topic/109402697/"

# use SelectorGadget to find the info
css_selector <- ".article"

content2017 <- url2017 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2017
```

### 2018 Data
```{r}
# identify the url
url2018 <- "https://www.douban.com/group/topic/124798328/"

# use SelectorGadget to find the info
css_selector <- ".article"

content2018 <- url2018 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2018
```

### 2019 Data
```{r}
# identify the url
url2019 <- "https://www.douban.com/note/745504873/"

# use SelectorGadget to find the info
css_selector <- ".note"

content2019 <- url2019 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2019
```

### 2020 Data
```{r}
# identify the url
url2020 <- "https://www.douban.com/group/topic/195581262/"

# use SelectorGadget to find the info
css_selector <- ".article"

content2020 <- url2020 %>% 
  read_html() %>% 
  html_nodes(css = css_selector) %>% 
  html_text()
content2020
```

## A quick analysis
### Preprocessing

```{r}
stopwords <- read.table("/Users/huangcindy/Downloads/stopwords-master/hit_stopwords.txt")
stopwords
class(stopwords)
stopwords <- as.vector(stopwords[, 1])
stopwords
```


### 2011 Data analysis
```{r}
# word segmentation
word_seg2011 <- segmentCN(content2011, returnType = 'tm')

wf2011 <- createWordFreq(unlist(strsplit(word_seg2011, ' ')), stopwords= stopwords)
head(wf2011,20)

library(wordcloud2)
wordcloud2(wf2011, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2011.png?raw=TRUE)

### 2012 Data analysis
```{r}
# word segmentation
word_seg2012 <- segmentCN(content2012, returnType = 'tm')

wf2012 <- createWordFreq(unlist(strsplit(word_seg2012, ' ')), stopwords= stopword)
head(wf2012,20)

library(wordcloud2)
wordcloud2(wf2012, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2012.png?raw=TRUE)


### 2013 Data analysis
```{r}
# word segmentation
word_seg2013 <- segmentCN(content2013, returnType = 'tm')

wf2013 <- createWordFreq(unlist(strsplit(word_seg2013, ' ')), stopwords= stopword)
head(wf2013,20)

library(wordcloud2)
wordcloud2(wf2013, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2013.png?raw=TRUE)


### 2014 Data analysis
```{r}
# word segmentation
word_seg2014 <- segmentCN(content2014, returnType = 'tm')

wf2014 <- createWordFreq(unlist(strsplit(word_seg2014, ' ')), stopwords= stopword)
head(wf2014,20)

wordcloud2(wf2014, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2014.png?raw=TRUE)


### 2015 Data analysis
```{r}
# word segmentation
word_seg2015 <- segmentCN(content2015, returnType = 'tm')

wf2015 <- createWordFreq(unlist(strsplit(word_seg2015, ' ')), stopwords= stopword)
head(wf2015,20)

library(wordcloud2)
wordcloud2(wf2015, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2015.png?raw=TRUE)


### 2016 Data analysis
```{r}
# word segmentation
word_seg2016 <- segmentCN(content2016, returnType = 'tm')

wf2016 <- createWordFreq(unlist(strsplit(word_seg2016, ' ')), stopwords= stopword)
head(wf2016,20)

wordcloud2(wf2016, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2016.png?raw=TRUE)


### 2017 Data analysis
```{r}
# word segmentation
word_seg2017 <- segmentCN(content2017, returnType = 'tm')

wf2017 <- createWordFreq(unlist(strsplit(word_seg2017, ' ')), stopwords= stopword)
head(wf2017,20)

wordcloud2(wf2017, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2017.png?raw=TRUE)


### 2018 Data analysis
```{r}
# word segmentation
word_seg2018 <- segmentCN(content2018, returnType = 'tm')

wf2018 <- createWordFreq(unlist(strsplit(word_seg2018, ' ')), stopwords= stopword)
head(wf2018,20)

wordcloud2(wf2018, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2018.png?raw=TRUE)


### 2019 Data analysis
```{r}
# word segmentation
word_seg2019 <- segmentCN(content2019, returnType = 'tm')

wf2019 <- createWordFreq(unlist(strsplit(word_seg2019, ' ')), stopwords= stopword)
head(wf2019,20)

wordcloud2(wf2019, color = "random-light", backgroundColor = "grey")
```

![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2019.png?raw=TRUE)


### 2020 Data analysis
```{r}
# word segmentation
word_seg2020 <- segmentCN(content2020, returnType = 'tm')

wf2020 <- createWordFreq(unlist(strsplit(word_seg2020, ' ')), stopwords= stopword)
head(wf2020,20)

wordcloud2(wf2020, color = "random-light", backgroundColor = "grey")
```
![](https://github.com/douglas-r-rice/douglas-r-rice.github.io/blob/main/_posts/wf2020.png?raw=TRUE)


# Conclusion
## Findings from internet content fromn 2011 to 2020
As we can see form the text and wordcloud images from year 2011 to 2020, there exists a growing desire for public sector jobs for Chinese young people.
